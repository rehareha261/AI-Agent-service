================================================================================
TODO - CORRECTIONS DES ERREURS DANS LES LOGS CELERY
================================================================================
Date: 6 octobre 2025
Analyse: Logs du workflow "Ajouter un fichier main" (ID: 5028526668)
Statut: Workflow fonctionne mais 3 erreurs critiques d√©tect√©es

================================================================================
üìä R√âSUM√â RAPIDE
================================================================================

‚úÖ POINTS POSITIFS:
   - Workflow complet fonctionnel (85 secondes)
   - Code g√©n√©r√© (main.txt)
   - PR #11 cr√©√©e avec succ√®s
   - Validation humaine Monday.com d√©tect√©e ("non" ‚Üí debug demand√©)
   - Statut Monday mis √† jour ("Working on it")

üî¥ ERREURS CRITIQUES (3):
   1. Erreur cr√©ation validation en DB (generated_code format invalide)
   2. Erreur sauvegarde r√©ponse validation en DB (validation non trouv√©e)
   3. Erreur debug OpenAI ('list' object has no attribute 'get')

‚ö†Ô∏è WARNINGS NON-CRITIQUES:
   - Pydantic serialization warnings (int ‚Üí str)
   - Colonnes Monday.com non identifi√©es

================================================================================
ERREUR 1/3 - CRITIQUE - CR√âATION VALIDATION EN DB
================================================================================

Log de l'erreur:
[12:08:44] ‚ùå Erreur cr√©ation validation val_5028526668_1759741724: 
           invalid input for query argument $9: 
           {'main.txt': "# Projet GenericDAO - R√©su... 
           (expected str, got dict)

Probl√®me:
- Le champ 'generated_code' dans la table validations attend un STRING (JSON)
- Mais le code Python envoie un DICT directement
- PostgreSQL rejette l'insertion

Fichier concern√©: nodes/monday_validation_node.py
Ligne: ~86-105

Solution:
Convertir le dictionnaire generated_code en JSON string avant insertion

--- CODE √Ä MODIFIER ---

Chercher dans nodes/monday_validation_node.py:

            # Cr√©er la demande de validation
            validation_request = HumanValidationRequest(
                validation_id=validation_id,
                workflow_id=state.get("workflow_id", ""),
                task_id=str(state["task"].task_id),
                task_title=state["task"].title,
                generated_code=generated_code if generated_code else {"summary": "Code g√©n√©r√© - voir fichiers modifi√©s"},
                code_summary=f"Impl√©mentation de: {state['task'].title}",

Remplacer par:

            import json
            
            # Convertir generated_code en JSON string pour la DB
            generated_code_str = json.dumps(
                generated_code if generated_code else {"summary": "Code g√©n√©r√© - voir fichiers modifi√©s"},
                ensure_ascii=False,
                indent=2
            )
            
            # Cr√©er la demande de validation
            validation_request = HumanValidationRequest(
                validation_id=validation_id,
                workflow_id=state.get("workflow_id", ""),
                task_id=str(state["task"].task_id),
                task_title=state["task"].title,
                generated_code=generated_code_str,  # ‚Üê STRING au lieu de DICT
                code_summary=f"Impl√©mentation de: {state['task'].title}",

--- ALTERNATIVE SI LE MOD√àLE ATTEND UN DICT ---

Si HumanValidationRequest attend un Dict mais que la DB attend un string,
modifier le mod√®le Pydantic dans models/schemas.py:

Chercher dans models/schemas.py:

    class HumanValidationRequest(BaseModel):
        # ...
        generated_code: Dict[str, str] = Field(..., description="Code g√©n√©r√© par fichier")

Ajouter un validateur:

    @field_validator('generated_code', mode='before')
    @classmethod
    def convert_generated_code_to_dict(cls, v):
        """Assure que generated_code est un dict."""
        if isinstance(v, str):
            import json
            return json.loads(v)
        return v

ET modifier la requ√™te SQL d'insertion pour convertir le dict en JSON:

Dans services/human_validation_service.py, chercher l'INSERT INTO et ajouter:

    json.dumps(validation.generated_code) au lieu de validation.generated_code

================================================================================
ERREUR 2/3 - CRITIQUE - SAUVEGARDE R√âPONSE VALIDATION
================================================================================

Log de l'erreur:
[12:09:17] ‚ùå Validation 467847794 non trouv√©e
[12:09:17] ‚ö†Ô∏è √âchec sauvegarde r√©ponse validation en DB

Probl√®me:
- La validation n'a pas √©t√© sauvegard√©e en DB (√† cause de ERREUR 1)
- Donc impossible de sauvegarder la r√©ponse humaine
- Cha√Æne de d√©pendances cass√©e

Fichier concern√©: services/monday_validation_service.py ou nodes/monday_validation_node.py

Solution:
Cette erreur sera automatiquement r√©solue quand ERREUR 1 sera corrig√©e.
Mais ajouter une gestion plus robuste:

--- CODE √Ä AJOUTER ---

Dans la fonction qui sauvegarde la r√©ponse de validation:

    try:
        # Chercher la validation en DB
        validation = await get_validation_by_id(validation_id)
        
        if not validation:
            logger.warning(f"‚ö†Ô∏è Validation {validation_id} non trouv√©e - cr√©ation d'un enregistrement de secours")
            
            # Cr√©er une validation de secours avec infos minimales
            validation = await create_fallback_validation(
                validation_id=validation_id,
                task_id=task_id,
                status="pending"
            )
        
        # Sauvegarder la r√©ponse
        await save_validation_response(validation.id, response)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde r√©ponse: {e}")
        # Ne pas bloquer le workflow
        pass

================================================================================
ERREUR 3/3 - CRITIQUE - DEBUG OPENAI
================================================================================

Log de l'erreur:
[12:09:18] ‚ùå Erreur debug OpenAI: 'list' object has no attribute 'get'

Probl√®me:
- Le n≈ìud debug_openai re√ßoit test_results comme une LIST
- Mais le code essaie d'appeler .get() dessus (m√©thode de dict)
- Python l√®ve AttributeError

Fichier concern√©: nodes/openai_debug_node.py
Ligne: ~50-100 (o√π test_results.get() est appel√©)

Solution:
V√©rifier le type de test_results avant d'appeler .get()

--- CODE √Ä MODIFIER ---

Chercher dans nodes/openai_debug_node.py toutes les occurrences de:

    test_results.get(...)

Remplacer par:

    # Normaliser test_results en dict
    if isinstance(test_results, list):
        test_results_dict = {
            "tests": test_results,
            "count": len(test_results),
            "all_passed": all(
                test.get("success", False) if isinstance(test, dict) else False 
                for test in test_results
            )
        }
    elif isinstance(test_results, dict):
        test_results_dict = test_results
    else:
        test_results_dict = {"raw": str(test_results)}
    
    # Maintenant utiliser test_results_dict.get(...)
    tests_passed = test_results_dict.get("all_passed", False)

--- ALTERNATIVE: FONCTION HELPER ---

Ajouter au d√©but du fichier nodes/openai_debug_node.py:

def _normalize_test_results(test_results) -> Dict[str, Any]:
    """
    Normalise test_results en dictionnaire pour √©viter AttributeError.
    
    Args:
        test_results: Peut √™tre list, dict, ou autre
        
    Returns:
        Dict structur√©
    """
    if not test_results:
        return {"tests": [], "count": 0, "all_passed": True}
    
    if isinstance(test_results, dict):
        return test_results
    
    if isinstance(test_results, list):
        return {
            "tests": test_results,
            "count": len(test_results),
            "all_passed": all(
                t.get("success", False) if isinstance(t, dict) else False 
                for t in test_results
            )
        }
    
    return {"raw": str(test_results), "type": str(type(test_results))}

Puis dans la fonction debug_openai:

    test_results = _normalize_test_results(state["results"].get("test_results"))
    # Maintenant utiliser test_results.get(...) en toute s√©curit√©

================================================================================
CORRECTION BONUS - PYDANTIC WARNINGS
================================================================================

Log du warning (r√©p√©t√© ~10 fois):
UserWarning: Pydantic serializer warnings:
Expected `str` but got `int` - serialized value may not be as expected

Probl√®me:
- Des champs d√©finis comme str re√ßoivent des int
- Probablement task_id, run_id, step_id, etc.

Solution RAPIDE:
Ajouter des validateurs dans models/schemas.py

Dans la classe GraphState ou TaskRequest:

    @field_validator('task_id', 'run_id', 'step_id', mode='before')
    @classmethod
    def convert_ids_to_str(cls, v):
        """Convertit les IDs en string."""
        if v is None:
            return v
        return str(v)

OU changer les types pour accepter les deux:

    task_id: Union[str, int] = Field(..., description="ID de la t√¢che")
    run_id: Optional[Union[str, int]] = Field(None, description="ID du run")

================================================================================
ORDRE D'APPLICATION DES CORRECTIONS
================================================================================

PRIORIT√â 1 (CRITIQUE):
‚ñ° ERREUR 1: Corriger generated_code (nodes/monday_validation_node.py)
  ‚Üí Permet de sauvegarder les validations en DB

PRIORIT√â 2 (CRITIQUE):
‚ñ° ERREUR 3: Corriger debug OpenAI (nodes/openai_debug_node.py)
  ‚Üí Permet au debug de fonctionner quand l'humain rejette

PRIORIT√â 3 (AUTO-R√âSOLU):
‚ñ° ERREUR 2: Sera r√©solu automatiquement apr√®s ERREUR 1

PRIORIT√â 4 (OPTIONNEL):
‚ñ° Warnings Pydantic (models/schemas.py)
  ‚Üí Nettoie les logs mais non-bloquant

================================================================================
FICHIERS √Ä MODIFIER
================================================================================

1. nodes/monday_validation_node.py
   Action: Convertir generated_code en JSON string
   Ligne: ~95
   Temps: 5 minutes

2. nodes/openai_debug_node.py
   Action: Normaliser test_results avant .get()
   Ligne: ~50-100 (chercher test_results.get)
   Temps: 10 minutes

3. models/schemas.py (optionnel)
   Action: Ajouter validateurs pour IDs
   Ligne: Dans HumanValidationRequest et GraphState
   Temps: 5 minutes

================================================================================
COMMANDES APR√àS MODIFICATIONS
================================================================================

1. V√©rifier les modifications:
   git diff nodes/monday_validation_node.py
   git diff nodes/openai_debug_node.py

2. Cr√©er des backups:
   cp nodes/monday_validation_node.py nodes/monday_validation_node.py.backup
   cp nodes/openai_debug_node.py nodes/openai_debug_node.py.backup

3. Red√©marrer Celery:
   pkill -f "celery.*worker"
   celery -A services.celery_app worker --loglevel=info

4. Tester avec un workflow:
   - Cr√©er t√¢che dans Monday.com
   - R√©pondre "oui" pour merger
   - OU "non" pour tester le debug

5. V√©rifier les logs:
   tail -f logs/workflow.log | grep -E "(‚ùå|‚úÖ|Erreur)"

6. V√©rifier la DB:
   psql postgresql://admin:password@localhost:5432/ai_agent_admin
   
   SELECT validation_id, task_id, status 
   FROM human_validations 
   ORDER BY created_at DESC 
   LIMIT 5;
   
   SELECT * 
   FROM human_validation_responses 
   ORDER BY created_at DESC 
   LIMIT 5;

================================================================================
CHECKLIST DE VALIDATION
================================================================================

Apr√®s avoir appliqu√© toutes les corrections:

‚ñ° ERREUR 1 disparue:
  Logs contiennent: "‚úÖ Validation humaine enregistr√©e en DB avec succ√®s"
  Au lieu de: "‚ùå Erreur cr√©ation validation"

‚ñ° ERREUR 2 disparue:
  Logs contiennent: "‚úÖ R√©ponse de validation sauvegard√©e en DB"
  Au lieu de: "‚ùå Validation non trouv√©e"

‚ñ° ERREUR 3 disparue:
  Debug fonctionne quand humain rejette
  Logs contiennent: "‚úÖ Analyse debug termin√©e"
  Au lieu de: "‚ùå Erreur debug OpenAI: 'list' object"

‚ñ° Warnings Pydantic r√©duits (si correction appliqu√©e)

‚ñ° Workflow complet fonctionne:
  - Validation "oui" ‚Üí merge
  - Validation "non" ‚Üí debug puis update Monday

‚ñ° DB contient les validations:
  Tables human_validations et human_validation_responses peupl√©es

================================================================================
TESTS DE NON-R√âGRESSION
================================================================================

Apr√®s corrections, tester ces sc√©narios:

Test 1: Validation "OUI"
- Cr√©er t√¢che Monday
- R√©pondre "oui"
- V√©rifier merge r√©ussi
- V√©rifier statut "Done"
- V√©rifier validation en DB

Test 2: Validation "NON"
- Cr√©er t√¢che Monday
- R√©pondre "non"
- V√©rifier debug lanc√©
- V√©rifier statut "Working on it"
- V√©rifier validation rejet√©e en DB

Test 3: Timeout validation
- Cr√©er t√¢che Monday
- Ne pas r√©pondre pendant 10 min
- V√©rifier timeout
- V√©rifier statut "Stuck"

================================================================================
TEMPS ESTIM√â
================================================================================

Correction ERREUR 1: 10 minutes
Correction ERREUR 3: 15 minutes
Tests et v√©rifications: 15 minutes
---------------------------------
TOTAL: 40 minutes

================================================================================
R√âSUM√â EX√âCUTIF
================================================================================

üìä √âtat actuel:
- Workflow fonctionne de bout en bout ‚úÖ
- PR cr√©√©e, validation humaine d√©tect√©e ‚úÖ
- 3 erreurs de persistance/debug ‚ùå

üéØ Objectif:
- Sauvegarder validations en DB
- Permettre debug apr√®s rejet
- Nettoyer les logs

üöÄ Actions:
1. Convertir generated_code en JSON string (5 min)
2. Normaliser test_results dans debug (15 min)
3. Tester workflow complet (20 min)

‚úÖ R√©sultat attendu:
- Validations persist√©es en DB
- Debug fonctionnel apr√®s rejet
- Workflow 100% op√©rationnel

================================================================================
NOTES IMPORTANTES
================================================================================

‚Ä¢ Ces corrections sont CRITIQUES pour la tra√ßabilit√©
‚Ä¢ Sans elles, les validations humaines ne sont pas sauvegard√©es
‚Ä¢ Le debug apr√®s rejet ne fonctionne pas
‚Ä¢ Mais le workflow continue de fonctionner (failover en place)

‚Ä¢ La correction de ERREUR 1 est la plus importante
‚Ä¢ Elle d√©bloque automatiquement ERREUR 2
‚Ä¢ ERREUR 3 est ind√©pendante (debug)

‚Ä¢ Pas de risque de r√©gression: corrections isol√©es
‚Ä¢ Backups automatiques recommand√©s
‚Ä¢ Tests n√©cessaires apr√®s chaque correction

================================================================================
AIDE ET SUPPORT
================================================================================

Si une correction √©choue:

1. V√©rifier les imports en haut du fichier (json, typing, etc.)
2. V√©rifier les num√©ros de ligne (peuvent varier)
3. Chercher les patterns exacts fournis
4. Consulter les logs pour identifier l'erreur exacte
5. Restaurer le backup si n√©cessaire

Fichiers de r√©f√©rence:
- TOUS_LES_CHANGEMENTS.txt (corrections base de donn√©es)
- CORRECTIONS_URGENTES_CELERY.md (analyse technique)
- TODO_CORRECTIONS_LOGS.txt (ce fichier)

================================================================================
FIN DU TODO
================================================================================
