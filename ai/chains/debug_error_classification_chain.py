"""Cha√Æne LangChain pour la classification et le regroupement des erreurs de debug."""

from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from config.settings import get_settings
from utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()


class ErrorCategory(str, Enum):
    """Cat√©gorie d'erreur identifi√©e."""
    IMPORT_ERROR = "import_error"
    SYNTAX_ERROR = "syntax_error"
    TYPE_ERROR = "type_error"
    ATTRIBUTE_ERROR = "attribute_error"
    NAME_ERROR = "name_error"
    ASSERTION_ERROR = "assertion_error"
    VALUE_ERROR = "value_error"
    KEY_ERROR = "key_error"
    INDEX_ERROR = "index_error"
    RUNTIME_ERROR = "runtime_error"
    LOGIC_ERROR = "logic_error"
    STYLE_ERROR = "style_error"
    CONFIGURATION_ERROR = "configuration_error"
    DEPENDENCY_ERROR = "dependency_error"
    OTHER = "other"


class ErrorPriority(int, Enum):
    """Niveau de priorit√© pour correction d'erreur."""
    CRITICAL = 5  # Bloquant, doit √™tre corrig√© en premier
    HIGH = 4      # Important, affecte la fonctionnalit√© principale
    MEDIUM = 3    # Mod√©r√©, affecte une fonctionnalit√© secondaire
    LOW = 2       # Mineur, probl√®me cosm√©tique ou de style
    TRIVIAL = 1   # Trivial, peut √™tre ignor√© temporairement


class FixStrategy(str, Enum):
    """Strat√©gie de correction recommand√©e."""
    ADD_IMPORT = "add_import"
    FIX_SYNTAX = "fix_syntax"
    ADD_ATTRIBUTE = "add_attribute"
    UPDATE_TYPE = "update_type"
    ADD_DEPENDENCY = "add_dependency"
    REFACTOR_LOGIC = "refactor_logic"
    UPDATE_CONFIG = "update_config"
    FIX_TEST = "fix_test"
    ADD_ERROR_HANDLING = "add_error_handling"
    MULTIPLE_ACTIONS = "multiple_actions"


class ErrorInstance(BaseModel):
    """Instance individuelle d'erreur."""
    error_message: str = Field(description="Message d'erreur complet")
    file_path: Optional[str] = Field(default=None, description="Chemin du fichier concern√©")
    line_number: Optional[int] = Field(default=None, description="Num√©ro de ligne")
    test_name: Optional[str] = Field(default=None, description="Nom du test en √©chec")


class ErrorGroup(BaseModel):
    """Groupe d'erreurs similaires."""
    category: ErrorCategory = Field(description="Cat√©gorie d'erreur")
    
    group_summary: str = Field(
        description="R√©sum√© du groupe d'erreurs"
    )
    
    files_involved: List[str] = Field(
        default_factory=list,
        description="Fichiers impliqu√©s dans ce groupe d'erreurs"
    )
    
    probable_root_cause: str = Field(
        description="Cause racine probable du groupe d'erreurs"
    )
    
    priority: ErrorPriority = Field(
        description="Priorit√© de correction (1-5, 5=critique)"
    )
    
    suggested_fix_strategy: FixStrategy = Field(
        description="Strat√©gie de correction sugg√©r√©e"
    )
    
    fix_description: str = Field(
        description="Description d√©taill√©e de la correction √† appliquer"
    )
    
    error_instances: List[ErrorInstance] = Field(
        default_factory=list,
        description="Instances d'erreurs individuelles dans ce groupe"
    )
    
    estimated_fix_time_minutes: int = Field(
        ge=1,
        description="Temps estim√© pour corriger en minutes"
    )
    
    dependencies: List[str] = Field(
        default_factory=list,
        description="D√©pendances √† ce groupe (autres groupes √† corriger d'abord)"
    )
    
    impact_scope: str = Field(
        description="√âtendue de l'impact (local, module, global)"
    )


class ErrorClassification(BaseModel):
    """Mod√®le Pydantic pour la classification compl√®te des erreurs."""
    
    total_errors: int = Field(description="Nombre total d'erreurs d√©tect√©es")
    
    groups: List[ErrorGroup] = Field(
        min_length=1,
        description="Groupes d'erreurs identifi√©s"
    )
    
    reduction_percentage: float = Field(
        ge=0.0,
        le=100.0,
        description="Pourcentage de r√©duction du nombre d'actions"
    )
    
    recommended_fix_order: List[int] = Field(
        description="Ordre recommand√© de correction (indices des groupes)"
    )
    
    critical_blockers: List[str] = Field(
        default_factory=list,
        description="Bloqueurs critiques identifi√©s"
    )
    
    estimated_total_fix_time: int = Field(
        description="Temps total estim√© pour toutes les corrections en minutes"
    )
    
    overall_complexity: str = Field(
        description="Complexit√© globale des corrections (simple, moderate, complex)"
    )
    
    class Config:
        """Configuration Pydantic."""
        json_schema_extra = {
            "example": {
                "total_errors": 5,
                "groups": [
                    {
                        "category": "import_error",
                        "group_summary": "Imports manquants dans 3 fichiers",
                        "files_involved": ["test_api.py", "api/routes.py"],
                        "probable_root_cause": "Module 'fastapi' non import√©",
                        "priority": 5,
                        "suggested_fix_strategy": "add_import",
                        "fix_description": "Ajouter 'from fastapi import FastAPI' en t√™te de fichier",
                        "error_instances": [
                            {
                                "error_message": "NameError: name 'FastAPI' is not defined",
                                "file_path": "api/routes.py",
                                "line_number": 10
                            }
                        ],
                        "estimated_fix_time_minutes": 2,
                        "dependencies": [],
                        "impact_scope": "module"
                    }
                ],
                "reduction_percentage": 40.0,
                "recommended_fix_order": [0, 1],
                "critical_blockers": ["Import manquant bloque l'ex√©cution"],
                "estimated_total_fix_time": 10,
                "overall_complexity": "simple"
            }
        }


def create_debug_error_classification_chain(
    provider: str = "anthropic",
    model: Optional[str] = None,
    temperature: float = 0.0,  # Temp√©rature basse pour classification pr√©cise
    max_tokens: int = 3000
):
    """
    Cr√©e une cha√Æne LCEL pour classifier et regrouper les erreurs.
    
    Args:
        provider: Provider LLM √† utiliser ("anthropic" ou "openai")
        model: Nom du mod√®le (optionnel, utilise le d√©faut du provider)
        temperature: Temp√©rature du mod√®le (0.0-1.0)
        max_tokens: Nombre maximum de tokens
        
    Returns:
        Cha√Æne LCEL configur√©e (Prompt ‚Üí LLM ‚Üí Parser)
        
    Raises:
        ValueError: Si le provider n'est pas support√©
        Exception: Si les cl√©s API sont manquantes
    """
    logger.info(f"üîó Cr√©ation debug_error_classification_chain avec provider={provider}")
    
    # 1. Cr√©er le parser Pydantic
    parser = PydanticOutputParser(pydantic_object=ErrorClassification)
    
    # 2. Cr√©er le prompt template avec instructions de formatage
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """Tu es un expert en analyse d'erreurs de code qui classifie et regroupe les erreurs pour optimiser les corrections.

Ton objectif est d'identifier les patterns d'erreurs, de les regrouper intelligemment, et de proposer une strat√©gie de correction efficace.

IMPORTANT: Tu DOIS retourner UNIQUEMENT du JSON valide, sans texte avant ou apr√®s.
Utilise le sch√©ma suivant:

{format_instructions}

Principes de classification:
1. REGROUPE les erreurs similaires ou li√©es
2. IDENTIFIE la cause racine commune
3. PRIORISE selon l'impact (ImportError > AssertionError > Style)
4. ORDONNE les corrections par d√©pendances
5. ESTIME le temps de correction r√©aliste
6. PROPOSE une strat√©gie de correction claire"""),
        ("user", """Analyse et classifie ces erreurs:

## LOGS DE TESTS
```
{test_logs}
```

## STACK TRACES
```
{stack_traces}
```

## DIFF R√âCENT
```
{recent_diff}
```

## FICHIERS MODIFI√âS
{modified_files}

## CONTEXTE ADDITIONNEL
{additional_context}

Classe ces erreurs en groupes coh√©rents et propose une strat√©gie de correction optimis√©e.""")
    ])
    
    # 3. Injecter les instructions de formatage du parser dans le prompt
    prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
    
    # 4. Cr√©er le LLM selon le provider
    if provider.lower() == "anthropic":
        if not settings.anthropic_api_key:
            raise Exception("ANTHROPIC_API_KEY manquante dans la configuration")
        
        llm = ChatAnthropic(
            model=model or "claude-3-5-sonnet-20241022",
            anthropic_api_key=settings.anthropic_api_key,
            temperature=temperature,
            max_tokens=max_tokens
        )
        logger.info(f"‚úÖ LLM Anthropic initialis√©: {model or 'claude-3-5-sonnet-20241022'}")
        
    elif provider.lower() == "openai":
        if not settings.openai_api_key:
            raise Exception("OPENAI_API_KEY manquante dans la configuration")
        
        llm = ChatOpenAI(
            model=model or "gpt-4",
            openai_api_key=settings.openai_api_key,
            temperature=temperature,
            max_tokens=max_tokens
        )
        logger.info(f"‚úÖ LLM OpenAI initialis√©: {model or 'gpt-4'}")
        
    else:
        raise ValueError(f"Provider non support√©: {provider}. Utilisez 'anthropic' ou 'openai'")
    
    # 5. Cr√©er la cha√Æne LCEL: Prompt ‚Üí LLM ‚Üí Parser
    chain = prompt | llm | parser
    
    logger.info("‚úÖ Debug error classification chain cr√©√©e avec succ√®s")
    return chain


async def classify_debug_errors(
    test_logs: str,
    stack_traces: Optional[str] = None,
    recent_diff: Optional[str] = None,
    modified_files: Optional[List[str]] = None,
    additional_context: Optional[Dict[str, Any]] = None,
    provider: str = "anthropic",
    fallback_to_openai: bool = True
) -> ErrorClassification:
    """
    Classifie et regroupe les erreurs de debug pour optimiser les corrections.
    
    Args:
        test_logs: Logs des tests en √©chec
        stack_traces: Stack traces des erreurs (optionnel)
        recent_diff: Diff r√©cent du code (optionnel)
        modified_files: Liste des fichiers r√©cemment modifi√©s (optionnel)
        additional_context: Contexte additionnel (dict)
        provider: Provider principal ("anthropic" ou "openai")
        fallback_to_openai: Si True, fallback vers OpenAI si le provider principal √©choue
        
    Returns:
        ErrorClassification valid√© par Pydantic
        
    Raises:
        Exception: Si tous les providers √©chouent
    """
    # Pr√©parer les inputs
    context_str = str(additional_context) if additional_context else "Aucun contexte additionnel"
    files_str = "\n".join(f"- {f}" for f in (modified_files or [])) if modified_files else "Non sp√©cifi√©s"
    
    inputs = {
        "test_logs": test_logs or "Non disponibles",
        "stack_traces": stack_traces or "Non disponibles",
        "recent_diff": recent_diff or "Non disponible",
        "modified_files": files_str,
        "additional_context": context_str
    }
    
    # Tentative avec le provider principal
    try:
        logger.info(f"üöÄ Classification des erreurs avec {provider}...")
        chain = create_debug_error_classification_chain(provider=provider)
        classification = await chain.ainvoke(inputs)
        
        logger.info(
            f"‚úÖ Classification g√©n√©r√©e: "
            f"{classification.total_errors} erreurs ‚Üí {len(classification.groups)} groupes "
            f"(r√©duction: {classification.reduction_percentage:.1f}%)"
        )
        return classification
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec classification avec {provider}: {e}")
        
        # Fallback vers OpenAI si configur√©
        if fallback_to_openai and provider.lower() != "openai":
            try:
                logger.info("üîÑ Fallback vers OpenAI...")
                chain_fallback = create_debug_error_classification_chain(provider="openai")
                classification = await chain_fallback.ainvoke(inputs)
                
                logger.info(f"‚úÖ Classification g√©n√©r√©e avec succ√®s (fallback OpenAI)")
                return classification
                
            except Exception as fallback_error:
                logger.error(f"‚ùå Fallback OpenAI √©chou√©: {fallback_error}")
                raise Exception(
                    f"Tous les providers ont √©chou√©. "
                    f"Principal: {e}, Fallback: {fallback_error}"
                )
        
        # Pas de fallback configur√©
        raise Exception(f"Classification √©chou√©e avec {provider}: {e}")


def extract_classification_metrics(classification: ErrorClassification) -> Dict[str, Any]:
    """
    Extrait les m√©triques d'une classification d'erreurs.
    
    Args:
        classification: Classification valid√©e
        
    Returns:
        Dictionnaire de m√©triques
    """
    priority_distribution = {}
    for group in classification.groups:
        priority_name = f"priority_{group.priority}"
        priority_distribution[priority_name] = priority_distribution.get(priority_name, 0) + 1
    
    category_distribution = {}
    for group in classification.groups:
        category = group.category.value
        category_distribution[category] = category_distribution.get(category, 0) + 1
    
    total_instances = sum(len(g.error_instances) for g in classification.groups)
    
    return {
        "total_errors": classification.total_errors,
        "total_groups": len(classification.groups),
        "total_error_instances": total_instances,
        "reduction_percentage": classification.reduction_percentage,
        "actions_before": classification.total_errors,
        "actions_after": len(classification.groups),
        "priority_distribution": priority_distribution,
        "category_distribution": category_distribution,
        "critical_blockers_count": len(classification.critical_blockers),
        "estimated_total_fix_time": classification.estimated_total_fix_time,
        "average_fix_time_per_group": (
            classification.estimated_total_fix_time / len(classification.groups)
            if classification.groups else 0
        ),
        "overall_complexity": classification.overall_complexity,
        "has_critical_errors": any(
            g.priority == ErrorPriority.CRITICAL for g in classification.groups
        )
    }


def get_priority_ordered_groups(classification: ErrorClassification) -> List[ErrorGroup]:
    """
    Retourne les groupes d'erreurs ordonn√©s par priorit√© et d√©pendances.
    
    Args:
        classification: Classification valid√©e
        
    Returns:
        Liste de groupes ordonn√©s
    """
    # Utiliser l'ordre recommand√© si disponible
    if classification.recommended_fix_order:
        try:
            return [classification.groups[i] for i in classification.recommended_fix_order]
        except IndexError:
            logger.warning("‚ö†Ô∏è Ordre recommand√© invalide, tri par priorit√©")
    
    # Sinon, trier par priorit√© d√©croissante
    return sorted(classification.groups, key=lambda g: g.priority, reverse=True)

