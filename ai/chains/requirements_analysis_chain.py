"""Cha√Æne LangChain pour l'analyse structur√©e des requirements."""

import os
from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from config.settings import get_settings
from utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()


class TaskComplexity(str, Enum):
    """Niveau de complexit√© d'une t√¢che."""
    TRIVIAL = "trivial"
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    VERY_COMPLEX = "very_complex"


class RiskLevel(str, Enum):
    """Niveau de risque identifi√©."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class FileValidationStatus(str, Enum):
    """Statut de validation d'un fichier."""
    VALID = "valid"
    NOT_FOUND = "not_found"
    UNCERTAIN = "uncertain"


class CandidateFile(BaseModel):
    """Fichier candidat pour modification."""
    path: str = Field(description="Chemin du fichier")
    action: str = Field(description="Action √† effectuer (create, modify, delete)")
    reason: str = Field(description="Raison de la modification")
    validation_status: FileValidationStatus = Field(
        default=FileValidationStatus.UNCERTAIN,
        description="Statut de validation du fichier"
    )


class TaskDependency(BaseModel):
    """D√©pendance identifi√©e pour la t√¢che."""
    name: str = Field(description="Nom de la d√©pendance")
    type: str = Field(description="Type (package, service, file, etc.)")
    version: Optional[str] = Field(default=None, description="Version requise si applicable")
    required: bool = Field(default=True, description="Si la d√©pendance est obligatoire")


class IdentifiedRisk(BaseModel):
    """Risque identifi√© lors de l'analyse."""
    description: str = Field(description="Description du risque")
    level: RiskLevel = Field(description="Niveau de gravit√© du risque")
    mitigation: str = Field(description="Strat√©gie de mitigation propos√©e")
    probability: int = Field(ge=1, le=10, description="Probabilit√© d'occurrence (1-10)")


class Ambiguity(BaseModel):
    """Ambigu√Øt√© identifi√©e dans les requirements."""
    question: str = Field(description="Question ou point ambigu")
    impact: str = Field(description="Impact de cette ambigu√Øt√©")
    suggested_assumption: Optional[str] = Field(
        default=None,
        description="Hypoth√®se sugg√©r√©e si pas de clarification"
    )


class RequirementsAnalysis(BaseModel):
    """Mod√®le Pydantic pour l'analyse compl√®te des requirements."""
    schema_version: int = Field(default=1, description="Version du sch√©ma")

    task_summary: str = Field(description="R√©sum√© de la t√¢che analys√©e")

    complexity: TaskComplexity = Field(description="Complexit√© estim√©e de la t√¢che")
    complexity_score: int = Field(
        ge=1,
        le=10,
        description="Score de complexit√© (1=tr√®s simple, 10=tr√®s complexe)"
    )

    estimated_duration_minutes: int = Field(
        ge=5,
        description="Dur√©e estim√©e en minutes"
    )

    candidate_files: List[CandidateFile] = Field(
        default_factory=list,
        description="Fichiers identifi√©s pour modification"
    )

    dependencies: List[TaskDependency] = Field(
        default_factory=list,
        description="D√©pendances identifi√©es"
    )

    risks: List[IdentifiedRisk] = Field(
        default_factory=list,
        description="Risques identifi√©s"
    )

    ambiguities: List[Ambiguity] = Field(
        default_factory=list,
        description="Ambigu√Øt√©s ou points n√©cessitant clarification"
    )

    missing_info: List[str] = Field(
        default_factory=list,
        description="Informations manquantes pour une impl√©mentation optimale"
    )

    implementation_approach: str = Field(
        description="Approche d'impl√©mentation recommand√©e"
    )

    test_strategy: str = Field(
        description="Strat√©gie de test recommand√©e"
    )

    breaking_changes_risk: bool = Field(
        default=False,
        description="Si l'impl√©mentation risque de casser du code existant"
    )

    requires_external_deps: bool = Field(
        default=False,
        description="Si l'impl√©mentation n√©cessite des d√©pendances externes"
    )

    quality_score: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Score de qualit√© de l'analyse (coverage)"
    )

    class Config:
        """Configuration Pydantic."""
        json_schema_extra = {
            "example": {
                "schema_version": 1,
                "task_summary": "Cr√©er une API REST pour g√©rer les utilisateurs",
                "complexity": "moderate",
                "complexity_score": 6,
                "estimated_duration_minutes": 45,
                "candidate_files": [
                    {
                        "path": "api/routes/users.py",
                        "action": "create",
                        "reason": "Nouvelles routes API utilisateurs",
                        "validation_status": "uncertain"
                    }
                ],
                "dependencies": [
                    {
                        "name": "fastapi",
                        "type": "package",
                        "version": ">=0.104.0",
                        "required": True
                    }
                ],
                "risks": [
                    {
                        "description": "Conflit avec routes existantes",
                        "level": "medium",
                        "mitigation": "V√©rifier les routes avant impl√©mentation",
                        "probability": 5
                    }
                ],
                "ambiguities": [],
                "missing_info": ["Sch√©ma de validation exact pour User"],
                "implementation_approach": "Cr√©er module API s√©par√© avec validation Pydantic",
                "test_strategy": "Tests unitaires + tests d'int√©gration API",
                "breaking_changes_risk": False,
                "requires_external_deps": False,
                "quality_score": 0.85
            }
        }


def create_requirements_analysis_chain(
    provider: str = "anthropic",
    model: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: int = 4000,
    max_retries: int = 2
):
    """
    Cr√©e une cha√Æne LCEL pour analyser les requirements de mani√®re structur√©e.

    Args:
        provider: Provider LLM √† utiliser ("anthropic" ou "openai")
        model: Nom du mod√®le (optionnel, utilise le d√©faut du provider)
        temperature: Temp√©rature du mod√®le (0.0-1.0)
        max_tokens: Nombre maximum de tokens
        max_retries: Nombre de tentatives en cas d'√©chec de validation

    Returns:
        Cha√Æne LCEL configur√©e (Prompt ‚Üí LLM ‚Üí Parser)

    Raises:
        ValueError: Si le provider n'est pas support√©
        Exception: Si les cl√©s API sont manquantes
    """
    logger.info(f"üîó Cr√©ation requirements_analysis_chain avec provider={provider}")

    # 1. Cr√©er le parser Pydantic
    parser = PydanticOutputParser(pydantic_object=RequirementsAnalysis)

    # 2. Cr√©er le prompt template avec instructions de formatage
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """Tu es un analyste technique expert qui analyse les requirements de projets logiciels.
Tu dois examiner la description de la t√¢che et g√©n√©rer une analyse structur√©e compl√®te au format JSON strict.

IMPORTANT: Tu DOIS retourner UNIQUEMENT du JSON valide, sans texte avant ou apr√®s.
Utilise le sch√©ma suivant:

{format_instructions}

Sois exhaustif dans ton analyse :
- Identifie TOUS les fichiers potentiellement concern√©s
- Liste TOUTES les d√©pendances n√©cessaires
- D√©tecte TOUS les risques possibles
- Signale TOUTES les ambigu√Øt√©s ou informations manquantes
- Estime la complexit√© de fa√ßon r√©aliste
- Propose une strat√©gie d'impl√©mentation claire

‚úÖ R√àGLES IMPORTANTES pour r√©duire les informations manquantes :
1. Si une information n'est pas fournie, propose une valeur par d√©faut raisonnable
2. Pour les crit√®res d'acceptation manquants, d√©duis-les du titre et de la description
3. Pour le contexte technique manquant, assume un contexte standard selon le type de projet
4. Pour les d√©pendances, liste les d√©pendances courantes m√™me si non mentionn√©es explicitement
5. Privil√©gie des estimations conservatrices plut√¥t que de signaler des manques"""),
        ("user", """Analyse cette t√¢che en d√©tail:

## INFORMATIONS DE LA T√ÇCHE

**Titre**: {task_title}
**Type**: {task_type}
**Priorit√©**: {priority}

**Description**:
{description}

**Crit√®res d'acceptation**:
{acceptance_criteria}

**Contexte technique**:
{technical_context}

**Fichiers mentionn√©s**:
{files_to_modify}

**Repository**: {repository_url}

## CONTEXTE ADDITIONNEL
{additional_context}

G√©n√®re une analyse compl√®te et structur√©e de cette t√¢che.""")
    ])

    # 3. Injecter les instructions de formatage du parser dans le prompt
    prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())

    # 4. Cr√©er le LLM selon le provider avec retry
    if provider.lower() == "anthropic":
        if not settings.anthropic_api_key:
            raise Exception("ANTHROPIC_API_KEY manquante dans la configuration")

        llm = ChatAnthropic(
            model=model or "claude-3-5-sonnet-20241022",
            anthropic_api_key=settings.anthropic_api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            max_retries=max_retries
        )
        logger.info(f"‚úÖ LLM Anthropic initialis√©: {model or 'claude-3-5-sonnet-20241022'}")

    elif provider.lower() == "openai":
        if not settings.openai_api_key:
            raise Exception("OPENAI_API_KEY manquante dans la configuration")

        llm = ChatOpenAI(
            model=model or "gpt-4",
            openai_api_key=settings.openai_api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            max_retries=max_retries
        )
        logger.info(f"‚úÖ LLM OpenAI initialis√©: {model or 'gpt-4'}")

    else:
        raise ValueError(f"Provider non support√©: {provider}. Utilisez 'anthropic' ou 'openai'")

    # 5. Cr√©er la cha√Æne LCEL: Prompt ‚Üí LLM ‚Üí Parser
    chain = prompt | llm | parser

    logger.info("‚úÖ Requirements analysis chain cr√©√©e avec succ√®s")
    return chain


async def generate_requirements_analysis(
    task_title: str,
    task_description: str,
    task_type: str = "feature",
    priority: str = "medium",
    acceptance_criteria: Optional[str] = None,
    technical_context: Optional[str] = None,
    files_to_modify: Optional[List[str]] = None,
    repository_url: Optional[str] = None,
    additional_context: Optional[Dict[str, Any]] = None,
    provider: str = "anthropic",
    fallback_to_openai: bool = True,
    validate_files: bool = True,
    run_step_id: Optional[int] = None
) -> RequirementsAnalysis:
    """
    G√©n√®re une analyse structur√©e des requirements avec fallback automatique.

    Args:
        task_title: Titre de la t√¢che
        task_description: Description d√©taill√©e
        task_type: Type de t√¢che (feature, bugfix, refactor, etc.)
        priority: Priorit√© de la t√¢che
        acceptance_criteria: Crit√®res d'acceptation
        technical_context: Contexte technique additionnel
        files_to_modify: Liste des fichiers √† modifier (si connus)
        repository_url: URL du repository
        additional_context: Contexte additionnel (dict)
        provider: Provider principal ("anthropic" ou "openai")
        fallback_to_openai: Si True, fallback vers OpenAI si le provider principal √©choue
        validate_files: Si True, valide l'existence des fichiers candidats

    Returns:
        RequirementsAnalysis valid√© par Pydantic

    Raises:
        Exception: Si tous les providers √©chouent
    """
    # Pr√©parer les inputs
    context_str = str(additional_context) if additional_context else "Aucun contexte additionnel"
    files_str = ", ".join(files_to_modify) if files_to_modify else "Non sp√©cifi√©s"

    inputs = {
        "task_title": task_title,
        "description": task_description,
        "task_type": task_type,
        "priority": priority,
        "acceptance_criteria": acceptance_criteria or "Non sp√©cifi√©s",
        "technical_context": technical_context or "Non sp√©cifi√©",
        "files_to_modify": files_str,
        "repository_url": repository_url or "Non sp√©cifi√©",
        "additional_context": context_str
    }

    # Cr√©er le callback pour enregistrer les interactions IA
    callbacks = []
    if run_step_id:
        from utils.langchain_db_callback import create_db_callback
        callbacks = [create_db_callback(run_step_id)]
        logger.debug(f"üìù Callback DB activ√© pour run_step_id={run_step_id}")

    # Tentative avec le provider principal
    try:
        logger.info(f"üöÄ G√©n√©ration analyse requirements avec {provider}...")
        chain = create_requirements_analysis_chain(provider=provider)
        analysis = await chain.ainvoke(inputs, config={"callbacks": callbacks})

        # Post-traitement: validation des fichiers
        if validate_files and analysis.candidate_files:
            _validate_candidate_files(analysis.candidate_files)

        # Calculer le score de qualit√©
        analysis.quality_score = _calculate_quality_score(analysis)

        logger.info(
            f"‚úÖ Analyse g√©n√©r√©e avec succ√®s: "
            f"{len(analysis.candidate_files)} fichiers, "
            f"{len(analysis.risks)} risques, "
            f"{len(analysis.ambiguities)} ambigu√Øt√©s, "
            f"quality_score={analysis.quality_score:.2f}"
        )
        return analysis

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec g√©n√©ration analyse avec {provider}: {e}")

        # Fallback vers OpenAI si configur√©
        if fallback_to_openai and provider.lower() != "openai":
            try:
                logger.info("üîÑ Fallback vers OpenAI...")
                chain_fallback = create_requirements_analysis_chain(provider="openai")
                analysis = await chain_fallback.ainvoke(inputs, config={"callbacks": callbacks})

                if validate_files and analysis.candidate_files:
                    _validate_candidate_files(analysis.candidate_files)

                analysis.quality_score = _calculate_quality_score(analysis)

                logger.info("‚úÖ Analyse g√©n√©r√©e avec succ√®s (fallback OpenAI)")
                return analysis

            except Exception as fallback_error:
                logger.error(f"‚ùå Fallback OpenAI √©chou√©: {fallback_error}")
                raise Exception(
                    f"Tous les providers ont √©chou√©. "
                    f"Principal: {e}, Fallback: {fallback_error}"
                )

        # Pas de fallback configur√©
        raise Exception(f"G√©n√©ration analyse √©chou√©e avec {provider}: {e}")


def _validate_candidate_files(files: List[CandidateFile]):
    """
    Valide l'existence des fichiers candidats.

    Args:
        files: Liste des fichiers candidats √† valider
    """
    for file in files:
        # Ne valider que les fichiers qui doivent exister (modify, delete)
        if file.action in ["modify", "delete"]:
            if os.path.exists(file.path):
                file.validation_status = FileValidationStatus.VALID
            else:
                file.validation_status = FileValidationStatus.NOT_FOUND
                logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {file.path}")
        elif file.action == "create":
            # Pour les cr√©ations, v√©rifier que le fichier n'existe pas d√©j√†
            if os.path.exists(file.path):
                file.validation_status = FileValidationStatus.UNCERTAIN
                logger.warning(f"‚ö†Ô∏è Fichier √† cr√©er existe d√©j√†: {file.path}")
            else:
                file.validation_status = FileValidationStatus.VALID


def _calculate_quality_score(analysis: RequirementsAnalysis) -> float:
    """
    Calcule un score de qualit√© pour l'analyse.

    ‚úÖ AM√âLIORATION: Seuil de qualit√© augment√© (0.70 ‚Üí 0.80)

    Le score est bas√© sur:
    - Pr√©sence et validit√© des fichiers candidats
    - Identification de risques
    - Gestion des d√©pendances
    - Compl√©tude de l'analyse
    - Clart√© des requirements (p√©nalit√©s pour ambigu√Øt√©s)

    Args:
        analysis: Analyse √† √©valuer

    Returns:
        Score entre 0.0 et 1.0 (minimum recommand√©: 0.80)
    """
    score = 0.0

    # ‚úÖ Points pour les fichiers candidats (max 0.35, augment√© de 0.30)
    if analysis.candidate_files:
        valid_files = sum(
            1 for f in analysis.candidate_files
            if f.validation_status == FileValidationStatus.VALID
        )
        uncertain_files = sum(
            1 for f in analysis.candidate_files
            if f.validation_status == FileValidationStatus.UNCERTAIN
        )
        total_files = len(analysis.candidate_files)

        # ‚úÖ FIX: Si tous les fichiers sont UNCERTAIN (pas encore valid√©s), donner un score partiel
        if uncertain_files == total_files:
            # Repository pas encore clon√©, donner cr√©dit pour avoir identifi√© les fichiers
            file_score = 0.30  # Score partiel pour identification
            logger.info(f"üìã {total_files} fichiers identifi√©s (validation en attente)")
        else:
            # Bonus si tous les fichiers sont valides
            file_score = (valid_files / total_files) * 0.35
            if valid_files == total_files and total_files >= 2:
                file_score += 0.05  # Bonus pour analyse compl√®te

        score += min(0.40, file_score)
    else:
        # ‚úÖ NOUVEAU: P√©nalit√© si aucun fichier identifi√©
        logger.warning("‚ö†Ô∏è Aucun fichier candidat identifi√© - qualit√© r√©duite")

    # ‚úÖ Points pour l'identification de risques (max 0.15)
    if analysis.risks:
        # Favoriser une analyse √©quilibr√©e des risques
        risk_score = min(0.15, len(analysis.risks) * 0.04)
        score += risk_score

    # ‚úÖ Points pour les d√©pendances (max 0.15)
    if analysis.dependencies:
        dep_score = min(0.15, len(analysis.dependencies) * 0.04)
        score += dep_score

    # ‚úÖ Points pour la compl√©tude (max 0.40, augment√© de 0.30)
    completeness = 0.0
    if analysis.implementation_approach and len(analysis.implementation_approach) > 20:
        completeness += 0.15  # Augment√© de 0.10
    if analysis.test_strategy and len(analysis.test_strategy) > 15:
        completeness += 0.15  # Augment√© de 0.10
    if analysis.estimated_duration_minutes > 0:
        completeness += 0.10
    score += completeness

    # ‚úÖ NOUVEAU: P√©nalit√©s pour r√©duire la qualit√© si ambigu√Øt√©s ou infos manquantes
    penalties = 0.0

    # P√©nalit√© pour ambigu√Øt√©s excessives
    if analysis.ambiguities and len(analysis.ambiguities) > 3:
        penalties += min(0.10, (len(analysis.ambiguities) - 3) * 0.03)
        logger.warning(f"‚ö†Ô∏è {len(analysis.ambiguities)} ambigu√Øt√©s d√©tect√©es - p√©nalit√© appliqu√©e")

    # P√©nalit√© pour informations manquantes critiques (‚úÖ AM√âLIORATION: Plus tol√©rant)
    if analysis.missing_info and len(analysis.missing_info) > 4:  # Augment√© de 2 √† 4
        penalties += min(0.08, (len(analysis.missing_info) - 4) * 0.02)  # P√©nalit√© r√©duite
        logger.warning(f"‚ö†Ô∏è {len(analysis.missing_info)} informations manquantes - p√©nalit√© r√©duite appliqu√©e")

    # Appliquer les p√©nalit√©s
    score = max(0.0, score - penalties)

    # ‚úÖ AM√âLIORATION: Seuil de qualit√© plus tol√©rant et logging am√©lior√©
    final_score = min(1.0, score)
    if final_score < 0.75:  # Seuil r√©duit de 0.80 √† 0.75
        logger.warning(
            f"‚ö†Ô∏è Score de qualit√© insuffisant: {final_score:.2f} < 0.75 "
            f"(fichiers: {len(analysis.candidate_files) if analysis.candidate_files else 0}, "
            f"ambigu√Øt√©s: {len(analysis.ambiguities)}, "
            f"infos manquantes: {len(analysis.missing_info)})"
        )
    elif final_score < 0.85:
        logger.info(
            f"üìä Score de qualit√© acceptable: {final_score:.2f} "
            f"(fichiers: {len(analysis.candidate_files) if analysis.candidate_files else 0}, "
            f"ambigu√Øt√©s: {len(analysis.ambiguities)}, "
            f"infos manquantes: {len(analysis.missing_info)})"
        )

    return final_score


def extract_analysis_metrics(analysis: RequirementsAnalysis) -> Dict[str, Any]:
    """
    Extrait les m√©triques d'une analyse de requirements.

    Args:
        analysis: Analyse valid√©e

    Returns:
        Dictionnaire de m√©triques
    """
    valid_files = sum(
        1 for f in analysis.candidate_files
        if f.validation_status == FileValidationStatus.VALID
    )

    high_risks = sum(
        1 for r in analysis.risks
        if r.level in [RiskLevel.HIGH, RiskLevel.CRITICAL]
    )

    return {
        "schema_version": analysis.schema_version,
        "complexity": analysis.complexity.value,
        "complexity_score": analysis.complexity_score,
        "estimated_duration_minutes": analysis.estimated_duration_minutes,
        "total_files": len(analysis.candidate_files),
        "valid_files": valid_files,
        "invalid_files": len(analysis.candidate_files) - valid_files,
        "file_coverage": valid_files / len(analysis.candidate_files) if analysis.candidate_files else 0,
        "total_dependencies": len(analysis.dependencies),
        "required_dependencies": sum(1 for d in analysis.dependencies if d.required),
        "total_risks": len(analysis.risks),
        "high_risks": high_risks,
        "risk_percentage": (high_risks / len(analysis.risks) * 100) if analysis.risks else 0,
        "total_ambiguities": len(analysis.ambiguities),
        "missing_info_count": len(analysis.missing_info),
        "quality_score": analysis.quality_score or 0.0,
        "breaking_changes_risk": analysis.breaking_changes_risk,
        "requires_external_deps": analysis.requires_external_deps
    }
