create sequence celery_taskmeta_id_seq
    as integer;

alter sequence celery_taskmeta_id_seq owner to admin;

grant select, update, usage on sequence celery_taskmeta_id_seq to ai_agent_app;

create sequence celery_tasksetmeta_id_seq
    as integer;

alter sequence celery_tasksetmeta_id_seq owner to admin;

grant select, update, usage on sequence celery_tasksetmeta_id_seq to ai_agent_app;

create sequence webhook_events_webhook_events_id_seq;

alter sequence webhook_events_webhook_events_id_seq owner to admin;

create sequence task_id_sequence;

alter sequence task_id_sequence owner to admin;

create sequence taskset_id_sequence;

alter sequence taskset_id_sequence owner to admin;

create table celery_taskmeta
(
    id        integer     default nextval('task_id_sequence'::regclass) not null
        primary key,
    task_id   varchar(155)                                              not null
        unique,
    status    varchar(50) default 'PENDING'::character varying          not null,
    result    bytea,
    date_done timestamp   default CURRENT_TIMESTAMP,
    traceback text,
    name      varchar(155),
    args      bytea,
    kwargs    bytea,
    worker    varchar(155),
    retries   integer     default 0,
    queue     varchar(155)
);

alter table celery_taskmeta
    owner to admin;

alter sequence celery_taskmeta_id_seq owned by celery_taskmeta.id;

create index celery_taskmeta_task_id_idx
    on celery_taskmeta (task_id);

create index celery_taskmeta_status_idx
    on celery_taskmeta (status);

create index celery_taskmeta_date_done_idx
    on celery_taskmeta (date_done);

grant delete, insert, references, select, trigger, truncate, update on celery_taskmeta to ai_agent_app;

create table celery_tasksetmeta
(
    id         integer   default nextval('taskset_id_sequence'::regclass) not null
        primary key,
    taskset_id varchar(155)                                               not null
        unique,
    result     bytea,
    date_done  timestamp default CURRENT_TIMESTAMP
);

alter table celery_tasksetmeta
    owner to admin;

alter sequence celery_tasksetmeta_id_seq owned by celery_tasksetmeta.id;

create index celery_tasksetmeta_taskset_id_idx
    on celery_tasksetmeta (taskset_id);

grant delete, insert, references, select, trigger, truncate, update on celery_tasksetmeta to ai_agent_app;

create table tasks
(
    tasks_id           bigint generated by default as identity
        primary key,
    monday_item_id     bigint                                                        not null
        unique,
    monday_board_id    bigint,
    title              varchar(500)                                                  not null,
    description        text,
    priority           varchar(50),
    repository_url     varchar(500)                                                  not null,
    repository_name    varchar(200),
    default_branch     varchar(100)             default 'main'::character varying,
    monday_status      varchar(100),
    internal_status    varchar(50)              default 'pending'::character varying not null
        constraint tasks_internal_status_chk
            check ((internal_status)::text = ANY
                   ((ARRAY ['pending'::character varying, 'processing'::character varying, 'testing'::character varying, 'debugging'::character varying, 'quality_check'::character varying, 'completed'::character varying, 'failed'::character varying])::text[])),
    created_by_user_id bigint,
    assigned_to        text,
    last_run_id        bigint,
    created_at         timestamp with time zone default now()                        not null,
    updated_at         timestamp with time zone default now()                        not null,
    started_at         timestamp with time zone,
    completed_at       timestamp with time zone
);

alter table tasks
    owner to admin;

create index idx_tasks_monday_item_id
    on tasks (monday_item_id);

create index idx_tasks_internal_status_partial
    on tasks (internal_status)
    where ((internal_status)::text = ANY
           ((ARRAY ['pending'::character varying, 'processing'::character varying])::text[]));

create table task_runs
(
    tasks_runs_id       bigint generated by default as identity
        primary key,
    task_id             bigint
                                                                                      references tasks
                                                                                          on delete set null,
    run_number          integer,
    status              varchar(50)              default 'started'::character varying not null
        constraint task_runs_status_chk
            check ((status)::text = ANY
                   ((ARRAY ['started'::character varying, 'running'::character varying, 'completed'::character varying, 'failed'::character varying, 'retry'::character varying])::text[])),
    celery_task_id      varchar(255)
        unique,
    current_node        varchar(100),
    progress_percentage integer                  default 0,
    ai_provider         varchar(50),
    model_name          varchar(100),
    result              jsonb,
    error_message       text,
    git_branch_name     varchar(255),
    pull_request_url    varchar(500),
    started_at          timestamp with time zone default now()                        not null,
    completed_at        timestamp with time zone,
    duration_seconds    integer
);

alter table task_runs
    owner to admin;

create index idx_task_runs_task_started
    on task_runs (task_id asc, started_at desc);

create index idx_task_runs_status
    on task_runs (status);

create index idx_task_runs_celery
    on task_runs (celery_task_id);

create unique index uq_task_runs_task_run_number
    on task_runs (task_id, run_number)
    where (task_id IS NOT NULL);

create table run_steps
(
    run_steps_id        bigint generated by default as identity
        primary key,
    task_run_id         bigint                                           not null
        references task_runs
            on delete cascade,
    node_name           varchar(100)                                     not null,
    step_order          integer                                          not null,
    status              varchar(50) default 'pending'::character varying not null
        constraint run_steps_status_chk
            check ((status)::text = ANY
                   ((ARRAY ['pending'::character varying, 'running'::character varying, 'completed'::character varying, 'failed'::character varying, 'skipped'::character varying, 'retry'::character varying])::text[])),
    retry_count         integer     default 0                            not null,
    max_retries         integer     default 3                            not null,
    input_data          jsonb,
    output_data         jsonb,
    output_log          text,
    error_details       text,
    started_at          timestamp with time zone,
    completed_at        timestamp with time zone,
    duration_seconds    integer,
    checkpoint_data     jsonb,
    checkpoint_saved_at timestamp with time zone
);

alter table run_steps
    owner to admin;

create index idx_run_steps_run_order
    on run_steps (task_run_id, step_order);

create index idx_run_steps_name_status
    on run_steps (node_name, status);

create index idx_run_steps_task_run
    on run_steps (task_run_id);

create index idx_run_steps_checkpoint
    on run_steps (run_steps_id)
    where (checkpoint_data IS NOT NULL);

create table ai_interactions
(
    ai_interactions_id bigint generated by default as identity
        primary key,
    run_step_id        bigint                                 not null
        references run_steps
            on delete cascade,
    ai_provider        varchar(50)                            not null,
    model_name         varchar(100)                           not null,
    prompt             text                                   not null,
    response           text,
    token_usage        jsonb,
    latency_ms         integer,
    created_at         timestamp with time zone default now() not null
);

alter table ai_interactions
    owner to admin;

create index idx_ai_interactions_step
    on ai_interactions (run_step_id);

create table ai_code_generations
(
    ai_code_generations_id bigint generated by default as identity
        primary key,
    task_run_id            bigint                                 not null
        references task_runs
            on delete cascade,
    provider               varchar(50)                            not null,
    model                  varchar(100)                           not null,
    generation_type        varchar(50),
    prompt                 text                                   not null,
    generated_code         text,
    tokens_used            integer,
    response_time_ms       integer,
    cost_estimate          numeric(12, 6),
    compilation_successful boolean,
    syntax_valid           boolean,
    files_modified         jsonb,
    generated_at           timestamp with time zone default now() not null
);

alter table ai_code_generations
    owner to admin;

create index idx_ai_code_gen_run
    on ai_code_generations (task_run_id);

create table test_results
(
    test_results_id      bigint generated by default as identity
        primary key,
    task_run_id          bigint                                                       not null
        unique
        references task_runs
            on delete cascade,
    passed               boolean                                                      not null,
    status               varchar(50)              default 'passed'::character varying not null,
    tests_total          integer                  default 0,
    tests_passed         integer                  default 0,
    tests_failed         integer                  default 0,
    tests_skipped        integer                  default 0,
    coverage_percentage  numeric(5, 2),
    pytest_report        jsonb,
    security_scan_report jsonb,
    executed_at          timestamp with time zone default now()                       not null,
    duration_seconds     integer
);

alter table test_results
    owner to admin;

create index idx_test_results_run
    on test_results (task_run_id);

create table pull_requests
(
    pull_requests_id bigint generated by default as identity
        primary key,
    task_id          bigint                                 not null
        references tasks
            on delete cascade,
    task_run_id      bigint
                                                            references task_runs
                                                                on delete set null,
    github_pr_number integer,
    github_pr_url    varchar(500),
    pr_title         varchar(500),
    pr_description   text,
    pr_status        varchar(50),
    mergeable        boolean,
    conflicts        boolean                  default false,
    reviews_required integer                  default 1,
    reviews_approved integer                  default 0,
    created_at       timestamp with time zone default now() not null,
    merged_at        timestamp with time zone,
    closed_at        timestamp with time zone,
    head_sha         char(40),
    base_branch      varchar(100)             default 'main'::character varying,
    feature_branch   varchar(100)
);

alter table pull_requests
    owner to admin;

create index idx_pr_task
    on pull_requests (task_id);

create index idx_pr_status
    on pull_requests (pr_status);

create index idx_pr_number
    on pull_requests (github_pr_number);

create table application_logs
(
    application_logs_id bigint generated by default as identity
        primary key,
    task_id             bigint
                                                               references tasks
                                                                   on delete set null,
    task_run_id         bigint
                                                               references task_runs
                                                                   on delete set null,
    run_step_id         bigint
                                                               references run_steps
                                                                   on delete set null,
    level               varchar(20)                            not null
        constraint application_logs_level_chk
            check ((level)::text = ANY
                   ((ARRAY ['DEBUG'::character varying, 'INFO'::character varying, 'WARNING'::character varying, 'ERROR'::character varying, 'CRITICAL'::character varying])::text[])),
    source_component    varchar(100),
    action              varchar(100),
    message             text                                   not null,
    metadata            jsonb,
    user_id             bigint,
    ip_address          inet,
    ts                  timestamp with time zone default now() not null
);

alter table application_logs
    owner to admin;

create table performance_metrics
(
    performance_metrics_id     bigint generated by default as identity
        primary key,
    task_id                    bigint
        references tasks
            on delete cascade,
    task_run_id                bigint
        references task_runs
            on delete cascade,
    total_duration_seconds     integer,
    queue_wait_time_seconds    integer,
    ai_processing_time_seconds integer,
    testing_time_seconds       integer,
    total_ai_calls             integer                  default 0,
    total_tokens_used          integer                  default 0,
    total_ai_cost              numeric(12, 6)           default 0.0,
    code_lines_generated       integer                  default 0,
    test_coverage_final        numeric(5, 2),
    security_issues_found      integer                  default 0,
    retry_attempts             integer                  default 0,
    success_rate               numeric(5, 2),
    recorded_at                timestamp with time zone default now() not null
);

alter table performance_metrics
    owner to admin;

create index idx_perf_task_run
    on performance_metrics (task_id, task_run_id);

create index idx_perf_recorded
    on performance_metrics (recorded_at desc);

create table system_config
(
    system_config_id bigint generated by default as identity
        primary key,
    key              varchar(100)                                                      not null
        unique,
    value            jsonb                                                             not null,
    description      text,
    config_type      varchar(50)              default 'application'::character varying not null,
    created_at       timestamp with time zone default now()                            not null,
    updated_at       timestamp with time zone default now()                            not null,
    updated_by       varchar(100)
);

alter table system_config
    owner to admin;

create table ai_usage_logs
(
    id               serial
        primary key,
    workflow_id      varchar(255)                          not null,
    task_id          varchar(255)                          not null,
    provider         varchar(50)                           not null,
    model            varchar(100)                          not null,
    operation        varchar(100)                          not null,
    input_tokens     integer                  default 0    not null,
    output_tokens    integer                  default 0    not null,
    total_tokens     integer                  default 0    not null,
    estimated_cost   numeric(10, 6)           default 0.0  not null
        constraint ai_usage_logs_cost_positive
            check (estimated_cost >= (0)::numeric),
    timestamp        timestamp with time zone default CURRENT_TIMESTAMP,
    duration_seconds numeric(8, 3),
    success          boolean                  default true not null,
    error_message    text,
    constraint ai_usage_logs_tokens_positive
        check ((input_tokens >= 0) AND (output_tokens >= 0) AND (total_tokens >= 0)),
    constraint ai_usage_logs_tokens_coherent
        check (total_tokens = (input_tokens + output_tokens))
);

comment on table ai_usage_logs is 'Logs des usages IA avec tracking des tokens et coûts';

comment on column ai_usage_logs.workflow_id is 'ID du workflow parent';

comment on column ai_usage_logs.task_id is 'ID de la tâche Monday.com';

comment on column ai_usage_logs.provider is 'Provider IA utilisé (claude, openai, etc.)';

comment on column ai_usage_logs.model is 'Modèle spécifique utilisé';

comment on column ai_usage_logs.operation is 'Type d''opération (analyze, implement, debug, etc.)';

comment on column ai_usage_logs.input_tokens is 'Nombre de tokens en input (prompt)';

comment on column ai_usage_logs.output_tokens is 'Nombre de tokens en output (réponse)';

comment on column ai_usage_logs.total_tokens is 'Total des tokens (input + output)';

comment on column ai_usage_logs.estimated_cost is 'Coût estimé en USD';

comment on constraint ai_usage_logs_cost_positive on ai_usage_logs is 'Les coûts ne peuvent pas être négatifs';

comment on column ai_usage_logs.duration_seconds is 'Durée de l''appel IA en secondes';

comment on constraint ai_usage_logs_tokens_positive on ai_usage_logs is 'Les tokens ne peuvent pas être négatifs';

comment on constraint ai_usage_logs_tokens_coherent on ai_usage_logs is 'total_tokens doit égaler input_tokens + output_tokens';

alter table ai_usage_logs
    owner to admin;

create index ai_usage_logs_workflow_id_idx
    on ai_usage_logs (workflow_id);

create index ai_usage_logs_task_id_idx
    on ai_usage_logs (task_id);

create index ai_usage_logs_provider_idx
    on ai_usage_logs (provider);

create index ai_usage_logs_timestamp_idx
    on ai_usage_logs (timestamp);

create index ai_usage_logs_workflow_timestamp_idx
    on ai_usage_logs (workflow_id, timestamp);

create table run_step_checkpoints
(
    checkpoint_id   bigint generated by default as identity
        primary key,
    step_id         bigint                                 not null
        references run_steps
            on delete cascade,
    checkpoint_data jsonb,
    created_at      timestamp with time zone default now() not null,
    updated_at      timestamp with time zone
);

alter table run_step_checkpoints
    owner to admin;

create index idx_checkpoints_step_id
    on run_step_checkpoints (step_id);

create index idx_checkpoints_created_at
    on run_step_checkpoints (created_at desc);

create table human_validations
(
    human_validations_id bigint generated by default as identity
        primary key,
    validation_id        varchar(100)                                                  not null
        unique,
    task_id              bigint                                                        not null
        references tasks
            on delete cascade,
    task_run_id          bigint
        references task_runs
            on delete cascade,
    run_step_id          bigint
        references run_steps
            on delete cascade,
    task_title           varchar(500)                                                  not null,
    task_description     text,
    original_request     text                                                          not null,
    status               varchar(50)              default 'pending'::character varying not null
        constraint human_validations_status_chk
            check ((status)::text = ANY
                   ((ARRAY ['pending'::character varying, 'approved'::character varying, 'rejected'::character varying, 'expired'::character varying, 'cancelled'::character varying])::text[])),
    generated_code       jsonb                                                         not null,
    code_summary         text                                                          not null,
    files_modified       text[]                                                        not null,
    implementation_notes text,
    test_results         jsonb,
    pr_info              jsonb,
    workflow_id          varchar(255),
    requested_by         varchar(100)             default 'ai_agent'::character varying,
    created_at           timestamp with time zone default now()                        not null,
    expires_at           timestamp with time zone
);

comment on table human_validations is 'Demandes de validation humaine pour les codes générés par l''IA';

comment on column human_validations.validation_id is 'ID unique généré par l''application pour tracking';

comment on column human_validations.generated_code is 'Code généré au format JSON {"filename": "content"}';

comment on column human_validations.files_modified is 'Array PostgreSQL des fichiers modifiés';

comment on column human_validations.expires_at is 'Date limite pour la validation (24h par défaut)';

alter table human_validations
    owner to admin;

create index idx_human_validations_validation_id
    on human_validations (validation_id);

create index idx_human_validations_task_id
    on human_validations (task_id);

create index idx_human_validations_status
    on human_validations (status);

create index idx_human_validations_expires_at
    on human_validations (expires_at)
    where (expires_at IS NOT NULL);

create index idx_human_validations_created_at
    on human_validations (created_at desc);

create index idx_human_validations_status_expires
    on human_validations (status, expires_at);

create table human_validation_responses
(
    human_validation_responses_id bigint generated by default as identity
        primary key,
    human_validation_id           bigint                                 not null
        references human_validations
            on delete cascade,
    validation_id                 varchar(100)                           not null,
    response_status               varchar(50)                            not null
        constraint human_validation_responses_status_chk
            check ((response_status)::text = ANY
                   ((ARRAY ['approved'::character varying, 'rejected'::character varying, 'expired'::character varying, 'cancelled'::character varying])::text[])),
    comments                      text,
    suggested_changes             text,
    approval_notes                text,
    validated_by                  varchar(100),
    validated_at                  timestamp with time zone default now() not null,
    should_merge                  boolean                  default false not null,
    should_continue_workflow      boolean                  default true  not null,
    validation_duration_seconds   integer,
    user_agent                    text,
    ip_address                    inet
);

comment on table human_validation_responses is 'Réponses des validateurs humains (approbation/rejet)';

alter table human_validation_responses
    owner to admin;

create index idx_human_validation_responses_validation_id
    on human_validation_responses (validation_id);

create index idx_human_validation_responses_status
    on human_validation_responses (response_status);

create index idx_human_validation_responses_validated_at
    on human_validation_responses (validated_at desc);

create index idx_human_validation_responses_validated_by
    on human_validation_responses (validated_by);

create table validation_actions
(
    validation_actions_id bigint generated by default as identity
        primary key,
    human_validation_id   bigint                                                        not null
        references human_validations
            on delete cascade,
    validation_id         varchar(100)                                                  not null,
    action_type           varchar(50)                                                   not null
        constraint validation_actions_type_chk
            check ((action_type)::text = ANY
                   ((ARRAY ['merge_pr'::character varying, 'reject_pr'::character varying, 'update_monday'::character varying, 'cleanup_branch'::character varying, 'notify_user'::character varying])::text[])),
    action_status         varchar(50)              default 'pending'::character varying not null
        constraint validation_actions_status_chk
            check ((action_status)::text = ANY
                   ((ARRAY ['pending'::character varying, 'in_progress'::character varying, 'completed'::character varying, 'failed'::character varying, 'cancelled'::character varying])::text[])),
    action_data           jsonb,
    result_data           jsonb,
    merge_commit_hash     varchar(100),
    merge_commit_url      varchar(500),
    created_at            timestamp with time zone default now()                        not null,
    started_at            timestamp with time zone,
    completed_at          timestamp with time zone,
    error_message         text,
    retry_count           integer                  default 0                            not null
);

comment on table validation_actions is 'Actions effectuées suite à la validation (merge, etc.)';

alter table validation_actions
    owner to admin;

create index idx_validation_actions_validation_id
    on validation_actions (validation_id);

create index idx_validation_actions_type_status
    on validation_actions (action_type, action_status);

create index idx_validation_actions_created_at
    on validation_actions (created_at desc);

create materialized view mv_dashboard_stats as
SELECT date_trunc('hour'::text, tasks.created_at)                     AS hour_bucket,
       tasks.internal_status,
       count(*)                                                       AS task_count,
       avg(EXTRACT(epoch FROM tasks.completed_at - tasks.started_at)) AS avg_duration_seconds
FROM tasks
WHERE tasks.created_at >= (now() - '7 days'::interval)
GROUP BY (date_trunc('hour'::text, tasks.created_at)), tasks.internal_status;

alter materialized view mv_dashboard_stats owner to admin;

create unique index mv_dashboard_stats_hour_bucket_internal_status_idx
    on mv_dashboard_stats (hour_bucket, internal_status);

create unique index mv_dashboard_stats_hour_bucket_internal_status_idx1
    on mv_dashboard_stats (hour_bucket, internal_status);

create materialized view mv_realtime_monitoring as
SELECT tasks.internal_status,
       count(*)                                                        AS count,
       avg(EXTRACT(epoch FROM now() - tasks.started_at) / 60::numeric) AS avg_minutes_since_start
FROM tasks
WHERE tasks.internal_status::text = ANY
      (ARRAY ['pending'::character varying, 'processing'::character varying, 'testing'::character varying, 'debugging'::character varying]::text[])
GROUP BY tasks.internal_status;

alter materialized view mv_realtime_monitoring owner to admin;

create unique index mv_realtime_monitoring_internal_status_idx
    on mv_realtime_monitoring (internal_status);

create unique index mv_realtime_monitoring_internal_status_idx1
    on mv_realtime_monitoring (internal_status);

create materialized view mv_cost_analysis as
SELECT date_trunc('day'::text, pm.recorded_at) AS day,
       tr.ai_provider,
       tr.model_name,
       sum(pm.total_ai_cost)                   AS daily_cost,
       count(*)                                AS runs_count,
       avg(pm.total_tokens_used)               AS avg_tokens
FROM performance_metrics pm
         JOIN task_runs tr ON tr.tasks_runs_id = pm.task_run_id
WHERE pm.recorded_at >= (now() - '30 days'::interval)
GROUP BY (date_trunc('day'::text, pm.recorded_at)), tr.ai_provider, tr.model_name;

alter materialized view mv_cost_analysis owner to admin;

create unique index mv_cost_analysis_day_ai_provider_model_name_idx
    on mv_cost_analysis (day, ai_provider, model_name);

create unique index mv_cost_analysis_day_ai_provider_model_name_idx1
    on mv_cost_analysis (day, ai_provider, model_name);

create view pg_stat_statements_info(dealloc, stats_reset) as
SELECT pg_stat_statements_info.dealloc,
       pg_stat_statements_info.stats_reset
FROM pg_stat_statements_info() pg_stat_statements_info(dealloc, stats_reset);

alter table pg_stat_statements_info
    owner to admin;

grant select on pg_stat_statements_info to public;

grant delete, insert, references, select, trigger, truncate, update on pg_stat_statements_info to ai_agent_app;

create view pg_stat_statements
            (userid, dbid, toplevel, queryid, query, plans, total_plan_time, min_plan_time, max_plan_time,
             mean_plan_time, stddev_plan_time, calls, total_exec_time, min_exec_time, max_exec_time, mean_exec_time,
             stddev_exec_time, rows, shared_blks_hit, shared_blks_read, shared_blks_dirtied, shared_blks_written,
             local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written, temp_blks_read, temp_blks_written,
             blk_read_time, blk_write_time, temp_blk_read_time, temp_blk_write_time, wal_records, wal_fpi, wal_bytes,
             jit_functions, jit_generation_time, jit_inlining_count, jit_inlining_time, jit_optimization_count,
             jit_optimization_time, jit_emission_count, jit_emission_time)
as
SELECT pg_stat_statements.userid,
       pg_stat_statements.dbid,
       pg_stat_statements.toplevel,
       pg_stat_statements.queryid,
       pg_stat_statements.query,
       pg_stat_statements.plans,
       pg_stat_statements.total_plan_time,
       pg_stat_statements.min_plan_time,
       pg_stat_statements.max_plan_time,
       pg_stat_statements.mean_plan_time,
       pg_stat_statements.stddev_plan_time,
       pg_stat_statements.calls,
       pg_stat_statements.total_exec_time,
       pg_stat_statements.min_exec_time,
       pg_stat_statements.max_exec_time,
       pg_stat_statements.mean_exec_time,
       pg_stat_statements.stddev_exec_time,
       pg_stat_statements.rows,
       pg_stat_statements.shared_blks_hit,
       pg_stat_statements.shared_blks_read,
       pg_stat_statements.shared_blks_dirtied,
       pg_stat_statements.shared_blks_written,
       pg_stat_statements.local_blks_hit,
       pg_stat_statements.local_blks_read,
       pg_stat_statements.local_blks_dirtied,
       pg_stat_statements.local_blks_written,
       pg_stat_statements.temp_blks_read,
       pg_stat_statements.temp_blks_written,
       pg_stat_statements.blk_read_time,
       pg_stat_statements.blk_write_time,
       pg_stat_statements.temp_blk_read_time,
       pg_stat_statements.temp_blk_write_time,
       pg_stat_statements.wal_records,
       pg_stat_statements.wal_fpi,
       pg_stat_statements.wal_bytes,
       pg_stat_statements.jit_functions,
       pg_stat_statements.jit_generation_time,
       pg_stat_statements.jit_inlining_count,
       pg_stat_statements.jit_inlining_time,
       pg_stat_statements.jit_optimization_count,
       pg_stat_statements.jit_optimization_time,
       pg_stat_statements.jit_emission_count,
       pg_stat_statements.jit_emission_time
FROM pg_stat_statements(true) pg_stat_statements(userid, dbid, toplevel, queryid, query, plans, total_plan_time,
                                                 min_plan_time, max_plan_time, mean_plan_time, stddev_plan_time, calls,
                                                 total_exec_time, min_exec_time, max_exec_time, mean_exec_time,
                                                 stddev_exec_time, rows, shared_blks_hit, shared_blks_read,
                                                 shared_blks_dirtied, shared_blks_written, local_blks_hit,
                                                 local_blks_read, local_blks_dirtied, local_blks_written,
                                                 temp_blks_read, temp_blks_written, blk_read_time, blk_write_time,
                                                 temp_blk_read_time, temp_blk_write_time, wal_records, wal_fpi,
                                                 wal_bytes, jit_functions, jit_generation_time, jit_inlining_count,
                                                 jit_inlining_time, jit_optimization_count, jit_optimization_time,
                                                 jit_emission_count, jit_emission_time);

alter table pg_stat_statements
    owner to admin;

grant select on pg_stat_statements to public;

grant delete, insert, references, select, trigger, truncate, update on pg_stat_statements to ai_agent_app;

create view dashboard_summary
            (task_id, title, monday_status, internal_status, priority, created_at, last_run_status, current_node,
             progress_percentage, github_pr_url, pr_status)
as
SELECT t.tasks_id AS task_id,
       t.title,
       t.monday_status,
       t.internal_status,
       t.priority,
       t.created_at,
       tr.status  AS last_run_status,
       tr.current_node,
       tr.progress_percentage,
       pr.github_pr_url,
       pr.pr_status
FROM tasks t
         LEFT JOIN LATERAL ( SELECT r.tasks_runs_id,
                                    r.task_id,
                                    r.run_number,
                                    r.status,
                                    r.celery_task_id,
                                    r.current_node,
                                    r.progress_percentage,
                                    r.ai_provider,
                                    r.model_name,
                                    r.result,
                                    r.error_message,
                                    r.git_branch_name,
                                    r.pull_request_url,
                                    r.started_at,
                                    r.completed_at,
                                    r.duration_seconds
                             FROM task_runs r
                             WHERE r.task_id = t.tasks_id
                             ORDER BY r.started_at DESC
                             LIMIT 1) tr ON true
         LEFT JOIN LATERAL ( SELECT p.pull_requests_id,
                                    p.task_id,
                                    p.task_run_id,
                                    p.github_pr_number,
                                    p.github_pr_url,
                                    p.pr_title,
                                    p.pr_description,
                                    p.pr_status,
                                    p.mergeable,
                                    p.conflicts,
                                    p.reviews_required,
                                    p.reviews_approved,
                                    p.created_at,
                                    p.merged_at,
                                    p.closed_at,
                                    p.head_sha,
                                    p.base_branch,
                                    p.feature_branch
                             FROM pull_requests p
                             WHERE p.task_id = t.tasks_id
                             ORDER BY p.created_at DESC
                             LIMIT 1) pr ON true
ORDER BY t.created_at DESC;

alter table dashboard_summary
    owner to admin;

create view performance_dashboard
            (date, total_tasks, completed_tasks, failed_tasks, avg_duration, avg_cost, avg_coverage) as
SELECT date_trunc('day'::text, t.created_at)                       AS date,
       count(t.tasks_id)                                           AS total_tasks,
       count(*) FILTER (WHERE tr.status::text = 'completed'::text) AS completed_tasks,
       count(*) FILTER (WHERE tr.status::text = 'failed'::text)    AS failed_tasks,
       avg(pm.total_duration_seconds)                              AS avg_duration,
       avg(pm.total_ai_cost)                                       AS avg_cost,
       avg(pm.test_coverage_final)                                 AS avg_coverage
FROM tasks t
         LEFT JOIN LATERAL ( SELECT r.tasks_runs_id,
                                    r.task_id,
                                    r.run_number,
                                    r.status,
                                    r.celery_task_id,
                                    r.current_node,
                                    r.progress_percentage,
                                    r.ai_provider,
                                    r.model_name,
                                    r.result,
                                    r.error_message,
                                    r.git_branch_name,
                                    r.pull_request_url,
                                    r.started_at,
                                    r.completed_at,
                                    r.duration_seconds
                             FROM task_runs r
                             WHERE r.task_id = t.tasks_id
                             ORDER BY r.started_at DESC
                             LIMIT 1) tr ON true
         LEFT JOIN performance_metrics pm ON pm.task_id = t.tasks_id
GROUP BY (date_trunc('day'::text, t.created_at))
ORDER BY (date_trunc('day'::text, t.created_at)) DESC;

alter table performance_dashboard
    owner to admin;

create view ai_cost_daily_summary
            (usage_date, provider, total_calls, total_input_tokens, total_output_tokens, total_tokens, total_cost,
             avg_cost_per_call, unique_workflows, unique_tasks)
as
SELECT date(ai_usage_logs."timestamp")           AS usage_date,
       ai_usage_logs.provider,
       count(*)                                  AS total_calls,
       sum(ai_usage_logs.input_tokens)           AS total_input_tokens,
       sum(ai_usage_logs.output_tokens)          AS total_output_tokens,
       sum(ai_usage_logs.total_tokens)           AS total_tokens,
       sum(ai_usage_logs.estimated_cost)         AS total_cost,
       avg(ai_usage_logs.estimated_cost)         AS avg_cost_per_call,
       count(DISTINCT ai_usage_logs.workflow_id) AS unique_workflows,
       count(DISTINCT ai_usage_logs.task_id)     AS unique_tasks
FROM ai_usage_logs
WHERE ai_usage_logs.success = true
GROUP BY (date(ai_usage_logs."timestamp")), ai_usage_logs.provider
ORDER BY (date(ai_usage_logs."timestamp")) DESC, (sum(ai_usage_logs.estimated_cost)) DESC;

comment on view ai_cost_daily_summary is 'Résumé quotidien des coûts IA par provider';

alter table ai_cost_daily_summary
    owner to admin;

create view ai_cost_by_workflow
            (workflow_id, task_id, total_ai_calls, total_input_tokens, total_output_tokens, total_tokens,
             total_workflow_cost, started_at, last_ai_call, duration_seconds, providers_used, operations_performed)
as
SELECT ai_usage_logs.workflow_id,
       ai_usage_logs.task_id,
       count(*)                                                                            AS total_ai_calls,
       sum(ai_usage_logs.input_tokens)                                                     AS total_input_tokens,
       sum(ai_usage_logs.output_tokens)                                                    AS total_output_tokens,
       sum(ai_usage_logs.total_tokens)                                                     AS total_tokens,
       sum(ai_usage_logs.estimated_cost)                                                   AS total_workflow_cost,
       min(ai_usage_logs."timestamp")                                                      AS started_at,
       max(ai_usage_logs."timestamp")                                                      AS last_ai_call,
       EXTRACT(epoch FROM max(ai_usage_logs."timestamp") - min(ai_usage_logs."timestamp")) AS duration_seconds,
       string_agg(DISTINCT ai_usage_logs.provider::text, ', '::text)                       AS providers_used,
       string_agg(DISTINCT ai_usage_logs.operation::text, ', '::text)                      AS operations_performed
FROM ai_usage_logs
WHERE ai_usage_logs.success = true
GROUP BY ai_usage_logs.workflow_id, ai_usage_logs.task_id
ORDER BY (sum(ai_usage_logs.estimated_cost)) DESC;

comment on view ai_cost_by_workflow is 'Coûts agrégés par workflow avec métriques de performance';

alter table ai_cost_by_workflow
    owner to admin;

create view validation_dashboard
            (human_validations_id, validation_id, task_title, status, created_at, expires_at, is_urgent,
             has_test_failures, files_count, pr_url, priority, repository_url, validated_by, validated_at,
             validation_comments)
as
SELECT hv.human_validations_id,
       hv.validation_id,
       hv.task_title,
       hv.status,
       hv.created_at,
       hv.expires_at,
       CASE
           WHEN hv.expires_at IS NOT NULL AND hv.expires_at < (now() + '01:00:00'::interval) THEN true
           ELSE false
           END                            AS is_urgent,
       CASE
           WHEN hv.test_results IS NOT NULL AND ((hv.test_results ->> 'success'::text)::boolean) = false THEN true
           ELSE false
           END                            AS has_test_failures,
       array_length(hv.files_modified, 1) AS files_count,
       hv.pr_info ->> 'url'::text         AS pr_url,
       t.priority,
       t.repository_url,
       hvr.validated_by,
       hvr.validated_at,
       hvr.comments                       AS validation_comments
FROM human_validations hv
         JOIN tasks t ON hv.task_id = t.tasks_id
         LEFT JOIN human_validation_responses hvr ON hv.human_validations_id = hvr.human_validation_id
ORDER BY (
             CASE
                 WHEN hv.status::text = 'pending'::text THEN 0
                 ELSE 1
                 END),
         (
             CASE
                 WHEN hv.expires_at IS NOT NULL AND hv.expires_at < (now() + '01:00:00'::interval) THEN 0
                 ELSE 1
                 END), hv.created_at DESC;

comment on view validation_dashboard is 'Vue optimisée pour l''interface d''administration des validations';

alter table validation_dashboard
    owner to admin;

create view validation_history
            (validation_id, task_title, status, created_at, expires_at, response_status, validated_by, validated_at,
             validation_duration_seconds, action_type, action_status, merge_commit_hash, repository_url, priority)
as
SELECT hv.validation_id,
       hv.task_title,
       hv.status,
       hv.created_at,
       hv.expires_at,
       hvr.response_status,
       hvr.validated_by,
       hvr.validated_at,
       hvr.validation_duration_seconds,
       va.action_type,
       va.action_status,
       va.merge_commit_hash,
       t.repository_url,
       t.priority
FROM human_validations hv
         JOIN tasks t ON hv.task_id = t.tasks_id
         LEFT JOIN human_validation_responses hvr ON hv.human_validations_id = hvr.human_validation_id
         LEFT JOIN validation_actions va ON hv.human_validations_id = va.human_validation_id
WHERE hv.status::text <> 'pending'::text
ORDER BY hv.created_at DESC;

comment on view validation_history is 'Historique complet des validations avec actions associées';

alter table validation_history
    owner to admin;

create function uuid_nil() returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_nil() owner to admin;

create function uuid_ns_dns() returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_ns_dns() owner to admin;

create function uuid_ns_url() returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_ns_url() owner to admin;

create function uuid_ns_oid() returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_ns_oid() owner to admin;

create function uuid_ns_x500() returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_ns_x500() owner to admin;

create function uuid_generate_v1() returns uuid
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_generate_v1() owner to admin;

create function uuid_generate_v1mc() returns uuid
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_generate_v1mc() owner to admin;

create function uuid_generate_v3(namespace uuid, name text) returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_generate_v3(uuid, text) owner to admin;

create function uuid_generate_v4() returns uuid
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_generate_v4() owner to admin;

create function uuid_generate_v5(namespace uuid, name text) returns uuid
    immutable
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function uuid_generate_v5(uuid, text) owner to admin;

create function pg_stat_statements_reset(userid oid default 0, dbid oid default 0, queryid bigint default 0) returns void
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function pg_stat_statements_reset(oid, oid, bigint) owner to admin;

create function pg_stat_statements_info(out dealloc bigint, out stats_reset timestamp with time zone) returns record
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;
$$;

alter function pg_stat_statements_info(out bigint, out timestamp with time zone) owner to admin;

create function pg_stat_statements(showtext boolean, out userid oid, out dbid oid, out toplevel boolean, out queryid bigint, out query text, out plans bigint, out total_plan_time double precision, out min_plan_time double precision, out max_plan_time double precision, out mean_plan_time double precision, out stddev_plan_time double precision, out calls bigint, out total_exec_time double precision, out min_exec_time double precision, out max_exec_time double precision, out mean_exec_time double precision, out stddev_exec_time double precision, out rows bigint, out shared_blks_hit bigint, out shared_blks_read bigint, out shared_blks_dirtied bigint, out shared_blks_written bigint, out local_blks_hit bigint, out local_blks_read bigint, out local_blks_dirtied bigint, out local_blks_written bigint, out temp_blks_read bigint, out temp_blks_written bigint, out blk_read_time double precision, out blk_write_time double precision, out temp_blk_read_time double precision, out temp_blk_write_time double precision, out wal_records bigint, out wal_fpi bigint, out wal_bytes numeric, out jit_functions bigint, out jit_generation_time double precision, out jit_inlining_count bigint, out jit_inlining_time double precision, out jit_optimization_count bigint, out jit_optimization_time double precision, out jit_emission_count bigint, out jit_emission_time double precision) returns setof setof record
    strict
    parallel safe
    language c
as
$$
begin
-- missing source code
end;

$$;

alter function pg_stat_statements(boolean, out oid, out oid, out boolean, out bigint, out text, out bigint, out double precision, out double precision, out double precision, out double precision, out double precision, out bigint, out double precision, out double precision, out double precision, out double precision, out double precision, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out bigint, out double precision, out double precision, out double precision, out double precision, out bigint, out bigint, out numeric, out bigint, out double precision, out bigint, out double precision, out bigint, out double precision, out bigint, out double precision) owner to admin;

create function cleanup_old_logs() returns void
    language plpgsql
as
$$
BEGIN
    -- Nettoyage existant
    DELETE FROM webhook_events WHERE received_at < NOW() - INTERVAL '6 months';
    DELETE FROM application_logs WHERE ts < NOW() - INTERVAL '3 months';
    
    -- Nouveau: nettoyer les anciennes validations
    DELETE FROM human_validations WHERE created_at < NOW() - INTERVAL '3 months';
    DELETE FROM validation_actions WHERE created_at < NOW() - INTERVAL '3 months';
    
    -- Marquer les validations expirées
    PERFORM mark_expired_validations();
END;
$$;

alter function cleanup_old_logs() owner to admin;

create function trg_touch_updated_at() returns trigger
    language plpgsql
as
$$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$;

alter function trg_touch_updated_at() owner to admin;

create trigger touch_tasks_updated_at
    before update
    on tasks
    for each row
execute procedure trg_touch_updated_at();

create trigger touch_system_config_updated_at
    before update
    on system_config
    for each row
execute procedure trg_touch_updated_at();

create function sync_task_last_run() returns trigger
    language plpgsql
as
$$
BEGIN
    UPDATE tasks 
    SET last_run_id = NEW.tasks_runs_id,
        updated_at = NOW()
    WHERE tasks_id = NEW.task_id;
    RETURN NEW;
END;
$$;

alter function sync_task_last_run() owner to admin;

create trigger tr_sync_task_last_run
    after insert or update
    on task_runs
    for each row
execute procedure sync_task_last_run();

create function calculate_duration() returns trigger
    language plpgsql
as
$$
BEGIN
    IF NEW.completed_at IS NOT NULL AND OLD.completed_at IS NULL THEN
        NEW.duration_seconds = EXTRACT(EPOCH FROM (NEW.completed_at - NEW.started_at))::INTEGER;
    END IF;
    RETURN NEW;
END;
$$;

alter function calculate_duration() owner to admin;

create trigger tr_calculate_run_duration
    before update
    on task_runs
    for each row
execute procedure calculate_duration();

create trigger tr_calculate_step_duration
    before update
    on run_steps
    for each row
execute procedure calculate_duration();

create function sync_task_status() returns trigger
    language plpgsql
as
$$
BEGIN
    IF NEW.status = 'completed' AND (OLD.status IS NULL OR OLD.status != 'completed') THEN
        UPDATE tasks 
        SET internal_status = 'completed',
            completed_at = NEW.completed_at,
            updated_at = NOW()
        WHERE tasks_id = NEW.task_id;

    ELSIF NEW.status = 'failed' THEN
        UPDATE tasks 
        SET internal_status = 'failed',
            updated_at = NOW()
        WHERE tasks_id = NEW.task_id
          AND last_run_id = NEW.tasks_runs_id;

    ELSIF NEW.status = 'running' AND (OLD.status IS NULL OR OLD.status = 'started') THEN
        UPDATE tasks 
        SET internal_status = 'processing',
            started_at = COALESCE(started_at, NEW.started_at),
            updated_at = NOW()
        WHERE tasks_id = NEW.task_id;
    END IF;

    RETURN NEW;
END;
$$;

alter function sync_task_status() owner to admin;

create trigger tr_sync_task_status
    after update
    on task_runs
    for each row
execute procedure sync_task_status();

create function validate_status_transition() returns trigger
    language plpgsql
as
$$
DECLARE
    valid_transitions JSONB := '{
        "pending": ["processing", "failed"],
        "processing": ["testing", "debugging", "completed", "failed"],
        "testing": ["quality_check", "debugging", "completed", "failed"],
        "debugging": ["testing", "completed", "failed"],
        "quality_check": ["completed", "failed"],
        "completed": [],
        "failed": ["pending", "processing"]
    }'::JSONB;
BEGIN
    -- ✅ CORRECTION CRITIQUE: Ignorer les transitions identiques (idempotentes)
    IF OLD.internal_status = NEW.internal_status THEN
        RETURN NEW;
    END IF;

    -- Valider les autres transitions
    IF OLD.internal_status IS NOT NULL AND 
       NOT (valid_transitions->OLD.internal_status ? NEW.internal_status) THEN
        RAISE EXCEPTION 'Invalid status transition from % to %', 
            OLD.internal_status, NEW.internal_status;
    END IF;
    
    RETURN NEW;
END;
$$;

alter function validate_status_transition() owner to admin;

create trigger tr_validate_task_status
    before update
    on tasks
    for each row
execute procedure validate_status_transition();

create function log_critical_changes() returns trigger
    language plpgsql
as
$$
BEGIN
    IF TG_TABLE_NAME = 'tasks' AND OLD.internal_status != NEW.internal_status THEN
        INSERT INTO application_logs (
            task_id, level, source_component, action, message, metadata
        ) VALUES (
            NEW.tasks_id, 'INFO', 'trigger', 'status_change',
            format('Task status changed from %s to %s', OLD.internal_status, NEW.internal_status),
            jsonb_build_object(
                'old_status', OLD.internal_status,
                'new_status', NEW.internal_status,
                'task_title', NEW.title
            )
        );
    END IF;
    RETURN NEW;
END;
$$;

alter function log_critical_changes() owner to admin;

create trigger tr_log_task_changes
    after update
    on tasks
    for each row
execute procedure log_critical_changes();

create function auto_cleanup() returns trigger
    language plpgsql
as
$$
BEGIN
    IF NEW.status = 'completed' THEN
        DELETE FROM task_runs 
        WHERE task_id = NEW.task_id 
          AND status IN ('completed', 'failed')
          AND tasks_runs_id NOT IN (
              SELECT tasks_runs_id FROM task_runs 
              WHERE task_id = NEW.task_id 
              ORDER BY started_at DESC 
              LIMIT 10
          );
    END IF;
    RETURN NEW;
END;
$$;

alter function auto_cleanup() owner to admin;

create trigger tr_auto_cleanup
    after update
    on task_runs
    for each row
execute procedure auto_cleanup();

create function optimize_database() returns void
    language plpgsql
as
$$
DECLARE
    table_name TEXT;
BEGIN
    FOR table_name IN 
        SELECT tablename FROM pg_tables 
        WHERE schemaname = 'public' 
          AND tablename IN ('tasks', 'task_runs', 'run_steps', 'ai_interactions')
    LOOP
        EXECUTE format('ANALYZE %I', table_name);
    END LOOP;

    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_dashboard_stats;
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_realtime_monitoring;
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_cost_analysis;

    INSERT INTO application_logs (level, source_component, action, message)
    VALUES ('INFO', 'maintenance', 'optimize_database', 'Database optimization completed');
END;
$$;

alter function optimize_database() owner to admin;

create function health_check()
    returns TABLE(metric_name text, metric_value numeric, status text)
    language plpgsql
as
$$
BEGIN
    -- Tâches en attente trop longtemps
    RETURN QUERY
    SELECT 'pending_tasks_old' as metric_name,
           COUNT(*)::NUMERIC as metric_value,
           CASE WHEN COUNT(*) > 100 THEN 'WARNING' ELSE 'OK' END as status
    FROM tasks 
    WHERE internal_status = 'pending' 
      AND created_at < NOW() - INTERVAL '1 hour';
    
    -- Utilisation de l'espace disque
    RETURN QUERY
    SELECT 'database_size_mb' as metric_name,
           pg_database_size(current_database())::NUMERIC / 1024 / 1024 as metric_value,
           'INFO' as status;
    
    -- Taux de succès des 24 dernières heures
    RETURN QUERY
    SELECT 'success_rate_24h' as metric_name,
           (COUNT(*) FILTER (WHERE tr.status = 'completed')::NUMERIC / NULLIF(COUNT(*), 0) * 100) as metric_value,
           CASE 
               WHEN (COUNT(*) FILTER (WHERE tr.status = 'completed')::NUMERIC / NULLIF(COUNT(*), 0) * 100) < 80 
               THEN 'WARNING' 
               ELSE 'OK' 
           END as status
    FROM tasks t
    LEFT JOIN task_runs tr ON tr.task_id = t.tasks_id AND tr.tasks_runs_id = t.last_run_id
    WHERE t.created_at >= NOW() - INTERVAL '24 hours';
    
END;
$$;

alter function health_check() owner to admin;

create function get_current_month_ai_stats()
    returns TABLE(provider_name character varying, total_cost numeric, total_tokens bigint, total_calls bigint, unique_workflows bigint, avg_cost_per_call numeric)
    language plpgsql
as
$$
BEGIN
    RETURN QUERY
    SELECT 
        provider as provider_name,
        SUM(estimated_cost)::DECIMAL(10, 6) as total_cost,
        SUM(total_tokens) as total_tokens,
        COUNT(*)::BIGINT as total_calls,
        COUNT(DISTINCT workflow_id)::BIGINT as unique_workflows,
        AVG(estimated_cost)::DECIMAL(10, 6) as avg_cost_per_call
    FROM ai_usage_logs
    WHERE EXTRACT(YEAR FROM timestamp) = EXTRACT(YEAR FROM CURRENT_DATE)
      AND EXTRACT(MONTH FROM timestamp) = EXTRACT(MONTH FROM CURRENT_DATE)
      AND success = true
    GROUP BY provider
    ORDER BY total_cost DESC;
END;
$$;

comment on function get_current_month_ai_stats() is 'Statistiques IA du mois en cours par provider';

alter function get_current_month_ai_stats() owner to admin;

create function get_expensive_workflows(cost_threshold numeric DEFAULT 1.0)
    returns TABLE(workflow_id character varying, task_id character varying, total_cost numeric, total_tokens bigint, ai_calls_count bigint, started_at timestamp with time zone, duration_minutes integer, providers_used text)
    language plpgsql
as
$$
BEGIN
    RETURN QUERY
    SELECT 
        w.workflow_id,
        w.task_id,
        w.total_workflow_cost,
        w.total_tokens,
        w.total_ai_calls,
        w.started_at,
        (w.duration_seconds / 60)::INTEGER as duration_minutes,
        w.providers_used
    FROM ai_cost_by_workflow w
    WHERE w.total_workflow_cost >= cost_threshold
    ORDER BY w.total_workflow_cost DESC
    LIMIT 50;
END;
$$;

comment on function get_expensive_workflows(numeric) is 'Trouve les workflows qui dépassent un seuil de coût';

alter function get_expensive_workflows(numeric) owner to admin;

create function mark_expired_validations() returns integer
    language plpgsql
as
$$
DECLARE
    expired_count INTEGER;
BEGIN
    UPDATE human_validations 
    SET status = 'expired', 
        updated_at = NOW()
    WHERE status = 'pending' 
      AND expires_at IS NOT NULL 
      AND expires_at < NOW();
    
    GET DIAGNOSTICS expired_count = ROW_COUNT;
    
    -- Log l'opération
    IF expired_count > 0 THEN
        INSERT INTO application_logs (level, component, message, metadata)
        VALUES ('INFO', 'human_validation', 'Marked expired validations', 
                jsonb_build_object('expired_count', expired_count));
    END IF;
    
    RETURN expired_count;
END;
$$;

comment on function mark_expired_validations() is 'Marque automatiquement les validations expirées';

alter function mark_expired_validations() owner to admin;

create function sync_validation_status() returns trigger
    language plpgsql
as
$$
BEGIN
    -- Quand une réponse est créée, mettre à jour le statut de la validation
    UPDATE human_validations 
    SET status = NEW.response_status,
        updated_at = NOW()
    WHERE human_validations_id = NEW.human_validation_id;
    
    RETURN NEW;
END;
$$;

alter function sync_validation_status() owner to admin;

create trigger sync_validation_status_trigger
    after insert
    on human_validation_responses
    for each row
execute procedure sync_validation_status();

create function get_validation_stats()
    returns TABLE(total_validations bigint, pending_validations bigint, approved_validations bigint, rejected_validations bigint, expired_validations bigint, avg_validation_time_minutes numeric, urgent_validations bigint)
    language plpgsql
as
$$
BEGIN
    RETURN QUERY
    SELECT 
        COUNT(*) as total_validations,
        COUNT(*) FILTER (WHERE status = 'pending') as pending_validations,
        COUNT(*) FILTER (WHERE status = 'approved') as approved_validations,
        COUNT(*) FILTER (WHERE status = 'rejected') as rejected_validations,
        COUNT(*) FILTER (WHERE status = 'expired') as expired_validations,
        ROUND(AVG(
            CASE 
                WHEN hvr.validated_at IS NOT NULL AND hv.created_at IS NOT NULL
                THEN EXTRACT(EPOCH FROM (hvr.validated_at - hv.created_at)) / 60.0
                ELSE NULL
            END
        ), 2) as avg_validation_time_minutes,
        COUNT(*) FILTER (
            WHERE status = 'pending' 
              AND expires_at IS NOT NULL 
              AND expires_at < NOW() + INTERVAL '1 hour'
        ) as urgent_validations
    FROM human_validations hv
    LEFT JOIN human_validation_responses hvr ON hv.human_validations_id = hvr.human_validation_id
    WHERE hv.created_at >= NOW() - INTERVAL '30 days'; -- Stats des 30 derniers jours
END;
$$;

comment on function get_validation_stats() is 'Retourne les statistiques de validation pour le dashboard';

alter function get_validation_stats() owner to admin;

-- Cyclic dependencies found

create table webhook_events_2025_09
    partition of webhook_events
        (
            primary key (webhook_events_id, received_at),
            constraint webhook_events_related_task_id_fkey
                foreign key (related_task_id) references tasks
                    on delete set null
            )
        FOR VALUES FROM ('2025-09-01 00:00:00+00') TO ('2025-10-01 00:00:00+00');

alter table webhook_events_2025_09
    owner to admin;

create index idx_webhook_events_processed_part
    on webhook_events_2025_09 (processed, received_at);

create index idx_webhook_events_source_part
    on webhook_events_2025_09 (source, event_type);

-- Cyclic dependencies found

create table webhook_events
(
    webhook_events_id bigint generated by default as identity,
    source            varchar(50)                                                   not null,
    event_type        varchar(100),
    payload           jsonb                                                         not null,
    headers           jsonb,
    signature         text,
    processed         boolean                  default false                        not null,
    processing_status varchar(50)              default 'pending'::character varying not null,
    error_message     text,
    related_task_id   bigint
                                                                                    references tasks
                                                                                        on delete set null,
    received_at       timestamp with time zone default now()                        not null,
    processed_at      timestamp with time zone,
    primary key (webhook_events_id, received_at)
)
    partition by RANGE (received_at);

alter table webhook_events
    owner to admin;

create table webhook_events_2025_10
    partition of webhook_events
        (
            primary key (webhook_events_id, received_at),
            constraint webhook_events_related_task_id_fkey
                foreign key (related_task_id) references tasks
                    on delete set null
            )
        FOR VALUES FROM ('2025-10-01 00:00:00+00') TO ('2025-11-01 00:00:00+00');

alter table webhook_events_2025_10
    owner to admin;


