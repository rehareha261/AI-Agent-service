# üìä Guide d'Impl√©mentation : Code Quality Feedback

**Date**: 12 octobre 2025  
**Table**: `code_quality_feedback`  
**Objectif**: Int√©grer le feedback sur la qualit√© du code g√©n√©r√© par IA dans le workflow

---

## üéØ Vue d'Ensemble

La table `code_quality_feedback` permet de collecter et analyser des feedbacks structur√©s sur la qualit√© du code g√©n√©r√© par l'IA. Ces feedbacks proviennent de **4 sources** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           SOURCES DE FEEDBACK QUALIT√â CODE               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. üß™ Tests automatiques (pytest, jest, etc.)          ‚îÇ
‚îÇ  2. üîç Linters (pylint, flake8, eslint, etc.)           ‚îÇ
‚îÇ  3. üë§ Validation humaine (code review)                  ‚îÇ
‚îÇ  4. üõ°Ô∏è Scans s√©curit√© (bandit, snyk, etc.)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìç Points d'Int√©gration dans le Workflow

### **Workflow LangGraph Actuel**

```
prepare_environment
        ‚Üì
analyze_requirements
        ‚Üì
implement_task (Claude g√©n√®re le code)
        ‚Üì
run_tests ‚Üê üéØ POINT 1: Feedback automatique tests
        ‚Üì
quality_assurance_automation ‚Üê üéØ POINT 2: Feedback linters/QA
        ‚Üì
finalize_pr
        ‚Üì
monday_validation ‚Üê üéØ POINT 3: Demande feedback humain
        ‚Üì
[Attente validation humaine]
        ‚Üì
merge_after_validation ‚Üê üéØ POINT 4: Enregistrer d√©cision finale
        ‚Üì
update_monday
```

---

## üîß Impl√©mentation D√©taill√©e

### **POINT 1 : Feedback depuis Tests Automatiques**

#### Fichier : `nodes/test_node.py`

**Emplacement** : Apr√®s l'ex√©cution des tests (ligne ~150-200)

```python
# Dans la fonction run_tests(), apr√®s l'ex√©cution des tests

async def run_tests(state: GraphState) -> GraphState:
    """Ex√©cute les tests et enregistre le feedback qualit√©."""
    
    # ... code existant pour ex√©cuter les tests ...
    
    # Apr√®s r√©cup√©ration des r√©sultats de tests
    test_results = await testing_engine._run_all_tests(
        working_directory=working_directory,
        include_coverage=True,
        code_changes=code_changes
    )
    
    # üéØ NOUVEAU: Enregistrer feedback qualit√© depuis les tests
    if test_results:
        await _save_test_quality_feedback(state, test_results)
    
    return state


async def _save_test_quality_feedback(state: GraphState, test_results: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualit√© bas√© sur les r√©sultats de tests.
    
    Args:
        state: √âtat du workflow
        test_results: R√©sultats des tests ex√©cut√©s
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("üìä Enregistrement du feedback qualit√© depuis les tests")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Extraire les m√©triques depuis test_results
    total_tests = test_results.get('total', 0)
    passed_tests = test_results.get('passed', 0)
    failed_tests = test_results.get('failed', 0)
    code_coverage = test_results.get('coverage_percent', 0)
    
    # Calculer un score de correctness bas√© sur les tests
    if total_tests > 0:
        correctness_score = int((passed_tests / total_tests) * 5)  # Score sur 5
    else:
        correctness_score = 3  # Score neutre si pas de tests
    
    # R√©cup√©rer l'ID de g√©n√©ration de code
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("‚ö†Ô∏è Impossible d'enregistrer feedback: IDs manquants")
        return
    
    # Construire le feedback
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "automated_tests",
        "feedback_source": test_results.get('framework', 'pytest'),
        
        # Ratings
        "overall_rating": correctness_score,
        "code_correctness": correctness_score,
        "code_testability": 4 if passed_tests > 0 else 2,
        
        # M√©triques
        "code_coverage_percent": code_coverage,
        
        # D√©cision
        "code_accepted": (failed_tests == 0 and passed_tests > 0),
        "requires_rework": (failed_tests > 0),
        
        # D√©tails
        "comments": f"{passed_tests}/{total_tests} tests pass√©s, couverture: {code_coverage}%",
        "issues_found": [f"test_failure:{test['name']}" for test in test_results.get('failures', [])],
        
        # M√©tadonn√©es
        "metadata": {
            "test_framework": test_results.get('framework'),
            "execution_time_ms": test_results.get('duration_ms'),
            "test_count": total_tests,
            "passed_count": passed_tests,
            "failed_count": failed_tests
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"‚úÖ Feedback tests enregistr√©: feedback_id={feedback_id}")
        
        # Stocker l'ID dans l'√©tat pour r√©f√©rence
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement feedback tests: {e}", exc_info=True)
```

---

### **POINT 2 : Feedback depuis Linters/QA**

#### Fichier : `nodes/qa_node.py`

**Emplacement** : Apr√®s l'ex√©cution des linters (ligne ~500-600)

```python
# Dans la fonction quality_assurance_automation()

async def quality_assurance_automation(state: GraphState) -> GraphState:
    """Ex√©cute QA et enregistre le feedback qualit√©."""
    
    # ... code existant pour QA ...
    
    # Apr√®s ex√©cution de tous les linters
    qa_results = {
        "pylint_score": pylint_result.get('score', 0),
        "flake8_issues": len(flake8_result.get('issues', [])),
        "black_formatted": black_result.get('formatted', False),
        "security_issues": len(bandit_result.get('issues', [])),
        # ... autres r√©sultats QA ...
    }
    
    # üéØ NOUVEAU: Enregistrer feedback qualit√© depuis QA
    await _save_qa_quality_feedback(state, qa_results)
    
    return state


async def _save_qa_quality_feedback(state: GraphState, qa_results: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualit√© bas√© sur les r√©sultats QA.
    
    Args:
        state: √âtat du workflow
        qa_results: R√©sultats des outils QA/linters
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("üìä Enregistrement du feedback qualit√© depuis QA")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Calculer scores bas√©s sur les r√©sultats QA
    pylint_score = qa_results.get('pylint_score', 0)
    flake8_issues = qa_results.get('flake8_issues', 0)
    security_issues = qa_results.get('security_issues', 0)
    
    # Convertir pylint score (0-10) en rating (1-5)
    style_rating = max(1, min(5, int((pylint_score / 10) * 5)))
    
    # Score de s√©curit√© bas√© sur les issues
    security_rating = 5 if security_issues == 0 else max(1, 5 - (security_issues // 2))
    
    # R√©cup√©rer les IDs
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("‚ö†Ô∏è Impossible d'enregistrer feedback QA: IDs manquants")
        return
    
    # Construire le feedback
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "linter",
        "feedback_source": "qa_automation",
        
        # Ratings
        "overall_rating": int((style_rating + security_rating) / 2),
        "code_style": style_rating,
        "code_security": security_rating,
        "code_maintainability": style_rating,  # Approximation
        
        # M√©triques
        "security_score": security_rating,
        
        # D√©cision
        "code_accepted": (flake8_issues == 0 and security_issues == 0),
        "requires_rework": (flake8_issues > 10 or security_issues > 0),
        "rework_priority": "high" if security_issues > 0 else "medium" if flake8_issues > 10 else "low",
        
        # D√©tails
        "comments": f"Pylint: {pylint_score}/10, Flake8: {flake8_issues} issues, S√©curit√©: {security_issues} issues",
        "issues_found": [
            f"flake8:{issue}" for issue in qa_results.get('flake8_issues', [])[:5]
        ] + [
            f"security:{issue}" for issue in qa_results.get('security_issues', [])[:5]
        ],
        "positive_aspects": qa_results.get('positive_aspects', []),
        
        # M√©tadonn√©es
        "metadata": {
            "pylint_score": pylint_score,
            "flake8_issues_count": flake8_issues,
            "security_issues_count": security_issues,
            "black_formatted": qa_results.get('black_formatted', False),
            "tools_used": ["pylint", "flake8", "black", "bandit"]
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"‚úÖ Feedback QA enregistr√©: feedback_id={feedback_id}")
        
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement feedback QA: {e}", exc_info=True)
```

---

### **POINT 3 : Feedback depuis Validation Humaine**

#### Fichier : `nodes/monday_validation_node.py`

**Emplacement** : Lors de la r√©ception de la r√©ponse de validation (ligne ~200-300)

```python
# Dans monday_validation() ou lors du traitement de la r√©ponse

async def process_validation_response(validation_response: Dict[str, Any], state: GraphState) -> None:
    """
    Traite la r√©ponse de validation humaine et enregistre le feedback.
    
    Args:
        validation_response: R√©ponse de l'utilisateur depuis Monday
        state: √âtat du workflow
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("üìä Enregistrement du feedback qualit√© depuis validation humaine")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Extraire les informations de la validation
    approved = validation_response.get('approved', False)
    response_text = validation_response.get('response_text', '')
    reviewer_monday_user_id = validation_response.get('user_id')
    
    # Parser la r√©ponse pour extraire des ratings (si fournis)
    # Ex: "Code correct mais style √† am√©liorer - 4/5"
    # On peut utiliser un LLM pour analyser le feedback textuel
    
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("‚ö†Ô∏è Impossible d'enregistrer feedback humain: IDs manquants")
        return
    
    # Rating bas√© sur l'approbation
    overall_rating = 5 if approved else 3
    
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "human",
        "feedback_source": "monday_validation",
        
        # Ratings
        "overall_rating": overall_rating,
        
        # D√©cision
        "code_accepted": approved,
        "requires_rework": not approved,
        "rework_priority": "high" if not approved else None,
        
        # D√©tails
        "comments": response_text,
        "reviewer_name": validation_response.get('reviewer_name', 'Unknown'),
        
        # M√©tadonn√©es
        "metadata": {
            "validation_id": validation_response.get('validation_id'),
            "monday_user_id": reviewer_monday_user_id,
            "response_type": validation_response.get('response_type'),
            "validation_timestamp": validation_response.get('timestamp')
        }
    }
    
    # üéØ OPTIONNEL: Utiliser un LLM pour analyser le feedback textuel
    if response_text:
        analyzed_feedback = await _analyze_human_feedback_with_llm(response_text)
        if analyzed_feedback:
            feedback_data.update({
                "code_correctness": analyzed_feedback.get('correctness_score'),
                "code_style": analyzed_feedback.get('style_score'),
                "code_maintainability": analyzed_feedback.get('maintainability_score'),
                "positive_aspects": analyzed_feedback.get('positive_aspects', []),
                "issues_found": analyzed_feedback.get('issues_found', []),
                "suggestions": analyzed_feedback.get('suggestions')
            })
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"‚úÖ Feedback humain enregistr√©: feedback_id={feedback_id}")
        
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement feedback humain: {e}", exc_info=True)


async def _analyze_human_feedback_with_llm(feedback_text: str) -> Dict[str, Any]:
    """
    Utilise un LLM pour analyser le feedback textuel humain.
    
    Args:
        feedback_text: Commentaire de l'utilisateur
        
    Returns:
        Dictionnaire avec scores et aspects extraits
    """
    from tools.claude_code_tool import ClaudeCodeTool
    
    claude = ClaudeCodeTool()
    
    prompt = f"""Analyse ce feedback de code review et extrais les informations structur√©es:

Feedback: "{feedback_text}"

Extrais:
1. Score de correctness (1-5)
2. Score de style (1-5)
3. Score de maintenabilit√© (1-5)
4. Points positifs (liste)
5. Probl√®mes identifi√©s (liste)
6. Suggestions d'am√©lioration

R√©ponds en JSON:
{{
  "correctness_score": int,
  "style_score": int,
  "maintainability_score": int,
  "positive_aspects": [str],
  "issues_found": [str],
  "suggestions": str
}}
"""
    
    try:
        response = await claude.execute(prompt)
        import json
        return json.loads(response)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur analyse LLM du feedback: {e}")
        return {}
```

---

### **POINT 4 : Feedback Final apr√®s Merge**

#### Fichier : `nodes/merge_node.py`

**Emplacement** : Apr√®s le merge r√©ussi (ligne ~100-150)

```python
# Dans la fonction merge_after_validation()

async def merge_after_validation(state: GraphState) -> GraphState:
    """Merge la PR et enregistre le feedback final."""
    
    # ... code existant pour merger ...
    
    # Apr√®s merge r√©ussi
    if merge_successful:
        # üéØ NOUVEAU: Enregistrer feedback final
        await _save_final_quality_feedback(state, merge_result)
    
    return state


async def _save_final_quality_feedback(state: GraphState, merge_result: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualit√© final apr√®s merge.
    
    Args:
        state: √âtat du workflow
        merge_result: R√©sultat du merge
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("üìä Enregistrement du feedback qualit√© final")
    
    feedback_service = CodeQualityFeedbackService()
    
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("‚ö†Ô∏è Impossible d'enregistrer feedback final: IDs manquants")
        return
    
    # Agr√©ger tous les feedbacks pr√©c√©dents
    previous_feedbacks = state["results"].get("quality_feedback_ids", [])
    
    # Calculer un score global bas√© sur le succ√®s du merge
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "code_review",
        "feedback_source": "merge_final",
        
        # Ratings
        "overall_rating": 5,  # Merge r√©ussi = code accept√©
        "code_correctness": 5,
        "code_style": 4,
        "code_security": 4,
        "code_maintainability": 4,
        
        # D√©cision
        "code_accepted": True,
        "requires_rework": False,
        
        # D√©tails
        "comments": f"Code merg√© avec succ√®s dans la branche principale. {len(previous_feedbacks)} feedbacks collect√©s.",
        "positive_aspects": ["merge_successful", "all_checks_passed", "validated_by_human"],
        
        # M√©tadonn√©es
        "metadata": {
            "merge_commit_sha": merge_result.get('commit_sha'),
            "pr_url": merge_result.get('pr_url'),
            "merged_at": merge_result.get('merged_at'),
            "previous_feedbacks_count": len(previous_feedbacks),
            "previous_feedback_ids": previous_feedbacks
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"‚úÖ Feedback final enregistr√©: feedback_id={feedback_id}")
        
        # Mettre √† jour tous les feedbacks pr√©c√©dents pour les lier au final
        await feedback_service.link_feedbacks_to_final(
            previous_feedback_ids=previous_feedbacks,
            final_feedback_id=feedback_id
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement feedback final: {e}", exc_info=True)
```

---

## üõ†Ô∏è Service D√©di√© : `CodeQualityFeedbackService`

### Fichier : `services/code_quality_feedback_service.py` (√Ä cr√©er)

```python
"""Service de gestion des feedbacks qualit√© code."""

import asyncpg
from typing import Dict, Any, List, Optional
from utils.logger import get_logger

logger = get_logger(__name__)


class CodeQualityFeedbackService:
    """Service pour enregistrer et g√©rer les feedbacks qualit√© code."""
    
    def __init__(self):
        """Initialise le service."""
        from services.database_persistence_service import db_persistence
        self.db = db_persistence
    
    async def save_feedback(self, feedback_data: Dict[str, Any]) -> int:
        """
        Enregistre un nouveau feedback qualit√©.
        
        Args:
            feedback_data: Donn√©es du feedback
            
        Returns:
            ID du feedback cr√©√©
        """
        query = """
            INSERT INTO code_quality_feedback (
                ai_code_generation_id,
                task_run_id,
                overall_rating,
                code_correctness,
                code_style,
                code_efficiency,
                code_security,
                code_maintainability,
                code_testability,
                feedback_type,
                feedback_source,
                comments,
                positive_aspects,
                issues_found,
                suggestions,
                code_accepted,
                requires_rework,
                rework_priority,
                lines_of_code,
                cyclomatic_complexity,
                code_coverage_percent,
                performance_score,
                security_score,
                reviewer_name,
                review_duration_seconds,
                review_tool_used,
                metadata
            ) VALUES (
                $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                $21, $22, $23, $24, $25, $26, $27
            )
            RETURNING feedback_id
        """
        
        try:
            async with self.db.pool.acquire() as conn:
                result = await conn.fetchrow(
                    query,
                    feedback_data.get('ai_code_generation_id'),
                    feedback_data.get('task_run_id'),
                    feedback_data.get('overall_rating'),
                    feedback_data.get('code_correctness'),
                    feedback_data.get('code_style'),
                    feedback_data.get('code_efficiency'),
                    feedback_data.get('code_security'),
                    feedback_data.get('code_maintainability'),
                    feedback_data.get('code_testability'),
                    feedback_data.get('feedback_type'),
                    feedback_data.get('feedback_source'),
                    feedback_data.get('comments'),
                    feedback_data.get('positive_aspects'),
                    feedback_data.get('issues_found'),
                    feedback_data.get('suggestions'),
                    feedback_data.get('code_accepted'),
                    feedback_data.get('requires_rework'),
                    feedback_data.get('rework_priority'),
                    feedback_data.get('lines_of_code'),
                    feedback_data.get('cyclomatic_complexity'),
                    feedback_data.get('code_coverage_percent'),
                    feedback_data.get('performance_score'),
                    feedback_data.get('security_score'),
                    feedback_data.get('reviewer_name'),
                    feedback_data.get('review_duration_seconds'),
                    feedback_data.get('review_tool_used'),
                    feedback_data.get('metadata')
                )
                
                feedback_id = result['feedback_id']
                logger.info(f"‚úÖ Feedback enregistr√©: ID={feedback_id}, type={feedback_data.get('feedback_type')}")
                return feedback_id
                
        except Exception as e:
            logger.error(f"‚ùå Erreur enregistrement feedback: {e}", exc_info=True)
            raise
    
    async def get_feedbacks_for_run(self, task_run_id: int) -> List[Dict[str, Any]]:
        """
        R√©cup√®re tous les feedbacks d'un run.
        
        Args:
            task_run_id: ID du task run
            
        Returns:
            Liste des feedbacks
        """
        query = """
            SELECT * FROM code_quality_feedback
            WHERE task_run_id = $1
            ORDER BY created_at ASC
        """
        
        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(query, task_run_id)
            return [dict(row) for row in rows]
    
    async def get_aggregate_stats(self, task_run_id: int) -> Dict[str, Any]:
        """
        Calcule les statistiques agr√©g√©es des feedbacks.
        
        Args:
            task_run_id: ID du task run
            
        Returns:
            Statistiques agr√©g√©es
        """
        query = """
            SELECT 
                COUNT(*) as feedback_count,
                AVG(overall_rating) as avg_overall_rating,
                AVG(code_correctness) as avg_correctness,
                AVG(code_style) as avg_style,
                AVG(code_security) as avg_security,
                SUM(CASE WHEN code_accepted THEN 1 ELSE 0 END) as accepted_count,
                SUM(CASE WHEN requires_rework THEN 1 ELSE 0 END) as rework_count
            FROM code_quality_feedback
            WHERE task_run_id = $1
        """
        
        async with self.db.pool.acquire() as conn:
            result = await conn.fetchrow(query, task_run_id)
            return dict(result) if result else {}
    
    async def link_feedbacks_to_final(
        self, 
        previous_feedback_ids: List[int], 
        final_feedback_id: int
    ) -> None:
        """
        Lie les feedbacks pr√©c√©dents au feedback final.
        
        Args:
            previous_feedback_ids: IDs des feedbacks pr√©c√©dents
            final_feedback_id: ID du feedback final
        """
        query = """
            UPDATE code_quality_feedback
            SET metadata = COALESCE(metadata, '{}'::jsonb) || 
                jsonb_build_object('final_feedback_id', $1)
            WHERE feedback_id = ANY($2)
        """
        
        async with self.db.pool.acquire() as conn:
            await conn.execute(query, final_feedback_id, previous_feedback_ids)
            logger.info(f"‚úÖ Feedbacks li√©s au final: {len(previous_feedback_ids)} ‚Üí {final_feedback_id}")


# Instance globale
code_quality_feedback_service = CodeQualityFeedbackService()
```

---

## üìä Utilisation des Donn√©es

### **Dashboard de Qualit√© Code**

```python
# services/analytics_service.py

async def get_code_quality_dashboard(days: int = 30) -> Dict[str, Any]:
    """G√©n√®re un dashboard de qualit√© code."""
    
    query = """
        SELECT 
            DATE(created_at) as date,
            feedback_type,
            AVG(overall_rating) as avg_rating,
            AVG(code_correctness) as avg_correctness,
            AVG(code_style) as avg_style,
            AVG(code_security) as avg_security,
            COUNT(*) as feedback_count,
            SUM(CASE WHEN code_accepted THEN 1 ELSE 0 END) as accepted_count
        FROM code_quality_feedback
        WHERE created_at > NOW() - INTERVAL '{days} days'
        GROUP BY DATE(created_at), feedback_type
        ORDER BY date DESC
    """
    
    # Ex√©cuter et formatter les r√©sultats
    ...
```

### **Am√©lioration des Prompts**

```python
# Utiliser les feedbacks pour am√©liorer les prompts

async def analyze_prompt_performance():
    """Analyse la performance des prompts bas√©e sur les feedbacks."""
    
    query = """
        SELECT 
            apt.template_id,
            apt.name,
            AVG(cqf.overall_rating) as avg_quality_rating,
            AVG(cqf.code_correctness) as avg_correctness,
            COUNT(*) as usage_count
        FROM code_quality_feedback cqf
        JOIN ai_code_generations acg ON cqf.ai_code_generation_id = acg.ai_code_generations_id
        JOIN ai_prompt_usage apu ON acg.task_run_id = apu.task_run_id
        JOIN ai_prompt_templates apt ON apu.template_id = apt.template_id
        WHERE cqf.created_at > NOW() - INTERVAL '7 days'
        GROUP BY apt.template_id, apt.name
        ORDER BY avg_quality_rating DESC
    """
    
    # Identifier les prompts √† am√©liorer
    ...
```

---

## ‚úÖ Checklist d'Impl√©mentation

- [ ] **1. Cr√©er le service** : `services/code_quality_feedback_service.py`
- [ ] **2. Modifier `test_node.py`** : Ajouter `_save_test_quality_feedback()`
- [ ] **3. Modifier `qa_node.py`** : Ajouter `_save_qa_quality_feedback()`
- [ ] **4. Modifier `monday_validation_node.py`** : Ajouter `process_validation_response()`
- [ ] **5. Modifier `merge_node.py`** : Ajouter `_save_final_quality_feedback()`
- [ ] **6. Appliquer la migration SQL** : `data/migration_conversational_features.sql`
- [ ] **7. Cr√©er les tests unitaires** : `tests/test_code_quality_feedback.py`
- [ ] **8. Mettre √† jour `GraphState`** : Ajouter `quality_feedback_ids` dans results
- [ ] **9. Cr√©er dashboard** : Vue analytics des feedbacks
- [ ] **10. Documentation** : README avec exemples d'utilisation

---

## üéØ B√©n√©fices Attendus

1. ‚úÖ **Tra√ßabilit√© compl√®te** de la qualit√© du code g√©n√©r√©
2. ‚úÖ **Am√©lioration continue** des prompts IA bas√©e sur feedback r√©el
3. ‚úÖ **Identification des patterns** de code probl√©matiques
4. ‚úÖ **M√©triques de performance** de l'IA sur le long terme
5. ‚úÖ **Confiance utilisateur** augment√©e (transparence)
6. ‚úÖ **Base de donn√©es** pour machine learning futur

---

**Pr√™t √† impl√©menter ! üöÄ**
