# ğŸ“Š Guide d'ImplÃ©mentation : Code Quality Feedback

**Date**: 12 octobre 2025  
**Table**: `code_quality_feedback`  
**Objectif**: IntÃ©grer le feedback sur la qualitÃ© du code gÃ©nÃ©rÃ© par IA dans le workflow

---

## ğŸ¯ Vue d'Ensemble

La table `code_quality_feedback` permet de collecter et analyser des feedbacks structurÃ©s sur la qualitÃ© du code gÃ©nÃ©rÃ© par l'IA. Ces feedbacks proviennent de **4 sources** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SOURCES DE FEEDBACK QUALITÃ‰ CODE               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ğŸ§ª Tests automatiques (pytest, jest, etc.)          â”‚
â”‚  2. ğŸ” Linters (pylint, flake8, eslint, etc.)           â”‚
â”‚  3. ğŸ‘¤ Validation humaine (code review)                  â”‚
â”‚  4. ğŸ›¡ï¸ Scans sÃ©curitÃ© (bandit, snyk, etc.)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Points d'IntÃ©gration dans le Workflow

### **Workflow LangGraph Actuel**

```
prepare_environment
        â†“
analyze_requirements
        â†“
implement_task (Claude gÃ©nÃ¨re le code)
        â†“
run_tests â† ğŸ¯ POINT 1: Feedback automatique tests
        â†“
quality_assurance_automation â† ğŸ¯ POINT 2: Feedback linters/QA
        â†“
finalize_pr
        â†“
monday_validation â† ğŸ¯ POINT 3: Demande feedback humain
        â†“
[Attente validation humaine]
        â†“
merge_after_validation â† ğŸ¯ POINT 4: Enregistrer dÃ©cision finale
        â†“
update_monday
```

---

## ğŸ”§ ImplÃ©mentation DÃ©taillÃ©e

### **POINT 1 : Feedback depuis Tests Automatiques**

#### Fichier : `nodes/test_node.py`

**Emplacement** : AprÃ¨s l'exÃ©cution des tests (ligne ~150-200)

```python
# Dans la fonction run_tests(), aprÃ¨s l'exÃ©cution des tests

async def run_tests(state: GraphState) -> GraphState:
    """ExÃ©cute les tests et enregistre le feedback qualitÃ©."""
    
    # ... code existant pour exÃ©cuter les tests ...
    
    # AprÃ¨s rÃ©cupÃ©ration des rÃ©sultats de tests
    test_results = await testing_engine._run_all_tests(
        working_directory=working_directory,
        include_coverage=True,
        code_changes=code_changes
    )
    
    # ğŸ¯ NOUVEAU: Enregistrer feedback qualitÃ© depuis les tests
    if test_results:
        await _save_test_quality_feedback(state, test_results)
    
    return state


async def _save_test_quality_feedback(state: GraphState, test_results: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualitÃ© basÃ© sur les rÃ©sultats de tests.
    
    Args:
        state: Ã‰tat du workflow
        test_results: RÃ©sultats des tests exÃ©cutÃ©s
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("ğŸ“Š Enregistrement du feedback qualitÃ© depuis les tests")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Extraire les mÃ©triques depuis test_results
    total_tests = test_results.get('total', 0)
    passed_tests = test_results.get('passed', 0)
    failed_tests = test_results.get('failed', 0)
    code_coverage = test_results.get('coverage_percent', 0)
    
    # Calculer un score de correctness basÃ© sur les tests
    if total_tests > 0:
        correctness_score = int((passed_tests / total_tests) * 5)  # Score sur 5
    else:
        correctness_score = 3  # Score neutre si pas de tests
    
    # RÃ©cupÃ©rer l'ID de gÃ©nÃ©ration de code
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("âš ï¸ Impossible d'enregistrer feedback: IDs manquants")
        return
    
    # Construire le feedback
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "automated_tests",
        "feedback_source": test_results.get('framework', 'pytest'),
        
        # Ratings
        "overall_rating": correctness_score,
        "code_correctness": correctness_score,
        "code_testability": 4 if passed_tests > 0 else 2,
        
        # MÃ©triques
        "code_coverage_percent": code_coverage,
        
        # DÃ©cision
        "code_accepted": (failed_tests == 0 and passed_tests > 0),
        "requires_rework": (failed_tests > 0),
        
        # DÃ©tails
        "comments": f"{passed_tests}/{total_tests} tests passÃ©s, couverture: {code_coverage}%",
        "issues_found": [f"test_failure:{test['name']}" for test in test_results.get('failures', [])],
        
        # MÃ©tadonnÃ©es
        "metadata": {
            "test_framework": test_results.get('framework'),
            "execution_time_ms": test_results.get('duration_ms'),
            "test_count": total_tests,
            "passed_count": passed_tests,
            "failed_count": failed_tests
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"âœ… Feedback tests enregistrÃ©: feedback_id={feedback_id}")
        
        # Stocker l'ID dans l'Ã©tat pour rÃ©fÃ©rence
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"âŒ Erreur enregistrement feedback tests: {e}", exc_info=True)
```

---

### **POINT 2 : Feedback depuis Linters/QA**

#### Fichier : `nodes/qa_node.py`

**Emplacement** : AprÃ¨s l'exÃ©cution des linters (ligne ~500-600)

```python
# Dans la fonction quality_assurance_automation()

async def quality_assurance_automation(state: GraphState) -> GraphState:
    """ExÃ©cute QA et enregistre le feedback qualitÃ©."""
    
    # ... code existant pour QA ...
    
    # AprÃ¨s exÃ©cution de tous les linters
    qa_results = {
        "pylint_score": pylint_result.get('score', 0),
        "flake8_issues": len(flake8_result.get('issues', [])),
        "black_formatted": black_result.get('formatted', False),
        "security_issues": len(bandit_result.get('issues', [])),
        # ... autres rÃ©sultats QA ...
    }
    
    # ğŸ¯ NOUVEAU: Enregistrer feedback qualitÃ© depuis QA
    await _save_qa_quality_feedback(state, qa_results)
    
    return state


async def _save_qa_quality_feedback(state: GraphState, qa_results: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualitÃ© basÃ© sur les rÃ©sultats QA.
    
    Args:
        state: Ã‰tat du workflow
        qa_results: RÃ©sultats des outils QA/linters
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("ğŸ“Š Enregistrement du feedback qualitÃ© depuis QA")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Calculer scores basÃ©s sur les rÃ©sultats QA
    pylint_score = qa_results.get('pylint_score', 0)
    flake8_issues = qa_results.get('flake8_issues', 0)
    security_issues = qa_results.get('security_issues', 0)
    
    # Convertir pylint score (0-10) en rating (1-5)
    style_rating = max(1, min(5, int((pylint_score / 10) * 5)))
    
    # Score de sÃ©curitÃ© basÃ© sur les issues
    security_rating = 5 if security_issues == 0 else max(1, 5 - (security_issues // 2))
    
    # RÃ©cupÃ©rer les IDs
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("âš ï¸ Impossible d'enregistrer feedback QA: IDs manquants")
        return
    
    # Construire le feedback
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "linter",
        "feedback_source": "qa_automation",
        
        # Ratings
        "overall_rating": int((style_rating + security_rating) / 2),
        "code_style": style_rating,
        "code_security": security_rating,
        "code_maintainability": style_rating,  # Approximation
        
        # MÃ©triques
        "security_score": security_rating,
        
        # DÃ©cision
        "code_accepted": (flake8_issues == 0 and security_issues == 0),
        "requires_rework": (flake8_issues > 10 or security_issues > 0),
        "rework_priority": "high" if security_issues > 0 else "medium" if flake8_issues > 10 else "low",
        
        # DÃ©tails
        "comments": f"Pylint: {pylint_score}/10, Flake8: {flake8_issues} issues, SÃ©curitÃ©: {security_issues} issues",
        "issues_found": [
            f"flake8:{issue}" for issue in qa_results.get('flake8_issues', [])[:5]
        ] + [
            f"security:{issue}" for issue in qa_results.get('security_issues', [])[:5]
        ],
        "positive_aspects": qa_results.get('positive_aspects', []),
        
        # MÃ©tadonnÃ©es
        "metadata": {
            "pylint_score": pylint_score,
            "flake8_issues_count": flake8_issues,
            "security_issues_count": security_issues,
            "black_formatted": qa_results.get('black_formatted', False),
            "tools_used": ["pylint", "flake8", "black", "bandit"]
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"âœ… Feedback QA enregistrÃ©: feedback_id={feedback_id}")
        
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"âŒ Erreur enregistrement feedback QA: {e}", exc_info=True)
```

---

### **POINT 3 : Feedback depuis Validation Humaine**

#### Fichier : `nodes/monday_validation_node.py`

**Emplacement** : Lors de la rÃ©ception de la rÃ©ponse de validation (ligne ~200-300)

```python
# Dans monday_validation() ou lors du traitement de la rÃ©ponse

async def process_validation_response(validation_response: Dict[str, Any], state: GraphState) -> None:
    """
    Traite la rÃ©ponse de validation humaine et enregistre le feedback.
    
    Args:
        validation_response: RÃ©ponse de l'utilisateur depuis Monday
        state: Ã‰tat du workflow
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("ğŸ“Š Enregistrement du feedback qualitÃ© depuis validation humaine")
    
    feedback_service = CodeQualityFeedbackService()
    
    # Extraire les informations de la validation
    approved = validation_response.get('approved', False)
    response_text = validation_response.get('response_text', '')
    reviewer_monday_user_id = validation_response.get('user_id')
    
    # Parser la rÃ©ponse pour extraire des ratings (si fournis)
    # Ex: "Code correct mais style Ã  amÃ©liorer - 4/5"
    # On peut utiliser un LLM pour analyser le feedback textuel
    
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("âš ï¸ Impossible d'enregistrer feedback humain: IDs manquants")
        return
    
    # Rating basÃ© sur l'approbation
    overall_rating = 5 if approved else 3
    
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "human",
        "feedback_source": "monday_validation",
        
        # Ratings
        "overall_rating": overall_rating,
        
        # DÃ©cision
        "code_accepted": approved,
        "requires_rework": not approved,
        "rework_priority": "high" if not approved else None,
        
        # DÃ©tails
        "comments": response_text,
        "reviewer_name": validation_response.get('reviewer_name', 'Unknown'),
        
        # MÃ©tadonnÃ©es
        "metadata": {
            "validation_id": validation_response.get('validation_id'),
            "monday_user_id": reviewer_monday_user_id,
            "response_type": validation_response.get('response_type'),
            "validation_timestamp": validation_response.get('timestamp')
        }
    }
    
    # ğŸ¯ OPTIONNEL: Utiliser un LLM pour analyser le feedback textuel
    if response_text:
        analyzed_feedback = await _analyze_human_feedback_with_llm(response_text)
        if analyzed_feedback:
            feedback_data.update({
                "code_correctness": analyzed_feedback.get('correctness_score'),
                "code_style": analyzed_feedback.get('style_score'),
                "code_maintainability": analyzed_feedback.get('maintainability_score'),
                "positive_aspects": analyzed_feedback.get('positive_aspects', []),
                "issues_found": analyzed_feedback.get('issues_found', []),
                "suggestions": analyzed_feedback.get('suggestions')
            })
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"âœ… Feedback humain enregistrÃ©: feedback_id={feedback_id}")
        
        if "quality_feedback_ids" not in state["results"]:
            state["results"]["quality_feedback_ids"] = []
        state["results"]["quality_feedback_ids"].append(feedback_id)
        
    except Exception as e:
        logger.error(f"âŒ Erreur enregistrement feedback humain: {e}", exc_info=True)


async def _analyze_human_feedback_with_llm(feedback_text: str) -> Dict[str, Any]:
    """
    Utilise un LLM pour analyser le feedback textuel humain.
    
    Args:
        feedback_text: Commentaire de l'utilisateur
        
    Returns:
        Dictionnaire avec scores et aspects extraits
    """
    from tools.claude_code_tool import ClaudeCodeTool
    
    claude = ClaudeCodeTool()
    
    prompt = f"""Analyse ce feedback de code review et extrais les informations structurÃ©es:

Feedback: "{feedback_text}"

Extrais:
1. Score de correctness (1-5)
2. Score de style (1-5)
3. Score de maintenabilitÃ© (1-5)
4. Points positifs (liste)
5. ProblÃ¨mes identifiÃ©s (liste)
6. Suggestions d'amÃ©lioration

RÃ©ponds en JSON:
{{
  "correctness_score": int,
  "style_score": int,
  "maintainability_score": int,
  "positive_aspects": [str],
  "issues_found": [str],
  "suggestions": str
}}
"""
    
    try:
        response = await claude.execute(prompt)
        import json
        return json.loads(response)
    except Exception as e:
        logger.warning(f"âš ï¸ Erreur analyse LLM du feedback: {e}")
        return {}
```

---

### **POINT 4 : Feedback Final aprÃ¨s Merge**

#### Fichier : `nodes/merge_node.py`

**Emplacement** : AprÃ¨s le merge rÃ©ussi (ligne ~100-150)

```python
# Dans la fonction merge_after_validation()

async def merge_after_validation(state: GraphState) -> GraphState:
    """Merge la PR et enregistre le feedback final."""
    
    # ... code existant pour merger ...
    
    # AprÃ¨s merge rÃ©ussi
    if merge_successful:
        # ğŸ¯ NOUVEAU: Enregistrer feedback final
        await _save_final_quality_feedback(state, merge_result)
    
    return state


async def _save_final_quality_feedback(state: GraphState, merge_result: Dict[str, Any]) -> None:
    """
    Enregistre le feedback qualitÃ© final aprÃ¨s merge.
    
    Args:
        state: Ã‰tat du workflow
        merge_result: RÃ©sultat du merge
    """
    from services.code_quality_feedback_service import CodeQualityFeedbackService
    
    logger.info("ğŸ“Š Enregistrement du feedback qualitÃ© final")
    
    feedback_service = CodeQualityFeedbackService()
    
    ai_code_generation_id = state["results"].get("ai_code_generation_id")
    task_run_id = state.get("tasks_runs_id")
    
    if not ai_code_generation_id or not task_run_id:
        logger.warning("âš ï¸ Impossible d'enregistrer feedback final: IDs manquants")
        return
    
    # AgrÃ©ger tous les feedbacks prÃ©cÃ©dents
    previous_feedbacks = state["results"].get("quality_feedback_ids", [])
    
    # Calculer un score global basÃ© sur le succÃ¨s du merge
    feedback_data = {
        "ai_code_generation_id": ai_code_generation_id,
        "task_run_id": task_run_id,
        "feedback_type": "code_review",
        "feedback_source": "merge_final",
        
        # Ratings
        "overall_rating": 5,  # Merge rÃ©ussi = code acceptÃ©
        "code_correctness": 5,
        "code_style": 4,
        "code_security": 4,
        "code_maintainability": 4,
        
        # DÃ©cision
        "code_accepted": True,
        "requires_rework": False,
        
        # DÃ©tails
        "comments": f"Code mergÃ© avec succÃ¨s dans la branche principale. {len(previous_feedbacks)} feedbacks collectÃ©s.",
        "positive_aspects": ["merge_successful", "all_checks_passed", "validated_by_human"],
        
        # MÃ©tadonnÃ©es
        "metadata": {
            "merge_commit_sha": merge_result.get('commit_sha'),
            "pr_url": merge_result.get('pr_url'),
            "merged_at": merge_result.get('merged_at'),
            "previous_feedbacks_count": len(previous_feedbacks),
            "previous_feedback_ids": previous_feedbacks
        }
    }
    
    try:
        feedback_id = await feedback_service.save_feedback(feedback_data)
        logger.info(f"âœ… Feedback final enregistrÃ©: feedback_id={feedback_id}")
        
        # Mettre Ã  jour tous les feedbacks prÃ©cÃ©dents pour les lier au final
        await feedback_service.link_feedbacks_to_final(
            previous_feedback_ids=previous_feedbacks,
            final_feedback_id=feedback_id
        )
        
    except Exception as e:
        logger.error(f"âŒ Erreur enregistrement feedback final: {e}", exc_info=True)
```

---

## ğŸ› ï¸ Service DÃ©diÃ© : `CodeQualityFeedbackService`

### Fichier : `services/code_quality_feedback_service.py` (Ã€ crÃ©er)

```python
"""Service de gestion des feedbacks qualitÃ© code."""

import asyncpg
from typing import Dict, Any, List, Optional
from utils.logger import get_logger

logger = get_logger(__name__)


class CodeQualityFeedbackService:
    """Service pour enregistrer et gÃ©rer les feedbacks qualitÃ© code."""
    
    def __init__(self):
        """Initialise le service."""
        from services.database_persistence_service import db_persistence
        self.db = db_persistence
    
    async def save_feedback(self, feedback_data: Dict[str, Any]) -> int:
        """
        Enregistre un nouveau feedback qualitÃ©.
        
        Args:
            feedback_data: DonnÃ©es du feedback
            
        Returns:
            ID du feedback crÃ©Ã©
        """
        query = """
            INSERT INTO code_quality_feedback (
                ai_code_generation_id,
                task_run_id,
                overall_rating,
                code_correctness,
                code_style,
                code_efficiency,
                code_security,
                code_maintainability,
                code_testability,
                feedback_type,
                feedback_source,
                comments,
                positive_aspects,
                issues_found,
                suggestions,
                code_accepted,
                requires_rework,
                rework_priority,
                lines_of_code,
                cyclomatic_complexity,
                code_coverage_percent,
                performance_score,
                security_score,
                reviewer_name,
                review_duration_seconds,
                review_tool_used,
                metadata
            ) VALUES (
                $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                $21, $22, $23, $24, $25, $26, $27
            )
            RETURNING feedback_id
        """
        
        try:
            async with self.db.pool.acquire() as conn:
                result = await conn.fetchrow(
                    query,
                    feedback_data.get('ai_code_generation_id'),
                    feedback_data.get('task_run_id'),
                    feedback_data.get('overall_rating'),
                    feedback_data.get('code_correctness'),
                    feedback_data.get('code_style'),
                    feedback_data.get('code_efficiency'),
                    feedback_data.get('code_security'),
                    feedback_data.get('code_maintainability'),
                    feedback_data.get('code_testability'),
                    feedback_data.get('feedback_type'),
                    feedback_data.get('feedback_source'),
                    feedback_data.get('comments'),
                    feedback_data.get('positive_aspects'),
                    feedback_data.get('issues_found'),
                    feedback_data.get('suggestions'),
                    feedback_data.get('code_accepted'),
                    feedback_data.get('requires_rework'),
                    feedback_data.get('rework_priority'),
                    feedback_data.get('lines_of_code'),
                    feedback_data.get('cyclomatic_complexity'),
                    feedback_data.get('code_coverage_percent'),
                    feedback_data.get('performance_score'),
                    feedback_data.get('security_score'),
                    feedback_data.get('reviewer_name'),
                    feedback_data.get('review_duration_seconds'),
                    feedback_data.get('review_tool_used'),
                    feedback_data.get('metadata')
                )
                
                feedback_id = result['feedback_id']
                logger.info(f"âœ… Feedback enregistrÃ©: ID={feedback_id}, type={feedback_data.get('feedback_type')}")
                return feedback_id
                
        except Exception as e:
            logger.error(f"âŒ Erreur enregistrement feedback: {e}", exc_info=True)
            raise
    
    async def get_feedbacks_for_run(self, task_run_id: int) -> List[Dict[str, Any]]:
        """
        RÃ©cupÃ¨re tous les feedbacks d'un run.
        
        Args:
            task_run_id: ID du task run
            
        Returns:
            Liste des feedbacks
        """
        query = """
            SELECT * FROM code_quality_feedback
            WHERE task_run_id = $1
            ORDER BY created_at ASC
        """
        
        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(query, task_run_id)
            return [dict(row) for row in rows]
    
    async def get_aggregate_stats(self, task_run_id: int) -> Dict[str, Any]:
        """
        Calcule les statistiques agrÃ©gÃ©es des feedbacks.
        
        Args:
            task_run_id: ID du task run
            
        Returns:
            Statistiques agrÃ©gÃ©es
        """
        query = """
            SELECT 
                COUNT(*) as feedback_count,
                AVG(overall_rating) as avg_overall_rating,
                AVG(code_correctness) as avg_correctness,
                AVG(code_style) as avg_style,
                AVG(code_security) as avg_security,
                SUM(CASE WHEN code_accepted THEN 1 ELSE 0 END) as accepted_count,
                SUM(CASE WHEN requires_rework THEN 1 ELSE 0 END) as rework_count
            FROM code_quality_feedback
            WHERE task_run_id = $1
        """
        
        async with self.db.pool.acquire() as conn:
            result = await conn.fetchrow(query, task_run_id)
            return dict(result) if result else {}
    
    async def link_feedbacks_to_final(
        self, 
        previous_feedback_ids: List[int], 
        final_feedback_id: int
    ) -> None:
        """
        Lie les feedbacks prÃ©cÃ©dents au feedback final.
        
        Args:
            previous_feedback_ids: IDs des feedbacks prÃ©cÃ©dents
            final_feedback_id: ID du feedback final
        """
        query = """
            UPDATE code_quality_feedback
            SET metadata = COALESCE(metadata, '{}'::jsonb) || 
                jsonb_build_object('final_feedback_id', $1)
            WHERE feedback_id = ANY($2)
        """
        
        async with self.db.pool.acquire() as conn:
            await conn.execute(query, final_feedback_id, previous_feedback_ids)
            logger.info(f"âœ… Feedbacks liÃ©s au final: {len(previous_feedback_ids)} â†’ {final_feedback_id}")


# Instance globale
code_quality_feedback_service = CodeQualityFeedbackService()
```

---

## ğŸ“Š Utilisation des DonnÃ©es

### **Dashboard de QualitÃ© Code**

```python
# services/analytics_service.py

async def get_code_quality_dashboard(days: int = 30) -> Dict[str, Any]:
    """GÃ©nÃ¨re un dashboard de qualitÃ© code."""
    
    query = """
        SELECT 
            DATE(created_at) as date,
            feedback_type,
            AVG(overall_rating) as avg_rating,
            AVG(code_correctness) as avg_correctness,
            AVG(code_style) as avg_style,
            AVG(code_security) as avg_security,
            COUNT(*) as feedback_count,
            SUM(CASE WHEN code_accepted THEN 1 ELSE 0 END) as accepted_count
        FROM code_quality_feedback
        WHERE created_at > NOW() - INTERVAL '{days} days'
        GROUP BY DATE(created_at), feedback_type
        ORDER BY date DESC
    """
    
    # ExÃ©cuter et formatter les rÃ©sultats
    ...
```

### **AmÃ©lioration des Prompts**

```python
# Utiliser les feedbacks pour amÃ©liorer les prompts

async def analyze_prompt_performance():
    """Analyse la performance des prompts basÃ©e sur les feedbacks."""
    
    query = """
        SELECT 
            apt.template_id,
            apt.name,
            AVG(cqf.overall_rating) as avg_quality_rating,
            AVG(cqf.code_correctness) as avg_correctness,
            COUNT(*) as usage_count
        FROM code_quality_feedback cqf
        JOIN ai_code_generations acg ON cqf.ai_code_generation_id = acg.ai_code_generations_id
        JOIN ai_prompt_usage apu ON acg.task_run_id = apu.task_run_id
        JOIN ai_prompt_templates apt ON apu.template_id = apt.template_id
        WHERE cqf.created_at > NOW() - INTERVAL '7 days'
        GROUP BY apt.template_id, apt.name
        ORDER BY avg_quality_rating DESC
    """
    
    # Identifier les prompts Ã  amÃ©liorer
    ...
```

---

## âœ… Checklist d'ImplÃ©mentation

- [ ] **1. CrÃ©er le service** : `services/code_quality_feedback_service.py`
- [ ] **2. Modifier `test_node.py`** : Ajouter `_save_test_quality_feedback()`
- [ ] **3. Modifier `qa_node.py`** : Ajouter `_save_qa_quality_feedback()`
- [ ] **4. Modifier `monday_validation_node.py`** : Ajouter `process_validation_response()`
- [ ] **5. Modifier `merge_node.py`** : Ajouter `_save_final_quality_feedback()`
- [ ] **6. Appliquer la migration SQL** : `data/migration_conversational_features.sql`
- [ ] **7. CrÃ©er les tests unitaires** : `tests/test_code_quality_feedback.py`
- [ ] **8. Mettre Ã  jour `GraphState`** : Ajouter `quality_feedback_ids` dans results
- [ ] **9. CrÃ©er dashboard** : Vue analytics des feedbacks
- [ ] **10. Documentation** : README avec exemples d'utilisation

---

## ğŸ¯ BÃ©nÃ©fices Attendus

1. âœ… **TraÃ§abilitÃ© complÃ¨te** de la qualitÃ© du code gÃ©nÃ©rÃ©
2. âœ… **AmÃ©lioration continue** des prompts IA basÃ©e sur feedback rÃ©el
3. âœ… **Identification des patterns** de code problÃ©matiques
4. âœ… **MÃ©triques de performance** de l'IA sur le long terme
5. âœ… **Confiance utilisateur** augmentÃ©e (transparence)
6. âœ… **Base de donnÃ©es** pour machine learning futur

---

**PrÃªt Ã  implÃ©menter ! ğŸš€**
