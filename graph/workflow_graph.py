"""Workflow principal utilisant LangGraph pour l'agent de d√©veloppement."""

import asyncio
import tempfile
import os
import uuid
import time
from datetime import datetime
from typing import Dict, Any, Optional, AsyncIterator

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from models.state import GraphState
from models.schemas import TaskRequest, WorkflowStatus
from utils.logger import get_logger

# Import des n≈ìuds
from nodes.prepare_node import prepare_environment
from nodes.analyze_node import analyze_requirements  
from nodes.implement_node import implement_task
from nodes.test_node import run_tests
from nodes.debug_node import debug_code
from nodes.openai_debug_node import openai_debug_after_human_request  # ‚úÖ IMPORT CORRECT
from nodes.qa_node import quality_assurance_automation
from nodes.finalize_node import finalize_pr
from nodes.monday_validation_node import monday_human_validation
from nodes.merge_node import merge_after_validation
from nodes.update_node import update_monday

# Imports des services et configurations
from services.database_persistence_service import db_persistence
from config.workflow_limits import WorkflowLimits
from utils.langsmith_tracing import workflow_tracer
from config.langsmith_config import langsmith_config

logger = get_logger(__name__)

# Constantes du workflow
MAX_WORKFLOW_NODES = 11  # Nombre de n≈ìuds business r√©els dans le workflow
TOTAL_GRAPH_NODES = 11   # ‚úÖ CORRECTION: Le nombre total = MAX_WORKFLOW_NODES (LangGraph ajoute __start__ et __end__ automatiquement)
MAX_DEBUG_ATTEMPTS = 2   # ‚úÖ CORRECTION: R√©duire √† 2 tentatives pour √©viter les boucles infinies

# ‚úÖ NOUVEAU: Configuration des timeouts et retry
WORKFLOW_TIMEOUT_SECONDS = WorkflowLimits.WORKFLOW_TIMEOUT  # Utiliser la configuration
NODE_TIMEOUT_SECONDS = 600      # 10 minutes maximum par n≈ìud
MAX_NODE_RETRIES = 2            # Maximum 2 essais par n≈ìud

def create_workflow_graph() -> StateGraph:
    """
    Cr√©e et configure le graphe de workflow LangGraph pour RabbitMQ avec validation humaine.

    Le graphe suit ce flux optimis√© conforme √† la conception :
    START ‚Üí prepare ‚Üí analyze ‚Üí implement ‚Üí test ‚Üí [debug ‚Üî test] ‚Üí qa ‚Üí finalize ‚Üí human_validation ‚Üí merge ‚Üí update ‚Üí END

    Returns:
        StateGraph configur√© et pr√™t √† √™tre compil√©
    """

    # Cr√©er le graphe avec GraphState
    workflow = StateGraph(GraphState)

    # Ajouter tous les n≈ìuds dans l'ordre de la conception
    workflow.add_node("prepare_environment", prepare_environment)
    workflow.add_node("analyze_requirements", analyze_requirements)
    workflow.add_node("implement_task", implement_task)
    workflow.add_node("run_tests", run_tests)
    workflow.add_node("debug_code", debug_code)
    workflow.add_node("quality_assurance_automation", quality_assurance_automation)
    workflow.add_node("finalize_pr", finalize_pr)
    workflow.add_node("monday_validation", monday_human_validation)
    workflow.add_node("openai_debug", openai_debug_after_human_request)
    workflow.add_node("merge_after_validation", merge_after_validation)
    workflow.add_node("update_monday", update_monday)

    # D√©finir le point d'entr√©e
    workflow.set_entry_point("prepare_environment")

    # Connexions principales du workflow selon la conception
    workflow.add_edge("prepare_environment", "analyze_requirements")
    workflow.add_edge("analyze_requirements", "implement_task")
    workflow.add_edge("implement_task", "run_tests")

    # Logique conditionnelle pour les tests
    workflow.add_conditional_edges(
        "run_tests",
        _should_debug,
        {
            "debug": "debug_code",      # Tests √©chou√©s ‚Üí debug
            "continue": "quality_assurance_automation",  # Tests r√©ussis ‚Üí QA
            "end": END                  # Erreur critique ‚Üí fin
        }
    )

    # Boucle de debug
    workflow.add_edge("debug_code", "run_tests")

    # Finalisation apr√®s QA avec validation Monday.com
    workflow.add_edge("quality_assurance_automation", "finalize_pr")
    workflow.add_edge("finalize_pr", "monday_validation")

    # Apr√®s validation Monday.com, d√©cider du chemin
    workflow.add_conditional_edges(
        "monday_validation",
        _should_merge_or_debug_after_monday_validation,
        {
            "merge": "merge_after_validation",    # Humain a dit "oui" ‚Üí merge
            "debug": "openai_debug",              # Humain a dit "non" ‚Üí debug OpenAI
            "update_only": "update_monday",       # Timeout/erreur ‚Üí update seulement
            "end": END                            # Erreur critique ‚Üí fin
        }
    )

    # ‚úÖ CORRECTION: Logique conditionnelle apr√®s debug OpenAI au lieu de retour automatique aux tests
    workflow.add_conditional_edges(
        "openai_debug",
        _should_continue_after_openai_debug,
        {
            "retest": "run_tests",           # Retry les tests apr√®s corrections
            "update_only": "update_monday",  # Limite atteinte ‚Üí update seulement
            "end": END                       # Erreur critique ‚Üí fin
        }
    )

    # Finalisation
    workflow.add_edge("merge_after_validation", "update_monday")
    workflow.add_edge("update_monday", END)

    logger.info("‚úÖ Graphe de workflow cr√©√© et configur√© pour RabbitMQ avec nouveaux n≈ìuds")
    return workflow

def _should_merge_or_debug_after_monday_validation(state: GraphState) -> str:
    """
    D√©termine le chemin apr√®s validation Monday.com.

    LOGIQUE AM√âLIOR√âE:
    - "oui" + pas de probl√®mes ‚Üí merge
    - "oui" + probl√®mes d√©tect√©s ‚Üí debug automatique (ignore la r√©ponse humaine)
    - "non/debug" ‚Üí debug OpenAI
    - timeout/erreur ‚Üí update seulement

    Args:
        state: √âtat actuel du workflow

    Returns:
        "merge" si approuv√© ET aucun probl√®me, "debug" si probl√®mes d√©tect√©s ou debug demand√©, "update_only" si erreur/timeout
    """
    # Protection: s'assurer que results existe
    results = state.get("results", {})
    if not isinstance(results, dict):
        logger.warning("‚ö†Ô∏è Results n'est pas un dictionnaire dans _should_merge_or_debug_after_monday_validation")
        return "update_only"

    # ‚úÖ CORRECTION CRITIQUE: V√©rifier les erreurs fatales EN PREMIER
    current_status = results.get("current_status", "")
    if current_status == "failed_validation":
        logger.error("‚ùå Erreur de validation critique d√©tect√©e - arr√™t du workflow")
        return "end"
    
    # V√©rifier si le finalize_pr a √©chou√© de mani√®re critique
    finalize_errors = results.get("error_logs", [])
    critical_finalize_errors = [
        "URL du repository non d√©finie",
        "Branche Git non d√©finie", 
        "R√©pertoire de travail non d√©fini",
        "Working directory non d√©fini"
    ]
    
    for error_log in finalize_errors:
        if any(critical_error in error_log for critical_error in critical_finalize_errors):
            logger.error(f"‚ùå Erreur critique de finalisation d√©tect√©e: {error_log}")
            logger.error("‚ùå Impossible de continuer le workflow - donn√©es manquantes")
            return "end"
    
    # V√©rifier si GitHub push a √©chou√© de mani√®re critique
    if results.get("skip_github", False):
        logger.warning("‚ö†Ô∏è GitHub push ignor√© - transition vers update Monday seulement")
        return "update_only"

    # ‚úÖ AM√âLIORATION: V√©rification et correction de coh√©rence d'√©tat
    human_decision = results.get("human_decision", "error")
    should_merge = results.get("should_merge", False)
    validation_status = results.get("human_validation_status")
    error = results.get("error")

    # D√©tection et correction d'incoh√©rences
    if human_decision == "approve" and not should_merge:
        logger.warning("‚ö†Ô∏è Incoh√©rence d√©tect√©e: approve sans should_merge - correction automatique")
        results["should_merge"] = True
        should_merge = True
    elif human_decision == "debug" and should_merge:
        logger.warning("‚ö†Ô∏è Incoh√©rence d√©tect√©e: debug avec should_merge=True - correction automatique")
        results["should_merge"] = False
        should_merge = False
    elif validation_status == "approve" and human_decision != "approve":
        logger.warning("‚ö†Ô∏è Incoh√©rence d√©tect√©e: validation_status/human_decision mismatch - correction automatique")
        results["human_decision"] = "approve"
        results["should_merge"] = True
        human_decision = "approve"
        should_merge = True
    elif validation_status == "rejected" and human_decision not in ["debug", "error", "timeout"]:
        logger.warning("‚ö†Ô∏è Incoh√©rence d√©tect√©e: validation rejected mais human_decision invalide - correction automatique")
        results["human_decision"] = "debug"
        results["should_merge"] = False
        human_decision = "debug"
        should_merge = False

    # Erreur critique ou timeout
    if error and "timeout" in error.lower():
        logger.warning("‚ö†Ô∏è Timeout validation Monday.com - update seulement")
        return "update_only"

    if human_decision == "error":
        logger.warning("‚ö†Ô∏è Erreur validation Monday.com - update seulement")
        return "update_only"
    
    # ‚úÖ NOUVEAU: G√©rer la validation automatique
    if human_decision == "timeout":
        logger.warning("‚ö†Ô∏è Timeout validation Monday.com - update seulement")
        return "update_only"
    
    # ‚úÖ NOUVEAU: G√©rer l'approbation automatique  
    if human_decision == "approve_auto":
        logger.info("‚úÖ Validation automatique approuv√©e - traiter comme approve")
        human_decision = "approve"
        should_merge = True
        results["should_merge"] = True

    # ‚úÖ NOUVELLE LOGIQUE: V√©rification automatique des probl√®mes m√™me si l'humain dit "oui"
    def _has_unresolved_issues(results: dict) -> tuple[bool, list[str]]:
        """V√©rifie s'il y a encore des probl√®mes non r√©solus dans le workflow."""
        issues = []

        # 1. V√©rifier les r√©sultats de tests
        test_results = results.get("test_results", {})
        if isinstance(test_results, dict):
            test_success = test_results.get("success", False)
            if not test_success:
                issues.append("tests √©chou√©s")
                failed_tests = test_results.get("failed_tests", [])
                if failed_tests:
                    issues.append(f"{len(failed_tests)} test(s) en √©chec")

        # 2. V√©rifier les erreurs de build/linting
        error_logs = results.get("error_logs", [])
        if error_logs:
            issues.append(f"{len(error_logs)} erreur(s) d√©tect√©e(s)")

        # 3. V√©rifier les erreurs d'impl√©mentation
        implementation_errors = results.get("implementation_errors", [])
        if implementation_errors:
            issues.append(f"{len(implementation_errors)} erreur(s) d'impl√©mentation")

        # 4. V√©rifier si la PR a √©t√© cr√©√©e
        pr_url = results.get("pr_url")
        if not pr_url:
            issues.append("pull request non cr√©√©e")

        # 5. V√©rifier les scores de qualit√©
        qa_results = results.get("qa_results", {})
        if isinstance(qa_results, dict):
            overall_score = qa_results.get("overall_score", 0)
            if overall_score < 70:  # Score minimal requis
                issues.append(f"score qualit√© trop bas ({overall_score}/100)")

        return len(issues) > 0, issues

    # D√©cision humaine de debug explicite
    if human_decision == "debug":
        logger.info("üîß Humain demande debug via Monday.com - lancer OpenAI debug")
        return "debug"

    # D√©cision humaine d'approuver - mais v√©rifier les probl√®mes
    if human_decision == "approve" and should_merge:
        # ‚úÖ NOUVELLE LOGIQUE: V√©rification automatique seulement si l'humain n'a pas √©t√© explicite
        has_issues, issue_list = _has_unresolved_issues(results)

        if has_issues:
            # ‚úÖ CORRECTION: Respecter la d√©cision humaine explicite mais avertir
            logger.warning("‚ö†Ô∏è Humain a dit 'oui' mais probl√®mes d√©tect√©s - RESPECT DE LA D√âCISION HUMAINE")
            logger.warning(f"   Probl√®mes d√©tect√©s: {', '.join(issue_list)}")
            logger.warning("   L'humain assume la responsabilit√© du merge malgr√© les probl√®mes")
            
            # Ajouter un avertissement dans l'√©tat mais continuer le merge
            if "ai_messages" not in results:
                results["ai_messages"] = []
            results["ai_messages"].append(f"‚ö†Ô∏è Merge approuv√© malgr√©: {', '.join(issue_list)}")
            results["human_override"] = True
            results["override_issues"] = issue_list
            
            return "merge"  # ‚úÖ CORRECTION: Respecter la d√©cision humaine
        else:
            logger.info("‚úÖ Humain a dit 'oui' et aucun probl√®me d√©tect√© - MERGE")
            return "merge"

    # Timeout ou autre
    if human_decision == "timeout":
        logger.info("‚è∞ Timeout validation Monday.com - update seulement")
        return "update_only"

    # Par d√©faut, update seulement
    logger.info(f"ü§∑ D√©cision inconnue ({human_decision}) - update seulement")
    return "update_only"

def _should_merge_after_validation(state: GraphState) -> str:
    """
    D√©termine si le workflow doit proc√©der au merge apr√®s validation humaine.

    Args:
        state: √âtat actuel du workflow

    Returns:
        "merge" si approuv√©, "skip_merge" si rejet√©, "end" si erreur critique
    """
    # Protection: s'assurer que results existe
    results = state.get("results", {})
    if not isinstance(results, dict):
        logger.warning("‚ö†Ô∏è Results n'est pas un dictionnaire dans _should_merge_after_validation")
        return "end"

    # V√©rifier si on a une r√©ponse de validation
    should_merge = results.get("should_merge", False)
    validation_status = results.get("human_validation_status")
    error = results.get("error")

    # Erreur critique
    if error and "timeout" in error.lower():
        logger.warning("‚ö†Ô∏è Timeout validation humaine - arr√™t du workflow")
        return "end"

    # Validation approuv√©e
    if should_merge and validation_status == "approve":
        logger.info("‚úÖ Validation approuv√©e - proc√©der au merge")
        return "merge"

    # Validation rejet√©e ou expir√©e
    logger.info(f"‚è≠Ô∏è Validation non approuv√©e ({validation_status}) - passer au update Monday")
    return "skip_merge"

def _should_debug(state: GraphState) -> str:
    """
    D√©termine si le workflow doit passer en mode debug.

    Args:
        state: √âtat actuel du workflow

    Returns:
        "debug" si debug n√©cessaire, "continue" si on peut passer √† QA, "end" si erreur critique
    """
    # Importer les limites configur√©es
    from config.workflow_limits import WorkflowLimits

    # Protection: s'assurer que results existe
    results = state.get("results", {})
    if not isinstance(results, dict):
        logger.warning("‚ö†Ô∏è Results n'est pas un dictionnaire dans _should_debug")
        return "end"

    # V√©rifier si on a des r√©sultats de tests
    if not results or "test_results" not in results:
        logger.warning("‚ö†Ô∏è Aucun r√©sultat de test trouv√© - Structure de donn√©es incorrecte")
        results["current_status"] = "error_no_test_structure"
        results["error"] = "Structure de donn√©es de test manquante"
        results["should_continue"] = False
        return "end"  # ARR√äT PROPRE car probl√®me structurel

    test_results_list = results["test_results"]

    # Si aucun test n'a √©t√© ex√©cut√© (liste vide)
    if not test_results_list:
        logger.info("üìù Aucun test ex√©cut√© - Continuation vers assurance qualit√©")
        results["no_tests_found"] = True
        results["test_status"] = "no_tests"
        if "ai_messages" not in results:
            results["ai_messages"] = []
        results["ai_messages"].append("üìù Aucun test ex√©cut√© - Passage direct √† l'assurance qualit√©")
        return "continue"  # ‚úÖ Continuer vers QA au lieu d'arr√™ter

    # ‚úÖ CORRECTION CRITIQUE: Prendre SEULEMENT le dernier r√©sultat de test
    # √âviter l'accumulation des √©checs des cycles pr√©c√©dents
    if isinstance(test_results_list, list):
        latest_test_result = test_results_list[-1]  # Seulement le plus r√©cent
    else:
        latest_test_result = test_results_list  # Au cas o√π ce serait un seul r√©sultat

    logger.info(f"üîç Analyse du dernier r√©sultat de test: {type(latest_test_result)}")

    # Analyser le DERNIER r√©sultat de test uniquement
    tests_passed = False
    failed_count = 0
    total_tests = 0

    if isinstance(latest_test_result, dict):
        # Format : {"success": bool, "total_tests": int, "failed_tests": [...], ...}
        tests_passed = latest_test_result.get("success", False)
        total_tests = latest_test_result.get("total_tests", 0)
        failed_count = latest_test_result.get("failed_tests", 0)

        # D√©tecter le flag sp√©cial "no_tests_found"
        if latest_test_result.get("no_tests_found", False):
            logger.info("üìù Flag 'no_tests_found' d√©tect√© - Continuation vers assurance qualit√©")
            results["no_tests_found"] = True
            results["test_status"] = "no_tests"
            if "ai_messages" not in results:
                results["ai_messages"] = []
            results["ai_messages"].append("üìù Aucun test trouv√© - Passage direct √† l'assurance qualit√©")
            return "continue"  # ‚úÖ Continuer vers QA au lieu d'arr√™ter

        # Si c'est un nombre, c'est le nombre d'√©checs
        if isinstance(failed_count, int):
            pass  # failed_count est d√©j√† un int
        elif isinstance(failed_count, list):
            failed_count = len(failed_count)
        else:
            failed_count = 0

    elif hasattr(latest_test_result, 'success'):
        # C'est un objet TestResult
        tests_passed = latest_test_result.success
        total_tests = getattr(latest_test_result, 'total_tests', 1)
        failed_count = getattr(latest_test_result, 'failed_tests', 0) if not tests_passed else 0
    else:
        # Format bool√©en simple
        tests_passed = bool(latest_test_result)
        total_tests = 1
        failed_count = 0 if tests_passed else 1

    # CAS SP√âCIAL : Si total_tests = 0, continuer vers QA avec un flag sp√©cial
    if total_tests == 0:
        logger.info("üìù Aucun test trouv√© (0/0) - Continuation vers assurance qualit√©")
        results["no_tests_found"] = True
        results["test_status"] = "no_tests"
        if "ai_messages" not in results:
            results["ai_messages"] = []
        results["ai_messages"].append("üìù Aucun test trouv√© - Passage direct √† l'assurance qualit√©")
        return "continue"  # ‚úÖ Continuer vers QA au lieu d'arr√™ter

    # SYST√àME DE COMPTAGE ROBUSTE DES TENTATIVES DE DEBUG
    # Initialiser le compteur s'il n'existe pas
    if "debug_attempts" not in results:
        results["debug_attempts"] = 0

    debug_attempts = results["debug_attempts"]
    MAX_DEBUG_ATTEMPTS = WorkflowLimits.MAX_DEBUG_ATTEMPTS

    logger.info(f"üîß Debug attempts: {debug_attempts}/{MAX_DEBUG_ATTEMPTS}, Tests: {total_tests} total, {failed_count} √©checs (dernier r√©sultat uniquement)")

    # LOGIQUE DE D√âCISION SIMPLIFI√âE ET ROBUSTE
    if tests_passed:
        logger.info("‚úÖ Tests r√©ussis - passage √† l'assurance qualit√©")
        return "continue"

    if debug_attempts >= MAX_DEBUG_ATTEMPTS:
        logger.warning(f"‚ö†Ô∏è Limite de debug atteinte ({debug_attempts}/{MAX_DEBUG_ATTEMPTS}) - passage forc√© √† QA")
        results["error"] = f"Tests √©chou√©s apr√®s {debug_attempts} tentatives de debug"
        return "continue"  # FORC√â vers QA au lieu de boucler

    # Incr√©menter le compteur AVANT de retourner "debug"
    results["debug_attempts"] += 1
    logger.info(f"üîß Tests √©chou√©s ({failed_count} √©checs) - lancement debug {results['debug_attempts']}/{MAX_DEBUG_ATTEMPTS}")
    return "debug"

def _should_continue_after_openai_debug(state: GraphState) -> str:
    """
    D√©termine le chemin apr√®s debug OpenAI suite √† validation humaine.
    
    LOGIQUE:
    - Si limite de debug atteinte ‚Üí update_only
    - Si debug r√©ussi ‚Üí retest 
    - Si erreur critique ‚Üí end
    
    Args:
        state: √âtat actuel du workflow
        
    Returns:
        "retest" pour retester, "update_only" si limite atteinte, "end" si erreur critique
    """
    results = state.get("results", {})
    if not isinstance(results, dict):
        logger.warning("‚ö†Ô∏è Results n'est pas un dictionnaire dans _should_continue_after_openai_debug")
        return "end"
    
    # V√©rifier si la limite de debug apr√®s validation humaine est atteinte
    if results.get("debug_limit_reached", False):
        logger.warning("‚ö†Ô∏è Limite de debug apr√®s validation humaine atteinte - update Monday seulement")
        return "update_only"
    
    # V√©rifier si le debug a √©chou√©
    if results.get("openai_debug_failed", False):
        logger.error("‚ùå Debug OpenAI a √©chou√© - update Monday seulement")
        return "update_only"
    
    # V√©rifier si on doit continuer le workflow
    if not results.get("should_continue", True):
        logger.warning("‚ö†Ô∏è Workflow marqu√© pour arr√™t - update Monday seulement")
        return "update_only"
    
    # Si debug r√©ussi, retester
    if results.get("openai_debug_completed", False):
        logger.info("‚úÖ Debug OpenAI termin√© avec succ√®s - relance des tests")
        return "retest"
    
    # Par d√©faut, retester
    logger.info("üîÑ Debug OpenAI termin√© - relance des tests")
    return "retest"

async def run_workflow(task_request: TaskRequest) -> Dict[str, Any]:
    """
    Execute un workflow complet pour une t√¢che donn√©e avec RabbitMQ.

    Args:
        task_request: Requ√™te de t√¢che √† traiter

    Returns:
        R√©sultat du workflow avec m√©triques
    """
    workflow_id = f"workflow_{task_request.task_id}_{int(datetime.now().timestamp())}"

    logger.info(f"üöÄ D√©marrage workflow {workflow_id} pour: {task_request.title}")

    # ‚úÖ NOUVEAU: Prot√©ger le workflow entier avec un timeout global
    try:
        return await asyncio.wait_for(
            _run_workflow_with_recovery(workflow_id, task_request),
            timeout=WORKFLOW_TIMEOUT_SECONDS
        )
    except asyncio.TimeoutError:
        logger.error(f"‚ùå Timeout global du workflow {workflow_id} apr√®s {WORKFLOW_TIMEOUT_SECONDS}s")
        return _create_timeout_result(task_request, workflow_id, "global_timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans le workflow {workflow_id}: {e}", exc_info=True)
        return _create_error_result(task_request, str(e))

async def _run_workflow_with_recovery(workflow_id: str, task_request: TaskRequest) -> Dict[str, Any]:
    """Execute le workflow avec gestion de r√©cup√©ration d'√©tat."""

    # Initialiser la persistence si n√©cessaire
    if not db_persistence.pool:
        await db_persistence.initialize()

    # ‚úÖ CORRECTION: D√©clarer toutes les variables au d√©but du scope
    task_db_id = None
    actual_task_run_id = None  # ID entier de la base
    uuid_task_run_id = None    # UUID pour LangSmith
    # G√©n√©rer task_run_id unique imm√©diatement
    task_run_id = f"run_{uuid.uuid4().hex[:12]}_{int(time.time())}"

    try:
        # Si task_request contient un monday_item_id, cr√©er la t√¢che
        if hasattr(task_request, 'monday_item_id'):
            monday_payload = {
                "pulseId": task_request.monday_item_id,
                "pulseName": task_request.title,
                "boardId": getattr(task_request, 'board_id', None),
                "columnValues": {}
            }
            task_db_id = await db_persistence.create_task_from_monday(monday_payload)
            logger.info(f"‚úÖ T√¢che cr√©√©e en base: task_db_id={task_db_id}")

        # ‚úÖ TOUJOURS cr√©er un task_run en base, m√™me si task_db_id=None
        logger.info(f"üîÑ Tentative cr√©ation task_run avec task_db_id={task_db_id}, task_run_id={task_run_id}")
        actual_task_run_id = await db_persistence.start_task_run(
            task_db_id,  # peut √™tre None si pas de Monday
            workflow_id,
            task_run_id
        )

        if actual_task_run_id:
            uuid_task_run_id = task_run_id
            logger.info(f"‚úÖ Task run cr√©√©: actual_id={actual_task_run_id}, uuid={uuid_task_run_id}")
        else:
            logger.warning("‚ö†Ô∏è Aucun task_run_id g√©n√©r√© - workflow sans persistence")

        # ‚úÖ NOUVEAU: Cr√©er un √©tat initial enrichi avec recovery
        initial_state = _create_initial_state_with_recovery(task_request, workflow_id, actual_task_run_id, uuid_task_run_id)

                # Cr√©er et compiler le graphe
        workflow_graph = create_workflow_graph()
        
        # Compiler avec un checkpointer pour g√©rer l'√©tat
        checkpointer = MemorySaver()
        compiled_graph = workflow_graph.compile(checkpointer=checkpointer)
        
        # Ex√©cuter le workflow avec monitoring
        workflow_start_time = time.time()
        final_state = None
        status = "unknown"
        success = False
        workflow_error = None

        # ‚úÖ NOUVEAU: Ex√©cuter avec gestion de r√©cup√©ration
        async for event in _execute_workflow_with_recovery(compiled_graph, initial_state, workflow_id):
            event_type = event.get("type")

            if event_type == "step":
                node_name = event.get("node")
                node_output = event.get("output")
                logger.info(f"üîÑ Ex√©cution du n≈ìud: {node_name}")

                # ‚úÖ NOUVEAU: Sauvegarder l'√©tat apr√®s chaque n≈ìud
                await _save_node_checkpoint(actual_task_run_id, node_name, node_output)
                
                # ‚úÖ CORRECTION: Mettre √† jour final_state √† chaque √©tape
                final_state = node_output
                
                # ‚úÖ V√âRIFICATION: Si c'est le dernier n≈ìud (update_monday), marquer comme termin√©
                if node_name == "update_monday":
                    status = final_state.get("results", {}).get("current_status", "completed")
                    success = final_state.get("results", {}).get("success", True)
                    workflow_error = final_state.get("results", {}).get("error", None)
                    logger.info(f"üèÅ Dernier n≈ìud ex√©cut√© (update_monday). Statut: {status}, Succ√®s: {success}")
                    # Ne pas break ici, laisser le workflow se terminer naturellement

            elif event_type == "error":
                logger.error(f"‚ùå Erreur dans le workflow: {event.get('error')}")
                workflow_error = event.get('error')
                final_state = event.get("state", initial_state)
                status = "failed"
                success = False
                break

            elif event_type == "end":
                final_state = event["output"]
                status = final_state.get("results", {}).get("current_status", "completed")
                success = final_state.get("results", {}).get("success", True)
                workflow_error = final_state.get("results", {}).get("error", None)
                if workflow_error:
                    success = False # Si une erreur est pr√©sente, ce n'est pas un succ√®s
                logger.info(f"üèÅ Workflow termin√© via END. Statut: {status}, Succ√®s: {success}")
                break

        # ‚úÖ AM√âLIORATION: Meilleure gestion de l'√©tat final
        if final_state is None:
            final_state = initial_state # Utiliser l'√©tat initial comme fallback
            workflow_error = "Workflow termin√© sans √©tat final clair"
            success = False
            status = "failed"
            logger.error(f"‚ùå {workflow_error}")
        else:
            # ‚úÖ NOUVEAU: V√©rification finale de l'√©tat
            if not isinstance(final_state, dict):
                logger.warning(f"‚ö†Ô∏è √âtat final inattendu (type: {type(final_state)}), conversion en dict")
                final_state = {"results": {}, "error": "√âtat final invalide"}
                success = False
                status = "failed"
            
            # Si pas encore d√©termin√©, extraire du final_state
            if 'success' not in locals() or success is None:
                success = final_state.get("results", {}).get("success", True)
            if 'status' not in locals() or status == "unknown":
                status = final_state.get("results", {}).get("current_status", "completed")
            if 'workflow_error' not in locals():
                workflow_error = final_state.get("results", {}).get("error", None)
            
            logger.info(f"‚úÖ √âtat final trait√©: status={status}, success={success}, error={workflow_error}")

        duration = time.time() - workflow_start_time

        # ‚úÖ NOUVEAU: Finaliser la persistence avec √©tat de r√©cup√©ration
        await _finalize_workflow_run(actual_task_run_id, success, workflow_error, final_state)

        # Retourner le r√©sultat final du workflow
        return _create_workflow_result(task_request, final_state, duration, success, workflow_error)

    except Exception as e:
        logger.error(f"‚ùå Erreur critique workflow {workflow_id}: {e}", exc_info=True)
        return _create_error_result(task_request, str(e))

async def _execute_workflow_with_recovery(compiled_graph, initial_state: Dict[str, Any], workflow_id: str) -> AsyncIterator[Dict[str, Any]]:
    """Execute le workflow n≈ìud par n≈ìud avec r√©cup√©ration d'erreur."""

    node_count = 0
    max_nodes = WorkflowLimits.MAX_NODES_SAFETY_LIMIT

    try:
        async for event in compiled_graph.astream(initial_state, config={"configurable": {"thread_id": workflow_id}}):
            node_count += 1

            # V√©rifier la limite de n≈ìuds
            if node_count > max_nodes:
                logger.error(f"‚ö†Ô∏è Limite de n≈ìuds atteinte ({node_count}/{max_nodes}) - arr√™t du workflow")
                yield {
                    "type": "error",
                    "error": f"Arr√™t forc√© - limite de {max_nodes} n≈ìuds atteinte",
                    "state": initial_state
                }
                return

            # ‚úÖ NOUVEAU: Timeout par n≈ìud avec retry
            try:
                yield await asyncio.wait_for(_process_node_event(event), timeout=NODE_TIMEOUT_SECONDS)
            except asyncio.TimeoutError:
                logger.warning(f"‚ö†Ô∏è Timeout du n≈ìud apr√®s {NODE_TIMEOUT_SECONDS}s - tentative de r√©cup√©ration")

                # Tenter de r√©cup√©rer avec l'√©tat actuel
                yield {
                    "type": "error",
                    "error": f"Timeout du n≈ìud apr√®s {NODE_TIMEOUT_SECONDS}s",
                    "state": event.get("output", initial_state)
                }
                return

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution du workflow: {e}", exc_info=True)
        yield {
            "type": "error",
            "error": str(e),
            "state": initial_state
        }

async def _process_node_event(event: Dict[str, Any]) -> Dict[str, Any]:
    """Traite un √©v√©nement de n≈ìud avec logging."""
    # V√©rifier d'abord si c'est un √©v√©nement de fin
    if "__end__" in event:
        return {
            "type": "end",
            "output": event.get("__end__", event)
        }
    
    # Sinon, traiter les n≈ìuds normaux
    for node_name, node_output in event.items():
        if node_name != "__end__":
            return {
                "type": "step",
                "node": node_name,
                "output": node_output
            }

    # Si aucun n≈ìud trouv√©, consid√©rer comme fin
    return {
        "type": "end",
        "output": event
    }

def _create_initial_state_with_recovery(task_request: TaskRequest, workflow_id: str, actual_task_run_id: Optional[int], uuid_task_run_id: Optional[str]) -> Dict[str, Any]:
    """Cr√©e l'√©tat initial avec support de r√©cup√©ration."""

    initial_state = {
        "task": task_request,
        "workflow_id": workflow_id,
        "run_id": actual_task_run_id,
        "uuid_run_id": uuid_task_run_id,
        "results": {},
        "error": None,
        "current_node": None,
        "completed_nodes": [],
        "node_retry_count": {},  # ‚úÖ NOUVEAU: Compteur de retry par n≈ìud
        "recovery_mode": False,  # ‚úÖ NOUVEAU: Mode r√©cup√©ration
        "checkpoint_data": {}    # ‚úÖ NOUVEAU: Donn√©es de checkpoint
    }

    return initial_state

async def _save_node_checkpoint(task_run_id: Optional[int], node_name: str, node_output: Dict[str, Any]):
    """Sauvegarde un checkpoint apr√®s chaque n≈ìud."""
    if not task_run_id:
        return

    try:
        checkpoint_data = {
            "node_name": node_name,
            "completed_at": datetime.now().isoformat(),
            "output_summary": {
                "has_results": bool(node_output.get("results")),
                "has_error": bool(node_output.get("error")),
                "current_status": node_output.get("results", {}).get("current_status", "unknown")
            }
        }

        await db_persistence.save_node_checkpoint(task_run_id, node_name, checkpoint_data)
        logger.debug(f"üíæ Checkpoint sauv√© pour {node_name}")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur sauvegarde checkpoint {node_name}: {e}")

async def _finalize_workflow_run(task_run_id: Optional[int], success: bool, error: Optional[str], final_state: Dict[str, Any]):
    """Finalise le workflow run avec l'√©tat final."""
    if not task_run_id:
        return

    try:
        # Calculer les m√©triques finales
        metrics = {
            "success": success,
            "error": error,
            "completed_nodes": final_state.get("completed_nodes", []),
            "final_status": final_state.get("results", {}).get("current_status", "unknown")
        }

        status = "completed" if success else "failed"
        await db_persistence.complete_task_run(task_run_id, status, metrics, error)
        logger.info(f"‚úÖ Workflow run {task_run_id} finalis√©")

    except Exception as e:
        logger.error(f"‚ùå Erreur finalisation workflow run {task_run_id}: {e}")

def _create_timeout_result(task_request: TaskRequest, workflow_id: str, timeout_type: str) -> Dict[str, Any]:
    """Cr√©e un r√©sultat pour un timeout de workflow."""
    return {
        "success": False,
        "status": "timeout",
        "error": f"Timeout {timeout_type} du workflow",
        "workflow_id": workflow_id,
        "task_id": task_request.task_id,
        "task_title": task_request.title,
        "duration": WORKFLOW_TIMEOUT_SECONDS if timeout_type == "global_timeout" else NODE_TIMEOUT_SECONDS,
        "results": {
            "current_status": "timeout",
            "error": f"Workflow interrompu par timeout {timeout_type}",
            "success": False
        }
    }

def _create_initial_state(task_request: TaskRequest, workflow_id: str, task_db_id: Optional[int] = None, actual_task_run_id: Optional[int] = None) -> GraphState:
    """Cr√©e l'√©tat initial du workflow sous forme de TypedDict."""
    return {
        "workflow_id": workflow_id,
        "status": WorkflowStatus.PENDING,
        "current_node": None,
        "completed_nodes": [],
        "task": task_request,
        "db_task_id": task_db_id,  # Ajout direct
        "db_run_id": actual_task_run_id, # Ajout direct
        "results": {
            "ai_messages": [],
            "error_logs": [],
            "modified_files": [],
            "test_results": [],
            "debug_attempts": 0
        },
        "error": None,
        "started_at": datetime.now(),
        "completed_at": None
    }

def _process_final_result(final_state: GraphState, task_request: TaskRequest) -> Dict[str, Any]:
    """
    Traite l'√©tat final du workflow et g√©n√®re le r√©sultat.

    Args:
        final_state: √âtat final du workflow
        task_request: Requ√™te de t√¢che originale

    Returns:
        Dictionnaire avec le r√©sultat du workflow
    """

    # ‚úÖ CORRECTION: Logique de succ√®s plus nuanc√©e et tol√©rante
    current_status = final_state.get('status', WorkflowStatus.PENDING)
    results = final_state.get("results") or {}
    completed_nodes = final_state.get("completed_nodes") or []
    error_message = final_state.get("error")

    # Crit√®res de succ√®s am√©lior√©s (plus tolerants)
    success = False

    if current_status == WorkflowStatus.COMPLETED:
        success = True
    elif len(completed_nodes) >= 3:  # Au moins 3 n≈ìuds compl√©t√©s = succ√®s partiel
        # V√©rifier si des √©tapes importantes sont termin√©es
        important_nodes = ["requirements_analysis", "code_generation", "quality_assurance"]
        completed_important = sum(1 for node in important_nodes if node in completed_nodes)

        if completed_important >= 2:  # Au moins 2 √©tapes importantes
            success = True
            logger.info(f"üü° Succ√®s partiel - {completed_important}/3 √©tapes importantes compl√©t√©es")

    # R√©√©valuer le succ√®s m√™me en cas d'erreur si beaucoup d'√©tapes sont termin√©es
    if not success and len(completed_nodes) >= 5:
        # Si beaucoup d'√©tapes sont faites mais qu'il y a une erreur finale (ex: tests)
        if any(key in results for key in ["code_changes", "pr_info", "quality_assurance"]):
            success = True
            logger.info(f"üü° Succ√®s avec erreurs mineures - {len(completed_nodes)} n≈ìuds compl√©t√©s")

    # Calculer la dur√©e
    duration = 0
    started_at = final_state.get("started_at")
    if started_at:
        end_time = final_state.get("completed_at") or datetime.now()
        duration = (end_time - started_at).total_seconds()

    # Les variables sont d√©j√† d√©finies plus haut, pas besoin de les red√©finir

    # Extraire les m√©triques depuis les r√©sultats
    pr_url = None
    if "pr_info" in results:
        pr_info = results["pr_info"]
        if isinstance(pr_info, dict):
            pr_url = pr_info.get("pr_url")
        else:
            pr_url = getattr(pr_info, "pr_url", None)

    # Calculer des m√©triques enrichies
    files_modified = 0
    tests_executed = 0
    qa_score = 0
    analysis_score = 0

    if "code_changes" in results:
        files_modified = len(results["code_changes"]) if isinstance(results["code_changes"], dict) else 0

    if "test_results" in results:
        test_results = results["test_results"]
        if isinstance(test_results, dict):
            tests_executed = test_results.get("total_tests", 0)
        elif isinstance(test_results, list):
            tests_executed = len(test_results)
        elif hasattr(test_results, 'total_tests'):
            # C'est un objet TestResult, extraire l'attribut
            tests_executed = getattr(test_results, 'total_tests', 0)

    # M√©triques QA
    if "quality_assurance" in results:
        qa_data = results["quality_assurance"]
        qa_score = qa_data.get("qa_summary", {}).get("overall_score", 0)

    # M√©triques d'analyse
    if "requirements_analysis" in results:
        analysis_data = results["requirements_analysis"]
        analysis_score = analysis_data.get("complexity_score", 5)

    # Construire le r√©sultat enrichi
    result = {
        "success": success,
        "status": current_status.value if current_status else "unknown",
        "workflow_id": final_state.get("workflow_id"),
        "task_id": task_request.task_id,
        "duration": duration,
        "completed_nodes": completed_nodes,
        "pr_url": pr_url,
        "error": error_message,
        "metrics": {
            "files_modified": files_modified,
            "tests_executed": tests_executed,
            "nodes_completed": len(completed_nodes),
            "duration_seconds": duration,
            "qa_score": qa_score,
            "analysis_complexity": analysis_score,
            "workflow_completeness": len(completed_nodes) / MAX_WORKFLOW_NODES * 100  # 11 n≈ìuds au total
        },
        "results": results
    }

    logger.info(f"üìä Workflow termin√© - Succ√®s: {success}, Dur√©e: {duration:.1f}s, N≈ìuds: {len(completed_nodes)}, QA: {qa_score}")

    return result

def _create_error_result(task_request: TaskRequest, error_msg: str) -> Dict[str, Any]:
    """Cr√©e un r√©sultat d'erreur standardis√©."""

    return {
        "success": False,
        "status": "failed",
        "workflow_id": f"error_{task_request.task_id}",
        "task_id": task_request.task_id,
        "duration": 0,
        "completed_nodes": [],
        "pr_url": None,
        "error": error_msg,
        "metrics": {
            "files_modified": 0,
            "tests_executed": 0,
            "nodes_completed": 0,
            "duration_seconds": 0,
            "qa_score": 0,
            "analysis_complexity": 0,
            "workflow_completeness": 0
        },
        "results": {}
    }

def _ensure_final_state(state: GraphState) -> GraphState:
    """
    Garantit que l'√©tat final est bien d√©fini avec tous les champs requis.

    Args:
        state: √âtat du workflow (potentiellement incomplet)

    Returns:
        √âtat compl√©t√© avec tous les champs requis pour un r√©sultat final
    """
    if not isinstance(state, dict):
        # Cr√©er un √©tat minimal si state n'est pas un dict
        state = {}

    # Assurer que les champs de base existent
    if "workflow_id" not in state:
        state["workflow_id"] = f"unknown_{int(time.time())}"

    if "status" not in state:
        state["status"] = WorkflowStatus.FAILED

    if "results" not in state:
        state["results"] = {}

    # Assurer que results contient les champs requis
    results = state["results"]
    required_result_fields = [
        "ai_messages", "error_logs", "modified_files",
        "test_results", "debug_attempts", "current_status",
        "success"
    ]

    for field in required_result_fields:
        if field not in results:
            if field == "success":
                results[field] = False
            elif field == "current_status":
                results[field] = "failed"
            elif field in ["ai_messages", "error_logs", "modified_files", "test_results"]:
                results[field] = []
            elif field == "debug_attempts":
                results[field] = 0

    # Assurer un message d'erreur si absent
    if "error" not in state and not results.get("success", False):
        state["error"] = "Workflow termin√© sans √©tat final clair"
        results["error"] = state["error"]

    # Timestamps de d√©but/fin
    if "started_at" not in state:
        state["started_at"] = datetime.now()

    if "completed_at" not in state:
        state["completed_at"] = datetime.now()

    # Liste des n≈ìuds compl√©t√©s
    if "completed_nodes" not in state:
        state["completed_nodes"] = []

    logger.info(f"‚úÖ √âtat final normalis√©: succ√®s={results.get('success')}, status={results.get('current_status')}")

    return state

def _create_workflow_result(task_request: TaskRequest, final_state: Dict[str, Any], duration: float, success: bool, error: Optional[str]) -> Dict[str, Any]:
    """Cr√©e le r√©sultat final du workflow."""

    # Extraire les informations importantes avec protection des types
    if isinstance(final_state, dict):
        final_results = final_state.get("results", {})
        completed_nodes = final_state.get("completed_nodes", [])
    else:
        # Si c'est un objet GraphState
        final_results = getattr(final_state, "results", {})
        completed_nodes = getattr(final_state, "completed_nodes", [])

    # Extraire les m√©triques depuis les r√©sultats
    pr_url = None
    if "pr_info" in final_results:
        pr_info = final_results["pr_info"]
        if isinstance(pr_info, dict):
            pr_url = pr_info.get("pr_url")
        else:
            pr_url = getattr(pr_info, "pr_url", None)

    # Calculer des m√©triques enrichies
    files_modified = 0
    tests_executed = 0

    if "code_changes" in final_results:
        files_modified = len(final_results["code_changes"]) if isinstance(final_results["code_changes"], dict) else 0

    if "test_results" in final_results:
        test_results = final_results["test_results"]
        if isinstance(test_results, dict):
            tests_executed = test_results.get("total_tests", 0)
        elif isinstance(test_results, list):
            tests_executed = len(test_results)

    return {
        "success": success,
        "status": "completed" if success else "failed",
        "error": error,
        "workflow_id": final_state.get("workflow_id", "unknown"),
        "task_id": task_request.task_id,
        "task_title": task_request.title,
        "duration": duration,
        "completed_nodes": completed_nodes,
        "files_modified": files_modified,
        "tests_executed": tests_executed,
        "pr_url": pr_url,
        "results": final_results
    }
