"""N≈ìud d'analyse des requirements - analyse les sp√©cifications et g√©n√®re un plan."""

from typing import Dict, Any
from models.state import GraphState
from tools.ai_engine_hub import ai_hub, AIRequest, TaskType
from utils.logger import get_logger
from utils.persistence_decorator import with_persistence

logger = get_logger(__name__)


@with_persistence("analyze_requirements")
async def analyze_requirements(state: GraphState) -> GraphState:
    """
    N≈ìud d'analyse des requirements : analyse la t√¢che et d√©termine la strat√©gie d'impl√©mentation.
    
    Ce n≈ìud :
    1. Parse la description de la t√¢che
    2. Identifie les composants techniques n√©cessaires
    3. D√©termine la complexit√© et les risques
    4. Pr√©pare le contexte pour l'impl√©mentation
    
    Args:
        state: √âtat actuel du graphe
        
    Returns:
        √âtat mis √† jour avec l'analyse des requirements
    """
    logger.info(f"üîç Analyse des requirements pour: {state['task'].title}")
    
    # ‚úÖ CORRECTION CRITIQUE: Assurer l'int√©grit√© de l'√©tat d√®s le d√©but
    from utils.error_handling import ensure_state_integrity
    ensure_state_integrity(state)

    state["results"]["current_status"] = "analyzing"
    state["results"]["ai_messages"].append("D√©but de l'analyse des requirements...")
    
    # Marquer le n≈ìud comme en cours
    state["current_node"] = "analyze_requirements"
    
    try:
        task = state["task"]
        
        # 1. Pr√©parer le contexte d'analyse
        analysis_context = {
            "task_title": task.title,
            "description": task.description,
            "task_type": task.task_type,
            "priority": task.priority,
            "acceptance_criteria": task.acceptance_criteria,
            "technical_context": task.technical_context,
            "files_to_modify": task.files_to_modify,
            "estimated_complexity": task.estimated_complexity,
            "repository_url": task.repository_url
        }
        
        # 2. Cr√©er le prompt d'analyse d√©taill√©
        analysis_prompt = _create_analysis_prompt(analysis_context)
        
        logger.info("ü§ñ G√©n√©ration du plan d'analyse avec AI Engine Hub...")
        
        # 3. Utiliser l'AI Engine Hub pour analyser
        # Enrichir le contexte avec les IDs pour le monitoring
        analysis_context["workflow_id"] = state.get("workflow_id", "unknown")
        analysis_context["task_id"] = task.task_id
        
        ai_request = AIRequest(
            prompt=analysis_prompt,
            task_type=TaskType.ANALYSIS,
            context=analysis_context
        )
        
        response = await ai_hub.analyze_requirements(ai_request)
        
        if not response.success:
            error_msg = f"Erreur lors de l'analyse des requirements: {response.error}"
            logger.error(error_msg)
            state["error"] = error_msg
            return state
        
        # 4. Parser et structurer la r√©ponse d'analyse
        analysis_result = _parse_analysis_response(response.content)
        
        # 5. Enrichir l'√©tat avec les r√©sultats d'analyse
        if not state["results"]:
            state["results"] = {}
            
        state["results"]["requirements_analysis"] = analysis_result
        
        # 6. Mettre √† jour les informations de la t√¢che si n√©cessaire
        if analysis_result.get("refined_files_to_modify"):
            task.files_to_modify = analysis_result["refined_files_to_modify"]
        
        if analysis_result.get("refined_complexity"):
            task.estimated_complexity = analysis_result["refined_complexity"]
        
        # 7. Logs de r√©sum√©
        implementation_plan = analysis_result.get("implementation_plan", {})
        estimated_effort = analysis_result.get("estimated_effort", "Unknown")
        risk_level = analysis_result.get("risk_level", "Medium")
        
        logger.info("‚úÖ Analyse requirements termin√©e",
                   estimated_effort=estimated_effort,
                   risk_level=risk_level,
                   files_count=len(analysis_result.get("refined_files_to_modify", [])),
                   steps_count=len(implementation_plan.get("steps", [])))
        
        # 8. Pr√©parer les informations pour les n≈ìuds suivants
        state["results"]["analysis_summary"] = {
            "complexity_score": analysis_result.get("complexity_score", 5),
            "estimated_duration_minutes": analysis_result.get("estimated_duration_minutes", 30),
            "requires_external_deps": analysis_result.get("requires_external_deps", False),
            "breaking_changes_risk": analysis_result.get("breaking_changes_risk", False),
            "test_strategy": analysis_result.get("test_strategy", "unit"),
            "implementation_approach": analysis_result.get("implementation_approach", "standard")
        }
        
        return state
        
    except Exception as e:
        error_msg = f"Exception lors de l'analyse des requirements: {str(e)}"
        logger.error(error_msg, exc_info=True)
        state["error"] = error_msg
        return state


def _create_analysis_prompt(context: Dict[str, Any]) -> str:
    """Cr√©e un prompt d√©taill√© pour l'analyse des requirements."""
    
    prompt = f"""
# üîç ANALYSE D√âTAILL√âE DES REQUIREMENTS - AI-Agent

Tu es un expert en analyse de requirements pour le d√©veloppement logiciel automatis√©.
Analyse en profondeur cette t√¢che et g√©n√®re un plan d'impl√©mentation structur√©.

## üìã INFORMATIONS DE LA T√ÇCHE

**Titre**: {context['task_title']}
**Type**: {context['task_type']}
**Priorit√©**: {context['priority']}

**Description**: 
{context['description']}

**Crit√®res d'acceptation**:
{context.get('acceptance_criteria', 'Non sp√©cifi√©s')}

**Contexte technique**:
{context.get('technical_context', 'Non sp√©cifi√©')}

**Fichiers sugg√©r√©s √† modifier**:
{context.get('files_to_modify', 'Non sp√©cifi√©s')}

**Complexit√© estim√©e initiale**: {context.get('estimated_complexity', 'Non √©valu√©e')}

**Repository**: {context['repository_url']}

## üéØ T√ÇCHES D'ANALYSE REQUISES

Fournis une analyse structur√©e au format JSON avec les cl√©s suivantes :

```json
{{
    "summary": "R√©sum√© en 2-3 phrases de ce qui doit √™tre fait",
    "complexity_analysis": {{
        "complexity_score": "Nombre de 1 √† 10",
        "complexity_factors": ["Liste des facteurs de complexit√©"],
        "technical_challenges": ["D√©fis techniques identifi√©s"]
    }},
    "implementation_plan": {{
        "approach": "Approche d'impl√©mentation recommand√©e",
        "steps": [
            {{
                "step": 1,
                "description": "Description de l'√©tape",
                "estimated_time_minutes": 15,
                "dependencies": ["D√©pendances de cette √©tape"],
                "deliverables": ["Livrables de cette √©tape"]
            }}
        ]
    }},
    "files_analysis": {{
        "refined_files_to_modify": ["Liste affin√©e des fichiers √† modifier"],
        "new_files_to_create": ["Nouveaux fichiers √† cr√©er"],
        "files_to_test": ["Fichiers n√©cessitant des tests sp√©cifiques"]
    }},
    "requirements_breakdown": {{
        "functional_requirements": ["Requirements fonctionnels"],
        "non_functional_requirements": ["Requirements non-fonctionnels"],
        "acceptance_criteria_refined": ["Crit√®res d'acceptation d√©taill√©s"]
    }},
    "risk_assessment": {{
        "risk_level": "Low/Medium/High",
        "potential_risks": ["Risques identifi√©s"],
        "mitigation_strategies": ["Strat√©gies d'att√©nuation"]
    }},
    "testing_strategy": {{
        "test_types_needed": ["unit", "integration", "e2e"],
        "test_scenarios": ["Sc√©narios de test cl√©s"],
        "edge_cases": ["Cas limites √† tester"]
    }},
    "external_dependencies": {{
        "requires_external_deps": false,
        "new_packages_needed": ["Nouveaux packages requis"],
        "api_integrations": ["Int√©grations API n√©cessaires"]
    }},
    "estimated_effort": {{
        "estimated_duration_minutes": 45,
        "confidence_level": "High/Medium/Low",
        "effort_breakdown": {{
            "analysis": 10,
            "implementation": 25,
            "testing": 10,
            "debugging": 5,
            "documentation": 5
        }}
    }},
    "success_criteria": {{
        "definition_of_done": ["Crit√®res de fin de t√¢che"],
        "quality_gates": ["Seuils de qualit√© √† respecter"],
        "acceptance_tests": ["Tests d'acceptation √† valider"]
    }}
}}
```

## üö® INSTRUCTIONS IMPORTANTES

1. **Sois sp√©cifique** : √âvite les g√©n√©ralit√©s, donne des d√©tails concrets
2. **Pense aux d√©pendances** : Identifie les interconnexions entre composants
3. **Consid√®re la maintenance** : L'impact sur le code existant
4. **Anticipe les probl√®mes** : Les points de friction potentiels
5. **Optimise pour l'automatisation** : Plan adapt√© √† l'ex√©cution par AI-Agent

R√©ponds UNIQUEMENT avec le JSON structur√©, sans texte additionnel.
"""
    
    return prompt


def _parse_analysis_response(response_content: str) -> Dict[str, Any]:
    """Parse et valide la r√©ponse d'analyse IA avec gestion intelligente code/JSON."""
    
    import json
    import re
    
    try:
        logger.info(f"üîç Parsing r√©ponse IA: {response_content[:200]}...")
        
        # √âtape 1: D√©tecter si c'est du code ou du JSON
        if _is_code_response(response_content):
            logger.info("üîç D√©tection: R√©ponse contient du code - Extraction intelligente")
            return _extract_analysis_from_code_response(response_content)
        
        # √âtape 2: Rechercher JSON dans diff√©rents formats de blocs
        json_str = None
        
        # Rechercher blocs JSON markdown
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',  # ```json { ... } ```
            r'```javascript\s*(\{.*?\})\s*```',  # ```javascript { ... } ```
            r'```js\s*(\{.*?\})\s*```',  # ```js { ... } ```
            r'```\s*(\{.*?\})\s*```',  # ``` { ... } ```
        ]
        
        for pattern in json_patterns:
            match = re.search(pattern, response_content, re.DOTALL)
            if match:
                json_str = match.group(1)
                logger.info(f"‚úÖ JSON trouv√© avec pattern: {pattern[:20]}...")
                break
        
        # Fallback : rechercher directement un objet JSON
        if not json_str:
            json_match = re.search(r'\{[^}]*(?:\{[^}]*\}[^}]*)*\}', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                logger.info("‚úÖ JSON trouv√© par regex directe")
            else:
                logger.warning("‚ö†Ô∏è Aucun JSON d√©tect√© - G√©n√©ration analyse par d√©faut")
                return _generate_analysis_from_text(response_content)
        
        # √âtape 2: Nettoyer et convertir JavaScript en JSON valide
        json_str = json_str.strip()
        logger.info(f"üßπ Nettoyage JSON: {json_str[:100]}...")
        
        # Supprimer les commentaires JavaScript
        json_str = re.sub(r'//.*?\n', '\n', json_str)
        json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
        
        # Convertir guillemets simples en doubles (cl√©s et valeurs)
        json_str = re.sub(r"'([^']*)'(\s*:)", r'"\1"\2', json_str)  # Cl√©s
        json_str = re.sub(r":\s*'([^']*)'", r': "\1"', json_str)   # Valeurs
        
        # Nettoyer les virgules trainantes
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # Nettoyer les retours √† la ligne dans les strings
        json_str = re.sub(r'\n\s*', ' ', json_str)
        
        # Corriger les propri√©t√©s JavaScript sans guillemets
        json_str = re.sub(r'(\w+)(\s*:)', r'"\1"\2', json_str)
        
        logger.info(f"‚úÖ JSON nettoy√©: {json_str[:100]}...")
        
        # √âtape 3: Parser le JSON
        try:
            analysis_result = json.loads(json_str)
            logger.info("‚úÖ JSON pars√© avec succ√®s")
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è Erreur JSON parsing: {e}")
            logger.info("üîß Tentative r√©paration du JSON...")
            # Tentative de r√©paration automatique du JSON
            try:
                json_str = _repair_json(json_str)
                analysis_result = json.loads(json_str)
                logger.info("‚úÖ JSON r√©par√© et pars√© avec succ√®s")
            except Exception as repair_error:
                logger.error(f"‚ùå Impossible de r√©parer le JSON: {repair_error}")
                logger.error(f"JSON probl√©matique: {json_str[:500]}...")
                return _get_default_analysis_with_error(f"JSON invalide: {str(e)}")
        
        # Validation et valeurs par d√©faut
        default_analysis = {
            "summary": "Analyse en cours...",
            "complexity_score": 5,
            "estimated_duration_minutes": 30,
            "risk_level": "Medium",
            "refined_files_to_modify": [],
            "implementation_plan": {"steps": []},
            "requires_external_deps": False,
            "breaking_changes_risk": False,
            "test_strategy": "unit",
            "implementation_approach": "standard"
        }
        
        # Fusionner avec les valeurs par d√©faut
        for key, default_value in default_analysis.items():
            if key not in analysis_result:
                analysis_result[key] = default_value
        
        # Extraire les valeurs importantes vers le niveau racine
        if "complexity_analysis" in analysis_result:
            analysis_result["complexity_score"] = analysis_result["complexity_analysis"].get("complexity_score", 5)
        
        if "estimated_effort" in analysis_result:
            analysis_result["estimated_duration_minutes"] = analysis_result["estimated_effort"].get("estimated_duration_minutes", 30)
        
        if "risk_assessment" in analysis_result:
            analysis_result["risk_level"] = analysis_result["risk_assessment"].get("risk_level", "Medium")
        
        if "files_analysis" in analysis_result:
            analysis_result["refined_files_to_modify"] = analysis_result["files_analysis"].get("refined_files_to_modify", [])
        
        if "external_dependencies" in analysis_result:
            analysis_result["requires_external_deps"] = analysis_result["external_dependencies"].get("requires_external_deps", False)
        
        return analysis_result
        
    except json.JSONDecodeError as e:
        logger.error(f"Erreur parsing JSON analyse: {e}")
        if 'json_str' in locals():
            logger.error(f"JSON probl√©matique (premiers 500 chars): {json_str[:500]}")
            logger.error(f"Erreur √† la position {e.pos}: caract√®re '{json_str[e.pos:e.pos+10] if e.pos < len(json_str) else 'fin'}'")
            
            # Tentative de r√©paration avanc√©e
            try:
                repaired_json = _advanced_json_repair(json_str)
                logger.info("Tentative de r√©paration JSON avanc√©e...")
                analysis_result = json.loads(repaired_json)
                logger.info("‚úÖ R√©paration JSON r√©ussie!")
                
                # Fusionner avec les valeurs par d√©faut apr√®s r√©paration
                default_analysis = {
                    "summary": "Analyse en cours...",
                    "complexity_score": 5,
                    "estimated_duration_minutes": 30,
                    "risk_level": "Medium",
                    "refined_files_to_modify": [],
                    "implementation_plan": {"steps": []},
                    "requires_external_deps": False,
                    "breaking_changes_risk": False,
                    "test_strategy": "unit",
                    "implementation_approach": "standard"
                }
                
                for key, default_value in default_analysis.items():
                    if key not in analysis_result:
                        analysis_result[key] = default_value
                        
                return analysis_result
                
            except Exception as repair_error:
                logger.error(f"√âchec r√©paration JSON avanc√©e: {repair_error}")
        else:
            logger.error("Variable json_str non disponible pour debug")
            
        return _get_default_analysis_with_error(f"Erreur parsing JSON: {str(e)}", response_content)
        
    except Exception as e:
        logger.error(f"Erreur inattendue parsing analyse: {e}")
        return _get_default_analysis_with_error(f"Erreur inattendue: {str(e)}", response_content, "High")


def _repair_json(json_str: str) -> str:
    """Tentative de r√©paration automatique d'un JSON malform√©."""
    import re
    
    # Supprimer les commentaires JavaScript
    json_str = re.sub(r'//.*?\n', '', json_str)
    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
    
    # Corriger les virgules en trop
    json_str = re.sub(r',\s*}', '}', json_str)
    json_str = re.sub(r',\s*]', ']', json_str)
    
    # Ajouter des guillemets manquants aux cl√©s (seulement si pas d√©j√† pr√©sents)
    json_str = re.sub(r'(?<!")(\w+)(?!"):', r'"\1":', json_str)
    
    # Remplacer les guillemets simples par des doubles dans les valeurs
    json_str = re.sub(r":\s*'([^']*)'", r': "\1"', json_str)
    
    # Corriger les valeurs bool√©ennes
    json_str = re.sub(r':\s*true\b', ': true', json_str)
    json_str = re.sub(r':\s*false\b', ': false', json_str)
    json_str = re.sub(r':\s*null\b', ': null', json_str)
    
    return json_str


def _advanced_json_repair(json_str: str) -> str:
    """R√©paration JSON avanc√©e avec d√©tection d'erreurs sp√©cifiques."""
    import re
    logger.info("üîß D√©but r√©paration JSON avanc√©e...")
    
    # 1. Supprimer les balises markdown JSON si pr√©sentes
    json_str = re.sub(r'^```json\s*', '', json_str)
    json_str = re.sub(r'\s*```$', '', json_str)
    
    # 2. Nettoyer les caract√®res invisibles et de contr√¥le
    json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
    
    # 3. Corriger les cl√©s sans guillemets (SIMPLE et efficace)
    json_str = re.sub(r'(\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_str)
    
    # 4. Corriger les guillemets simples en doubles pour les valeurs (SIMPLE)
    json_str = re.sub(r":\s*'([^']*)'", r': "\1"', json_str)
    
    # 5. Supprimer les virgules en trop avant les accolades/crochets fermants
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    
    # 6. Corriger les valeurs bool√©ennes Python en JSON
    json_str = re.sub(r':\s*True\b', ': true', json_str)
    json_str = re.sub(r':\s*False\b', ': false', json_str)
    json_str = re.sub(r':\s*None\b', ': null', json_str)
    
    # 7. Supprimer les commentaires JavaScript/Python
    json_str = re.sub(r'//.*?\n', '\n', json_str)
    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
    json_str = re.sub(r'#.*?\n', '\n', json_str)
    
    # 8. Normaliser les espaces multiples
    json_str = re.sub(r'\s+', ' ', json_str)
    json_str = json_str.strip()
    
    logger.info(f"üîß JSON apr√®s r√©paration (premiers 200 chars): {json_str[:200]}")
    
    return json_str


def _is_code_response(response: str) -> bool:
    """D√©tecte si la r√©ponse contient principalement du code plut√¥t que du JSON."""
    code_indicators = [
        'def ', 'function ', 'class ', 'import ', 'from ',
        'const ', 'let ', 'var ', 'if __name__',
        '```python', '```javascript', '```js', '```typescript'
    ]
    
    json_indicators = [
        '"summary":', '"complexity_score":', '"implementation_plan":',
        '"estimated_duration":', '"risk_level":'
    ]
    
    code_score = sum(1 for indicator in code_indicators if indicator in response)
    json_score = sum(1 for indicator in json_indicators if indicator in response)
    
    return code_score > json_score


def _extract_analysis_from_code_response(response: str) -> Dict[str, Any]:
    """Extrait une analyse intelligente d'une r√©ponse contenant du code."""
    import re
    
    logger.info("üß† Analyse intelligente de la r√©ponse code")
    
    # Analyser la complexit√© bas√©e sur le code g√©n√©r√©
    complexity_score = _estimate_complexity_from_code(response)
    
    # Extraire les fichiers mentionn√©s
    files_mentioned = re.findall(r'([a-zA-Z_][a-zA-Z0-9_]*\.(py|js|ts|jsx|tsx|html|css))', response)
    refined_files = list(set([f[0] for f in files_mentioned]))
    
    # D√©tecter les d√©pendances externes
    has_imports = bool(re.search(r'(import|require|from)\s+[\'"][^\'"]+[\'"]', response))
    
    # Estimer la dur√©e bas√©e sur la quantit√© de code
    lines_count = len([line for line in response.split('\n') if line.strip() and not line.strip().startswith('#')])
    estimated_duration = max(30, min(180, lines_count * 2))
    
    return {
        "summary": f"Analyse bas√©e sur le code g√©n√©r√© ({lines_count} lignes de code d√©tect√©es)",
        "complexity_score": complexity_score,
        "estimated_duration_minutes": estimated_duration,
        "risk_level": "Low" if complexity_score <= 3 else "Medium" if complexity_score <= 7 else "High",
        "refined_files_to_modify": refined_files,
        "implementation_plan": {
            "steps": [
                "Cr√©er les fichiers de base identifi√©s",
                "Impl√©menter la logique principale",
                "Ajouter la gestion d'erreurs",
                "√âcrire les tests correspondants"
            ]
        },
        "requires_external_deps": has_imports,
        "breaking_changes_risk": False,
        "test_strategy": "unit",
        "implementation_approach": "code-first",
        "code_analysis": {
            "lines_detected": lines_count,
            "files_mentioned": refined_files,
            "has_external_deps": has_imports,
            "generated_from_code": True
        }
    }


def _estimate_complexity_from_code(code: str) -> int:
    """Estime la complexit√© bas√©e sur le code g√©n√©r√©."""
    complexity_indicators = {
        'class ': 2, 'def ': 1, 'if ': 1, 'for ': 1, 'while ': 1,
        'try:': 2, 'except': 2, 'async ': 2, 'await ': 1,
        'import ': 0.5, 'from ': 0.5
    }
    
    score = 0
    for indicator, weight in complexity_indicators.items():
        count = code.count(indicator)
        score += count * weight
    
    return min(10, max(1, int(score / 3)))  # Normaliser entre 1-10


def _generate_analysis_from_text(response: str) -> Dict[str, Any]:
    """G√©n√®re une analyse bas√©e sur le texte de r√©ponse."""
    
    # Analyser le contenu textuel pour extraire des informations
    word_count = len(response.split())
    
    # D√©tecter la complexit√© bas√©e sur des mots-cl√©s
    complexity_keywords = ['complex', 'difficult', 'challenge', 'multiple', 'integration', 'advanced']
    simple_keywords = ['simple', 'basic', 'easy', 'straightforward', 'quick']
    
    complexity_mentions = sum(1 for keyword in complexity_keywords if keyword.lower() in response.lower())
    simple_mentions = sum(1 for keyword in simple_keywords if keyword.lower() in response.lower())
    
    if simple_mentions > complexity_mentions:
        complexity_score = 3
        estimated_duration = 30
        risk_level = "Low"
    elif complexity_mentions > simple_mentions:
        complexity_score = 7
        estimated_duration = 90
        risk_level = "Medium"
    else:
        complexity_score = 5
        estimated_duration = 60
        risk_level = "Medium"
    
    return {
        "summary": f"Analyse textuelle de la r√©ponse IA ({word_count} mots)",
        "complexity_score": complexity_score,
        "estimated_duration_minutes": estimated_duration,
        "risk_level": risk_level,
        "refined_files_to_modify": [],
        "implementation_plan": {
            "steps": [
                "Analyser les exigences d√©taill√©es",
                "Concevoir l'architecture",
                "Impl√©menter √©tape par √©tape",
                "Tester et valider"
            ]
        },
        "requires_external_deps": 'install' in response.lower() or 'dependency' in response.lower(),
        "breaking_changes_risk": 'breaking' in response.lower() or 'incompatible' in response.lower(),
        "test_strategy": "unit",
        "implementation_approach": "text-analysis",
        "text_analysis": {
            "word_count": word_count,
            "complexity_indicators": complexity_mentions,
            "simplicity_indicators": simple_mentions,
            "generated_from_text": True
        }
    }


def _get_default_analysis_with_error(error_msg: str, raw_response: str = "", risk_level: str = "Medium") -> Dict[str, Any]:
    """Retourne une analyse par d√©faut en cas d'erreur de parsing."""
    return {
        "error": error_msg,
        "raw_response": raw_response[:500] if raw_response else "",  # Premiers 500 caract√®res pour debug
        "summary": "Analyse non disponible en raison d'une erreur de parsing",
        "complexity_score": 5,
        "estimated_duration_minutes": 30,
        "risk_level": risk_level,
        "refined_files_to_modify": [],
        "implementation_plan": {"steps": []},
        "requires_external_deps": False,
        "breaking_changes_risk": False,
        "test_strategy": "unit",
        "implementation_approach": "standard"
    } 