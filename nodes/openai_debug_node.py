"""N≈ìud de debug avec OpenAI LLM suite √† demande humaine."""

import openai
from typing import Dict, Any, List
from models.state import GraphState
from utils.logger import get_logger
from config.settings import get_settings
from config.langsmith_config import langsmith_config

logger = get_logger(__name__)
settings = get_settings()


async def openai_debug_after_human_request(state: GraphState) -> GraphState:
    """
    N≈ìud de debug avec OpenAI LLM apr√®s demande humaine.
    
    Ce n≈ìud :
    1. Analyse la demande de debug humaine
    2. Utilise OpenAI pour analyser les probl√®mes
    3. G√©n√®re des suggestions de correction
    4. Met √† jour le code si possible
    5. Re-lance les tests
    
    Args:
        state: √âtat actuel du graphe
        
    Returns:
        √âtat mis √† jour avec les corrections OpenAI
    """
    task = state["task"]
    logger.info(f"üîß Debug OpenAI pour: {task.title}")
    
    # ‚úÖ CORRECTION CRITIQUE: Assurer l'int√©grit√© de l'√©tat d√®s le d√©but
    from utils.error_handling import ensure_state_integrity
    ensure_state_integrity(state)
    
    # Initialiser ai_messages si n√©cessaire
    if "ai_messages" not in state["results"]:
        state["results"]["ai_messages"] = []
    
    state["results"]["ai_messages"].append("üîß Analyse debug avec OpenAI LLM...")
    
    try:
        # ‚úÖ CORRECTION CRITIQUE: G√©rer les compteurs de debug pour √©viter les boucles infinies
        if "human_debug_attempts" not in state["results"]:
            state["results"]["human_debug_attempts"] = 0
        
        # Incr√©menter le compteur de debug apr√®s validation humaine
        state["results"]["human_debug_attempts"] += 1
        max_human_debug = 2  # Maximum 2 tentatives apr√®s validation humaine
        
        logger.info(f"üîß Debug OpenAI apr√®s validation humaine: tentative {state['results']['human_debug_attempts']}/{max_human_debug}")
        
        if state["results"]["human_debug_attempts"] > max_human_debug:
            logger.warning(f"‚ö†Ô∏è Limite de debug apr√®s validation humaine atteinte ({max_human_debug}) - arr√™t forc√©")
            state["results"]["ai_messages"].append("‚ö†Ô∏è Limite de debug atteinte - workflow arr√™t√©")
            state["results"]["debug_limit_reached"] = True
            state["results"]["should_continue"] = False
            return state
        
        # 1. R√©cup√©rer la demande de debug humaine
        debug_request = state["results"].get("debug_request", "Probl√®me √† corriger")
        human_comments = state["results"].get("validation_response", {}).get("comments", "")
        
        logger.info(f"üìù Demande debug humaine: {debug_request}")
        
        # 2. Pr√©parer le contexte pour OpenAI
        debug_context = _prepare_debug_context(state, debug_request, human_comments)
        
        # 3. Initialiser le client OpenAI
        openai_client = openai.AsyncOpenAI(api_key=settings.openai_api_key)
        
        # 4. Cr√©er le prompt de debug
        debug_prompt = _create_debug_prompt(debug_context)
        
        # 5. Appeler OpenAI pour l'analyse
        logger.info("ü§ñ Appel OpenAI pour analyse debug...")
        state["results"]["ai_messages"].append("ü§ñ Consultation OpenAI LLM...")
        
        response = await openai_client.chat.completions.create(
            model="gpt-4",  # Ou "gpt-3.5-turbo" selon pr√©f√©rence
            messages=[
                {
                    "role": "system",
                    "content": "Tu es un expert d√©veloppeur qui aide √† d√©bugger du code. Analyse les probl√®mes et propose des solutions concr√®tes."
                },
                {
                    "role": "user", 
                    "content": debug_prompt
                }
            ],
            temperature=0.1,  # Peu de cr√©ativit√©, focus sur la pr√©cision
            max_tokens=2000
        )
        
        debug_analysis = response.choices[0].message.content
        
        logger.info(f"‚úÖ Analyse OpenAI re√ßue: {len(debug_analysis)} caract√®res")
        state["results"]["ai_messages"].append("‚úÖ Analyse OpenAI termin√©e")
        
        # 6. Parser l'analyse OpenAI
        debug_suggestions = _parse_openai_debug_response(debug_analysis)
        
        # 7. Sauvegarder les r√©sultats de debug
        state["results"]["openai_debug"] = {
            "analysis": debug_analysis,
            "suggestions": debug_suggestions,
            "human_request": debug_request,
            "model_used": "gpt-4",
            "timestamp": state.get("current_time")
        }
        
        # 8. Tracer avec LangSmith
        if langsmith_config.client:
            try:
                langsmith_config.client.create_run(
                    name="openai_debug_analysis",
                    run_type="llm",
                    inputs={
                        "debug_request": debug_request,
                        "human_comments": human_comments,
                        "context": debug_context
                    },
                    outputs={
                        "analysis": debug_analysis,
                        "suggestions_count": len(debug_suggestions),
                        "model": "gpt-4"
                    },
                    session_name=state.get("langsmith_session"),
                    extra={
                        "workflow_id": state.get("workflow_id"),
                        "openai_debug": True,
                        "triggered_by": "human_validation"
                    }
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur LangSmith tracing: {e}")
        
        # 9. Appliquer les suggestions si possible
        if debug_suggestions:
            logger.info(f"üîß Application de {len(debug_suggestions)} suggestions debug")
            state["results"]["ai_messages"].append(f"üîß Application de {len(debug_suggestions)} suggestions...")
            
            # Ici on pourrait impl√©menter l'application automatique des corrections
            # Pour l'instant, on marque qu'un debug a √©t√© fait
            state["results"]["debug_applied"] = True
            state["results"]["debug_suggestions_applied"] = debug_suggestions
        
        # 10. Marquer le debug comme termin√©
        state["results"]["openai_debug_completed"] = True
        state["results"]["should_retest"] = True  # Indiquer qu'il faut re-tester
        
        logger.info(f"üîß Debug OpenAI termin√© avec {len(debug_suggestions)} suggestions")
        state["results"]["ai_messages"].append("‚úÖ Debug OpenAI termin√© - Pr√™t pour re-test")
        
    except Exception as e:
        error_msg = f"Erreur debug OpenAI: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        # ‚úÖ CORRECTION: Initialiser error_logs s'il n'existe pas
        if "error_logs" not in state["results"]:
            state["results"]["error_logs"] = []
        if "ai_messages" not in state["results"]:
            state["results"]["ai_messages"] = []
            
        state["results"]["error_logs"].append(error_msg)
        state["results"]["ai_messages"].append(f"‚ùå {error_msg}")
        state["results"]["openai_debug_failed"] = True
    
    return state


def _prepare_debug_context(state: GraphState, debug_request: str, human_comments: str) -> Dict[str, Any]:
    """Pr√©pare le contexte pour l'analyse debug OpenAI."""
    
    task = state["task"]
    results = state.get("results", {})
    
    context = {
        "task_info": {
            "title": task.title,
            "description": task.description,
            "type": task.task_type.value if hasattr(task.task_type, 'value') else str(task.task_type)
        },
        "human_feedback": {
            "debug_request": debug_request,
            "comments": human_comments
        },
        "test_results": results.get("test_results", {}),
        "error_logs": results.get("error_logs", []),
        "implementation_info": results.get("implementation_results", {}),
        "pr_info": results.get("pr_info", {}),
        "completed_nodes": state.get("completed_nodes", [])
    }
    
    return context


def _create_debug_prompt(context: Dict[str, Any]) -> str:
    """Cr√©e le prompt pour l'analyse debug OpenAI."""
    
    task_info = context["task_info"]
    human_feedback = context["human_feedback"]
    test_results = context["test_results"]
    error_logs = context["error_logs"]
    
    prompt = f"""
DEMANDE DE DEBUG SUITE √Ä FEEDBACK HUMAIN

=== CONTEXTE DE LA T√ÇCHE ===
Titre: {task_info["title"]}
Description: {task_info["description"]}
Type: {task_info["type"]}

=== FEEDBACK HUMAIN ===
Demande de debug: {human_feedback["debug_request"]}
Commentaires: {human_feedback["comments"]}

=== R√âSULTATS DES TESTS ===
{_format_test_results(test_results)}

=== ERREURS RENCONTR√âES ===
{_format_error_logs(error_logs)}

=== DEMANDE ===
Analyse les probl√®mes identifi√©s par l'humain et propose des solutions concr√®tes.

R√©ponds avec:
1. ANALYSE: Identification des probl√®mes principaux
2. CAUSES: Causes probables des probl√®mes  
3. SOLUTIONS: Solutions sp√©cifiques et actionables
4. PRIORIT√â: Ordre de r√©solution recommand√©

Sois pr√©cis et actionnable dans tes recommandations.
"""
    
    return prompt.strip()


def _format_test_results(test_results: Dict[str, Any]) -> str:
    """Formate les r√©sultats de tests pour le prompt."""
    if not test_results:
        return "Aucun r√©sultat de test disponible"
    
    if test_results.get("success"):
        return "‚úÖ Tests r√©ussis"
    else:
        failed_tests = test_results.get("failed_tests", [])
        if failed_tests:
            return f"‚ùå {len(failed_tests)} test(s) √©chou√©(s):\n" + "\n".join([f"- {test}" for test in failed_tests[:5]])
        else:
            return "‚ùå Tests √©chou√©s (d√©tails non disponibles)"


def _format_error_logs(error_logs: List[str]) -> str:
    """Formate les logs d'erreur pour le prompt."""
    if not error_logs:
        return "Aucune erreur loggu√©e"
    
    # Prendre les 5 derni√®res erreurs
    recent_errors = error_logs[-5:]
    return "\n".join([f"- {error}" for error in recent_errors])


def _parse_openai_debug_response(response: str) -> List[Dict[str, str]]:
    """Parse la r√©ponse OpenAI et extrait les suggestions actionables."""
    
    suggestions = []
    
    # Rechercher les sections de solutions
    import re
    
    # Pattern pour identifier les solutions
    solution_patterns = [
        r'(?:SOLUTIONS?|RECOMMANDATIONS?)[:\s]*(.*?)(?=\n\n|\n[A-Z]{2,}|$)',
        r'(?:\d+\.\s*)(.*?)(?=\n\d+\.|$)',
    ]
    
    for pattern in solution_patterns:
        matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)
        for match in matches:
            if match.strip() and len(match.strip()) > 10:
                suggestions.append({
                    "type": "suggestion",
                    "description": match.strip(),
                    "priority": "medium"
                })
    
    # Si pas de suggestions trouv√©es, cr√©er une suggestion g√©n√©rale
    if not suggestions:
        suggestions.append({
            "type": "general",
            "description": "Analyser la r√©ponse OpenAI compl√®te pour identifier les actions",
            "priority": "high",
            "full_response": response
        })
    
    return suggestions[:10]  # Limiter √† 10 suggestions maximum 