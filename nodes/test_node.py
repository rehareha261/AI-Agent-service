"""N≈ìud de tests - ex√©cute et valide les tests."""

import os
from typing import Dict, Any
from models.state import GraphState
from tools.claude_code_tool import ClaudeCodeTool
from utils.logger import get_logger
from utils.persistence_decorator import with_persistence
from utils.helpers import get_working_directory, validate_working_directory, ensure_working_directory

logger = get_logger(__name__)


def log_test_results_decorator(func):
    """D√©corateur pour logger les r√©sultats de tests."""
    async def wrapper(*args, **kwargs):
        # Logging avant le test
        logger.info("üß™ D√©marrage des tests automatis√©s")
        
        result = await func(*args, **kwargs)
        
        # Logging apr√®s le test
        if result and result.get("results", {}).get("test_results"):
            test_results = result["results"]["test_results"]
            if isinstance(test_results, list) and test_results:
                last_test = test_results[-1]
                success = last_test.get("success", False) if isinstance(last_test, dict) else False
                logger.info(f"‚úÖ Tests termin√©s - Succ√®s: {success}")
            
        return result
    return wrapper


@with_persistence("run_tests")
@log_test_results_decorator
async def run_tests(state: GraphState) -> GraphState:
    """
    N≈ìud de tests : ex√©cute les tests pour valider l'impl√©mentation.
    
    Ce n≈ìud :
    1. D√©tecte les fichiers de test dans le projet
    2. Ex√©cute les tests avec pytest/unittest
    3. Analyse les r√©sultats et erreurs
    4. D√©termine si le code est pr√™t pour la QA
    
    Args:
        state: √âtat actuel du graphe
        
    Returns:
        √âtat mis √† jour avec les r√©sultats des tests
    """
    logger.info(f"üß™ Lancement des tests pour: {state['task'].title}")

    # ‚úÖ CORRECTION CRITIQUE: Assurer l'int√©grit√© de l'√©tat d√®s le d√©but
    from utils.error_handling import ensure_state_integrity
    ensure_state_integrity(state)

    # Initialiser ai_messages si n√©cessaire
    if "ai_messages" not in state["results"]:
        state["results"]["ai_messages"] = []

    state["results"]["current_status"] = "testing".lower()
    state["results"]["ai_messages"].append("D√©but des tests...")
    
    try:
        # V√©rifier que l'environnement est pr√™t
        working_directory = get_working_directory(state)
        
        if not validate_working_directory(working_directory, "test_node"):
            # ‚úÖ AM√âLIORATION: Essayer de r√©cup√©rer le working_directory depuis prepare_result
            prepare_result = state["results"].get("prepare_result", {})
            if prepare_result and prepare_result.get("working_directory"):
                fallback_wd = prepare_result["working_directory"]
                logger.info(f"üîÑ Tentative de r√©cup√©ration du working_directory depuis prepare_result: {fallback_wd}")
                
                if validate_working_directory(fallback_wd, "test_node_fallback"):
                    working_directory = fallback_wd
                    # Mettre √† jour l'√©tat pour la coh√©rence
                    state["working_directory"] = working_directory
                    state["results"]["working_directory"] = working_directory
                    logger.info(f"‚úÖ Working directory r√©cup√©r√© avec succ√®s: {working_directory}")
                else:
                    logger.warning(f"‚ö†Ô∏è Working directory du prepare_result invalide: {fallback_wd}")
            
            # ‚úÖ NOUVELLE STRAT√âGIE: Essayer d'extraire le r√©pertoire depuis Git (si repository clon√©)
            if not validate_working_directory(working_directory, "test_node"):
                git_result = state["results"].get("git_result")
                if git_result and hasattr(git_result, '__dict__'):
                    # Si git_result est un objet avec des attributs
                    potential_dirs = [
                        getattr(git_result, 'working_directory', None),
                        getattr(git_result, 'repository_path', None),
                        getattr(git_result, 'directory', None)
                    ]
                elif isinstance(git_result, dict):
                    # Si git_result est un dictionnaire
                    potential_dirs = [
                        git_result.get('working_directory'),
                        git_result.get('repository_path'),
                        git_result.get('directory')
                    ]
                else:
                    potential_dirs = []
                
                for potential_dir in potential_dirs:
                    if potential_dir and validate_working_directory(potential_dir, "test_node_git_fallback"):
                        working_directory = potential_dir
                        state["working_directory"] = working_directory
                        state["results"]["working_directory"] = working_directory
                        logger.info(f"‚úÖ Working directory r√©cup√©r√© depuis git_result: {working_directory}")
                        break
            
            # Si toujours pas de working_directory valide, cr√©er un r√©pertoire de secours
            if not validate_working_directory(working_directory, "test_node"):
                try:
                    working_directory = ensure_working_directory(state, "test_node_")
                    logger.info(f"üìÅ R√©pertoire de test de secours cr√©√©: {working_directory}")
                except Exception as e:
                    error_msg = f"Impossible de cr√©er un r√©pertoire de travail pour les tests: {e}"
                    logger.error(error_msg)
                    state["results"]["error_logs"].append(error_msg)
                    state["results"]["ai_messages"].append(f"‚ùå {error_msg}")
                    state["results"]["should_continue"] = False
                    state["results"]["current_status"] = "failed".lower()
                    return state

        logger.info(f"üìÅ Utilisation du r√©pertoire de travail dans test_node: {working_directory}")
        
        # Initialiser le nouveau moteur de tests
        from tools.testing_engine import TestingEngine
        
        testing_engine = TestingEngine()
        # ‚úÖ S'assurer que working_directory est correctement d√©fini dans l'instance du moteur
        testing_engine.working_directory = working_directory

        # ‚úÖ NOUVELLE APPROCHE: Analyser d'abord les changements pour cibler les tests
        code_changes = state["results"].get("implementation_result", {}).get("modified_files", {})
        
        if not code_changes:
            # Si pas de changements IA d√©tect√©s, r√©cup√©rer depuis implement_result
            implement_result = state["results"].get("implement_result")
            if implement_result and isinstance(implement_result, dict):
                code_changes = implement_result.get("modified_files", {})
        
        logger.info(f"üîç Fichiers modifi√©s d√©tect√©s pour tests: {len(code_changes) if code_changes else 0}")
        
        # ‚úÖ AM√âLIORATION: Tests adaptatifs selon le contenu du repository
        logger.info("üß™ Lancement de la suite compl√®te de tests en couches...")
        
        # 1. D'abord v√©rifier s'il y a des tests existants dans le repository
        import os
        test_files_found = []
        for root, dirs, files in os.walk(working_directory):
            for file in files:
                if (file.startswith('test_') and file.endswith('.py')) or \
                   (file.endswith('_test.py')) or \
                   (file.endswith('.test.js')) or \
                   (file.endswith('.spec.js')):
                    test_files_found.append(os.path.join(root, file))
        
        if test_files_found:
            logger.info(f"üîç Tests existants trouv√©s dans le repository: {len(test_files_found)} fichiers")
            # Ex√©cuter les tests existants du repository
            result = await testing_engine._run_all_tests(
                working_directory=working_directory,
                include_coverage=True,
                code_changes=code_changes
            )
        else:
            logger.info("üìù Aucun test existant trouv√© - cr√©ation de tests automatiques pour le code g√©n√©r√©")
            
            # Si pas de tests existants, cr√©er des tests automatiques pour les fichiers modifi√©s
            if code_changes:
                result = await _create_and_run_automatic_tests(testing_engine, working_directory, code_changes)
            else:
                # Pas de code modifi√© et pas de tests - consid√©rer comme succ√®s avec avertissement
                result = {
                    "success": True,
                    "warning": "Aucun test trouv√© et aucun code modifi√© - validation par d√©faut",
                    "total_tests": 0,
                    "passed_tests": 0,
                    "failed_tests": 0,
                    "test_type": "validation_par_defaut",
                    "message": "Pas de tests requis pour cette t√¢che"
                }
                logger.info("‚ÑπÔ∏è Aucun test requis - validation automatique")

        # ‚úÖ CORRECTION: S'assurer que result est un dictionnaire
        if not isinstance(result, dict):
            logger.warning(f"‚ö†Ô∏è R√©sultat de test invalide (type {type(result)}): {result}")
            result = {
                "success": False,
                "error": f"Type de r√©sultat invalide: {type(result)}",
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }

        # Traiter les r√©sultats des tests
        test_success = result.get("success", False)
        
        # Stocker les r√©sultats dans une liste (pour compatibilit√© avec _should_debug)
        if "test_results" not in state["results"]:
            state["results"]["test_results"] = []
        state["results"]["test_results"].append(result)
        state["results"]["test_success"] = test_success
        
        # Messages d√©taill√©s selon les r√©sultats
        if test_success:
            total_tests = result.get("total_tests", 0)
            passed_tests = result.get("passed_tests", 0)
            
            if total_tests > 0:
                success_msg = f"‚úÖ Tests r√©ussis: {passed_tests}/{total_tests}"
                logger.info(success_msg)
                state["results"]["ai_messages"].append(success_msg)
            else:
                warning_msg = result.get("warning", "‚úÖ Validation automatique - aucun test requis")
                logger.info(warning_msg)
                state["results"]["ai_messages"].append(warning_msg)
            
            state["results"]["current_status"] = "tests_passed".lower()
        else:
            failed_tests = result.get("failed_tests", 1)
            total_tests = result.get("total_tests", 1)
            error_msg = result.get("error", "Tests √©chou√©s - v√©rifier la logique m√©tier")
            
            failure_msg = f"‚ùå Tests √©chou√©s: {failed_tests}/{total_tests} - {error_msg}"
            logger.warning(failure_msg)
            state["results"]["ai_messages"].append(failure_msg)
            state["results"]["current_status"] = "tests_failed".lower()
            
            # Ajouter les d√©tails d'erreur si disponibles
            if "error_details" in result:
                state["results"]["test_error_details"] = result["error_details"]

        # ‚úÖ D√âCISION: Continuer le workflow m√™me si les tests √©chouent (pour la validation humaine)
        state["results"]["should_continue"] = True
        
        logger.info("üèÅ Tests termin√©s")
        return state

    except Exception as e:
        error_msg = f"Exception lors des tests: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        # Cr√©er un r√©sultat d'erreur
        test_result = {
            "success": False,
            "error": error_msg,
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 1,
            "exception": str(e)
        }
        
        # Stocker dans une liste pour compatibilit√©
        if "test_results" not in state["results"]:
            state["results"]["test_results"] = []
        state["results"]["test_results"].append(test_result)
        state["results"]["test_success"] = False
        state["results"]["ai_messages"].append(f"‚ùå Exception tests: {error_msg}")
        state["results"]["current_status"] = "tests_error".lower()
        state["results"]["should_continue"] = True  # Continuer malgr√© l'erreur
        
        return state


async def _create_and_run_automatic_tests(testing_engine, working_directory: str, code_changes: dict) -> dict:
    """Cr√©e et ex√©cute des tests automatiques pour les fichiers modifi√©s."""
    import os
    
    logger.info(f"üîß Cr√©ation de tests automatiques pour {len(code_changes)} fichiers modifi√©s")
    
    # Cr√©er un r√©pertoire de tests temporaire
    test_dir = os.path.join(working_directory, "auto_tests")
    os.makedirs(test_dir, exist_ok=True)
    
    test_files_created = []
    
    for file_path, file_content in code_changes.items():
        if file_path.endswith('.py'):
            # Cr√©er un test basique pour le fichier Python
            test_content = _generate_basic_python_test(file_path, file_content)
            test_file_name = f"test_{os.path.basename(file_path)}"
            test_file_path = os.path.join(test_dir, test_file_name)
            
            with open(test_file_path, 'w') as f:
                f.write(test_content)
            
            test_files_created.append(test_file_path)
            logger.info(f"üìù Test automatique cr√©√©: {test_file_name}")
    
    if test_files_created:
        # Ex√©cuter les tests cr√©√©s
        try:
            result = await testing_engine._run_test_directory(test_dir)
            result["auto_generated"] = True
            result["test_files_created"] = len(test_files_created)
            return result
        except Exception as e:
            return {
                "success": False,
                "error": f"Erreur lors de l'ex√©cution des tests automatiques: {e}",
                "auto_generated": True,
                "test_files_created": len(test_files_created)
            }
    else:
        return {
            "success": True,
            "warning": "Aucun fichier Python modifi√© - tests automatiques non applicables",
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "auto_generated": True
        }


def _generate_basic_python_test(file_path: str, file_content: str) -> str:
    """G√©n√®re un test Python basique pour un fichier donn√©."""
    import os
    module_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Test basique qui v√©rifie que le module peut √™tre import√© et ex√©cute des v√©rifications de base
    test_content = f'''"""Test automatique g√©n√©r√© pour {file_path}"""
import unittest
import sys
import os

# Ajouter le r√©pertoire parent au path pour l'import
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

class Test{module_name.replace("_", "").title()}(unittest.TestCase):
    """Tests automatiques pour {module_name}"""
    
    def test_module_import(self):
        """Test que le module peut √™tre import√©"""
        try:
            import {module_name}
            self.assertTrue(True, "Module import√© avec succ√®s")
        except ImportError as e:
            self.fail(f"Impossible d'importer {module_name}: {{e}}")
    
    def test_basic_syntax(self):
        """Test basique de syntaxe"""
        # V√©rifier que le contenu contient au moins du code Python valide
        content = """{repr(file_content[:500])}"""
        self.assertIsInstance(content, str)
        self.assertGreater(len(content), 0)
    
    def setUp(self):
        """Configuration avant chaque test"""
        pass
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        pass

if __name__ == '__main__':
    unittest.main()
'''
    
    return test_content


async def _run_basic_tests(working_directory: str) -> Dict[str, Any]:
    """
    Fallback simple pour ex√©cuter des tests basiques quand TestingEngine √©choue.
    """
    logger.info("üß™ Ex√©cution de tests basiques...")
    
    # Cr√©er une instance de ClaudeCodeTool pour ex√©cuter les commandes
    claude_tool = ClaudeCodeTool()
    claude_tool.working_directory = working_directory
    
    # Commandes de test √† essayer dans l'ordre
    test_commands = [
        "python -m pytest -v --tb=short",
        "python -m pytest",
        "python -m unittest discover -v",
        "python -m unittest",
        "npm test",
        "yarn test"
    ]
    
    results = {
        "success": False,
        "test_results": {
            "passed": 0,
            "failed": 0,
            "errors": [],
            "coverage": 0
        },
        "command_used": None,
        "output": ""
    }
    
    for command in test_commands:
        logger.info(f"üß™ Test avec commande: {command}")
        
        try:
            result = await claude_tool._arun(
                action="execute_command",
                command=command,
                cwd=working_directory
            )
            
            if result.get("success", False):
                # Analyser la sortie pour extraire les r√©sultats
                output = result.get("output", "")
                results["output"] = output
                results["command_used"] = command
                
                # Parser simple des r√©sultats pytest/unittest
                if "pytest" in command.lower():
                    results.update(_parse_pytest_simple(output))
                elif "unittest" in command.lower():
                    results.update(_parse_unittest_simple(output))
                elif "test" in command.lower():  # npm/yarn test
                    results.update(_parse_npm_test_simple(output))
                
                # Si au moins une commande a fonctionn√©, consid√©rer comme succ√®s
                results["success"] = True
                logger.info(f"‚úÖ Tests ex√©cut√©s avec: {command}")
                break
                
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Commande {command} √©chou√©e: {e}")
            continue
    
    if not results["success"]:
        results["test_results"]["errors"].append("Aucune commande de test fonctionnelle trouv√©e")
        results["test_results"]["failed"] = 1
        logger.warning("‚ùå Aucune commande de test n'a fonctionn√©")
    
    return results


def _parse_pytest_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie pytest."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Chercher les patterns pytest
    import re
    
    # Pattern pour "X passed, Y failed"
    match = re.search(r'(\d+) passed.*?(\d+) failed', output)
    if match:
        results["test_results"]["passed"] = int(match.group(1))
        results["test_results"]["failed"] = int(match.group(2))
        results["success"] = int(match.group(2)) == 0
    
    # Pattern pour "X passed" seulement
    elif re.search(r'(\d+) passed', output):
        match = re.search(r'(\d+) passed', output)
        results["test_results"]["passed"] = int(match.group(1))
        results["success"] = True
    
    # Pas de tests trouv√©s
    elif "no tests ran" in output.lower() or "collected 0 items" in output.lower():
        results["test_results"]["errors"].append("Aucun test trouv√© par pytest")
    
    return results


def _parse_unittest_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie unittest."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Chercher "Ran X tests"
    import re
    match = re.search(r'Ran (\d+) tests?', output)
    if match:
        total_tests = int(match.group(1))
        
        # Chercher les √©checs
        if "FAILED" in output:
            # Compter les lignes avec FAIL
            failed_count = output.count("FAIL")
            results["test_results"]["failed"] = failed_count
            results["test_results"]["passed"] = total_tests - failed_count
        else:
            results["test_results"]["passed"] = total_tests
            results["success"] = True
    else:
        results["test_results"]["errors"].append("Aucun test trouv√© par unittest")
    
    return results


def _parse_npm_test_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie npm/yarn test."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Pattern simple pour d√©tecter succ√®s/√©chec npm test
    if "Test Suites: " in output:
        # Jest format
        import re
        match = re.search(r'(\d+) passed.*?(\d+) failed', output)
        if match:
            results["test_results"]["passed"] = int(match.group(1))
            results["test_results"]["failed"] = int(match.group(2))
            results["success"] = int(match.group(2)) == 0
    elif "passing" in output.lower():
        # Mocha format
        results["test_results"]["passed"] = 1
        results["success"] = True
    else:
        results["test_results"]["errors"].append("Format de test npm/yarn non reconnu")
    
    return results

async def _detect_test_commands(claude_tool: ClaudeCodeTool) -> list[str]:
    """D√©tecte les commandes de test disponibles selon le type de projet."""
    
    test_commands = []
    
    try:
        # V√©rifier les fichiers de configuration pour d√©tecter le type de projet
        
        # 1. Projet Node.js / JavaScript
        package_json_result = await claude_tool._arun(action="read_file", file_path="package.json", required=False)
        if package_json_result["success"] and package_json_result.get("file_exists", True):
            import json
            try:
                package_data = json.loads(package_json_result["content"])
                scripts = package_data.get("scripts", {})
                
                # Prioriser les scripts de test d√©finis
                if "test" in scripts:
                    test_commands.append("npm test")
                if "test:unit" in scripts:
                    test_commands.append("npm run test:unit")
                if "jest" in scripts:
                    test_commands.append("npm run jest")
                    
                # Ajouter des alternatives
                if not test_commands:
                    test_commands.extend(["npm test", "yarn test", "npx jest"])
                    
            except json.JSONDecodeError:
                test_commands.append("npm test")
        elif not package_json_result["success"]:
            logger.debug("üìù package.json non trouv√© - probablement un projet Python")
        
        # 2. Projet Python
        requirements_result = await claude_tool._arun(action="read_file", file_path="requirements.txt", required=False)
        setup_py_result = await claude_tool._arun(action="read_file", file_path="setup.py", required=False)
        pyproject_result = await claude_tool._arun(action="read_file", file_path="pyproject.toml", required=False)
        
        # ‚úÖ CORRECTION: V√©rifier si au moins un fichier existe et a √©t√© lu avec succ√®s
        python_config_files = [
            ("requirements.txt", requirements_result),
            ("setup.py", setup_py_result), 
            ("pyproject.toml", pyproject_result)
        ]
        
        # Log les fichiers Python non trouv√©s en debug seulement
        for file_name, result in python_config_files:
            if not result["success"]:
                logger.debug(f"üìù {file_name} non trouv√© - normal pour certains projets Python")
        
        python_files_exist = any(
            result["success"] and result.get("file_exists", True) 
            for _, result in python_config_files
        )
        
        if python_files_exist:
            # D√©tecter le framework de test utilis√©
            all_content = ""
            for config_file, result in python_config_files:
                if result["success"] and result.get("file_exists", True):
                    all_content += result["content"].lower()
            
            if "pytest" in all_content:
                test_commands.extend(["python -m pytest", "pytest"])
            if "unittest" in all_content:
                test_commands.append("python -m unittest discover")
            if "nose" in all_content:
                test_commands.append("nosetests")
            
            # Commandes par d√©faut Python
            if not any("python" in cmd for cmd in test_commands):
                test_commands.extend([
                    "python -m pytest",
                    "python -m unittest discover", 
                    "python -m pytest tests/",
                    "python -m unittest"
                ])
        
        # 3. Autres types de projets
        
        # Makefile
        makefile_result = await claude_tool._arun(action="read_file", file_path="Makefile")
        if makefile_result["success"] and makefile_result.get("file_exists", True) and "test" in makefile_result["content"].lower():
            test_commands.append("make test")
        
        # Cargo (Rust)
        cargo_result = await claude_tool._arun(action="read_file", file_path="Cargo.toml")
        if cargo_result["success"] and cargo_result.get("file_exists", True):
            test_commands.append("cargo test")
        
        # Go
        go_mod_result = await claude_tool._arun(action="read_file", file_path="go.mod")
        if go_mod_result["success"] and go_mod_result.get("file_exists", True):
            test_commands.append("go test ./...")
        
        # Composer (PHP)
        composer_result = await claude_tool._arun(action="read_file", file_path="composer.json")
        if composer_result["success"] and composer_result.get("file_exists", True):
            test_commands.extend(["composer test", "./vendor/bin/phpunit"])
        
    except Exception as e:
        logger.warning(f"Erreur lors de la d√©tection des commandes de test: {e}")
    
    # Supprimer les doublons tout en pr√©servant l'ordre
    seen = set()
    unique_commands = []
    for cmd in test_commands:
        if cmd not in seen:
            seen.add(cmd)
            unique_commands.append(cmd)
    
    return unique_commands

async def _analyze_test_failure(test_result: Dict[str, Any], state: Dict[str, Any]) -> str:
    """Analyse les √©checs de tests pour fournir un diagnostic."""
    
    # G√©rer √† la fois les objets TestResult et les dictionnaires
    if hasattr(test_result, 'stderr'):
        # C'est un objet TestResult
        error_output = getattr(test_result, 'stderr', '') or getattr(test_result, 'stdout', '')
        exit_code = getattr(test_result, 'exit_code', 1)
    else:
        # C'est un dictionnaire
        error_output = test_result.get("error", "") or test_result.get("output", "")
        test_results = test_result.get("test_results", {})
        errors = test_results.get("errors", [])
        if errors:
            error_output += "\n" + "\n".join(errors)
        exit_code = test_result.get("exit_code", 1)
    
    # Patterns d'erreurs communes
    error_patterns = {
        "module not found": "Module manquant - v√©rifier les imports et d√©pendances",
        "no module named": "Module Python manquant - v√©rifier les imports",
        "cannot find module": "Module Node.js manquant - v√©rifier package.json",
        "import error": "Erreur d'import - v√©rifier les chemins et d√©pendances",
        "syntax error": "Erreur de syntaxe dans le code",
        "indentation error": "Erreur d'indentation Python",
        "unexpected token": "Erreur de syntaxe JavaScript/TypeScript",
        "command not found": "Commande de test non trouv√©e - installer les d√©pendances",
        "permission denied": "Probl√®me de permissions - v√©rifier l'ex√©cutable",
        "connection refused": "Probl√®me de connexion - services externes requis?",
        "timeout": "Test trop lent - optimiser ou augmenter timeout",
        "assertion": "Assertion √©chou√©e - logique m√©tier incorrecte",
        "failed": "Test(s) √©chou√©(s)",
        "error": "Erreur g√©n√©rale"
    }
    
    error_analysis = "√âchec de test non sp√©cifique"
    
    # Chercher des patterns d'erreur dans la sortie
    error_output_lower = error_output.lower()
    for pattern, description in error_patterns.items():
        if pattern in error_output_lower:
            error_analysis = description
            break
    
    # Ajouter des d√©tails sp√©cifiques si disponibles
    if exit_code == 127:
        error_analysis = "Commande de test non trouv√©e - installer les d√©pendances"
    elif exit_code == 2:
        error_analysis = "Erreur de configuration ou arguments invalides"
    elif exit_code == 1:
        error_analysis = "Tests √©chou√©s - v√©rifier la logique m√©tier"
    
    # Extraire les premi√®res lignes d'erreur pour plus de contexte
    error_lines = error_output.split('\n')[:5]
    relevant_errors = [line.strip() for line in error_lines if line.strip() and not line.startswith('===')]
    
    if relevant_errors:
        error_context = ' | '.join(relevant_errors[:2])  # Maximum 2 lignes
        error_analysis += f" ({error_context})"
    
    return error_analysis

def should_continue_to_debug(state: Dict[str, Any]) -> bool:
    """D√©termine si le workflow doit continuer vers le debug."""
    
    # V√©rifier s'il y a eu des √©checs de tests r√©cents
    if not state["results"]["test_results"]:
        return False
    
    latest_test = state["results"]["test_results"][-1]
    
    # Si le dernier test a r√©ussi, pas besoin de debug
    if latest_test.success:
        return False
    
    # V√©rifier si on n'a pas d√©pass√© la limite de tentatives
    debug_attempts = state["results"].get("debug_attempts", 0)
    max_debug_attempts = state["results"].get("max_debug_attempts", 3)
    if debug_attempts >= max_debug_attempts:
        return False
    
    return True

def should_continue_to_finalize(state: Dict[str, Any]) -> bool:
    """D√©termine si le workflow peut passer √† la finalisation."""
    
    # V√©rifier s'il y a eu des tests
    if not state["results"]["test_results"]:
        return False
    
    # Le dernier test doit avoir r√©ussi
    latest_test = state["results"]["test_results"][-1]
    return latest_test.success 