"""NÅ“ud de tests - exÃ©cute et valide les tests."""

import os
from typing import Dict, Any
from models.state import GraphState
from tools.claude_code_tool import ClaudeCodeTool
from utils.logger import get_logger
from utils.persistence_decorator import with_persistence
from utils.helpers import get_working_directory, validate_working_directory, ensure_working_directory

logger = get_logger(__name__)


def log_test_results_decorator(func):
    """DÃ©corateur pour logger les rÃ©sultats de tests."""
    async def wrapper(*args, **kwargs):
        # Logging avant le test
        logger.info("ğŸ§ª DÃ©marrage des tests automatisÃ©s")
        
        result = await func(*args, **kwargs)
        
        # Logging aprÃ¨s le test
        if result and result.get("results", {}).get("test_results"):
            test_results = result["results"]["test_results"]
            if isinstance(test_results, list) and test_results:
                last_test = test_results[-1]
                success = last_test.get("success", False) if isinstance(last_test, dict) else False
                logger.info(f"âœ… Tests terminÃ©s - SuccÃ¨s: {success}")
            
        return result
    return wrapper


@with_persistence("run_tests")
@log_test_results_decorator
async def run_tests(state: GraphState) -> GraphState:
    """
    NÅ“ud de tests : exÃ©cute les tests pour valider l'implÃ©mentation.
    
    Ce nÅ“ud :
    1. DÃ©tecte les fichiers de test dans le projet
    2. ExÃ©cute les tests avec pytest/unittest
    3. Analyse les rÃ©sultats et erreurs
    4. DÃ©termine si le code est prÃªt pour la QA
    
    Args:
        state: Ã‰tat actuel du graphe
        
    Returns:
        Ã‰tat mis Ã  jour avec les rÃ©sultats des tests
    """
    logger.info(f"ğŸ§ª Lancement des tests pour: {state['task'].title}")

    # âœ… CORRECTION CRITIQUE: Assurer l'intÃ©gritÃ© de l'Ã©tat dÃ¨s le dÃ©but
    from utils.error_handling import ensure_state_integrity
    ensure_state_integrity(state)

    # Initialiser ai_messages si nÃ©cessaire
    if "ai_messages" not in state["results"]:
        state["results"]["ai_messages"] = []

    state["results"]["current_status"] = "testing".lower()
    state["results"]["ai_messages"].append("DÃ©but des tests...")
    
    try:
        # VÃ©rifier que l'environnement est prÃªt
        working_directory = get_working_directory(state)
        
        if not validate_working_directory(working_directory, "test_node"):
            # âœ… AMÃ‰LIORATION: Essayer de rÃ©cupÃ©rer le working_directory depuis prepare_result
            prepare_result = state["results"].get("prepare_result", {})
            if prepare_result and prepare_result.get("working_directory"):
                fallback_wd = prepare_result["working_directory"]
                logger.info(f"ğŸ”„ Tentative de rÃ©cupÃ©ration du working_directory depuis prepare_result: {fallback_wd}")
                
                if validate_working_directory(fallback_wd, "test_node_fallback"):
                    working_directory = fallback_wd
                    # Mettre Ã  jour l'Ã©tat pour la cohÃ©rence
                    state["working_directory"] = working_directory
                    state["results"]["working_directory"] = working_directory
                    logger.info(f"âœ… Working directory rÃ©cupÃ©rÃ© avec succÃ¨s: {working_directory}")
                else:
                    logger.warning(f"âš ï¸ Working directory du prepare_result invalide: {fallback_wd}")
            
            # âœ… NOUVELLE STRATÃ‰GIE: Essayer d'extraire le rÃ©pertoire depuis Git (si repository clonÃ©)
            if not validate_working_directory(working_directory, "test_node"):
                git_result = state["results"].get("git_result")
                if git_result and hasattr(git_result, '__dict__'):
                    # Si git_result est un objet avec des attributs
                    potential_dirs = [
                        getattr(git_result, 'working_directory', None),
                        getattr(git_result, 'repository_path', None),
                        getattr(git_result, 'directory', None)
                    ]
                elif isinstance(git_result, dict):
                    # Si git_result est un dictionnaire
                    potential_dirs = [
                        git_result.get('working_directory'),
                        git_result.get('repository_path'),
                        git_result.get('directory')
                    ]
                else:
                    potential_dirs = []
                
                for potential_dir in potential_dirs:
                    if potential_dir and validate_working_directory(potential_dir, "test_node_git_fallback"):
                        working_directory = potential_dir
                        state["working_directory"] = working_directory
                        state["results"]["working_directory"] = working_directory
                        logger.info(f"âœ… Working directory rÃ©cupÃ©rÃ© depuis git_result: {working_directory}")
                        break
            
            # Si toujours pas de working_directory valide, crÃ©er un rÃ©pertoire de secours
            if not validate_working_directory(working_directory, "test_node"):
                try:
                    working_directory = ensure_working_directory(state, "test_node_")
                    logger.info(f"ğŸ“ RÃ©pertoire de test de secours crÃ©Ã©: {working_directory}")
                except Exception as e:
                    error_msg = f"Impossible de crÃ©er un rÃ©pertoire de travail pour les tests: {e}"
                    logger.error(error_msg)
                    state["results"]["error_logs"].append(error_msg)
                    state["results"]["ai_messages"].append(f"âŒ {error_msg}")
                    state["results"]["should_continue"] = False
                    state["results"]["current_status"] = "failed".lower()
                    return state

        logger.info(f"ğŸ“ Utilisation du rÃ©pertoire de travail dans test_node: {working_directory}")
        
        # Initialiser le nouveau moteur de tests
        from tools.testing_engine import TestingEngine
        
        testing_engine = TestingEngine()
        # âœ… S'assurer que working_directory est correctement dÃ©fini dans l'instance du moteur
        testing_engine.working_directory = working_directory

        # âœ… NOUVELLE APPROCHE: Analyser d'abord les changements pour cibler les tests
        code_changes = state["results"].get("implementation_result", {}).get("modified_files", {})
        
        if not code_changes:
            # Si pas de changements IA dÃ©tectÃ©s, rÃ©cupÃ©rer depuis implement_result
            implement_result = state["results"].get("implement_result")
            if implement_result and isinstance(implement_result, dict):
                code_changes = implement_result.get("modified_files", {})
        
        logger.info(f"ğŸ” Fichiers modifiÃ©s dÃ©tectÃ©s pour tests: {len(code_changes) if code_changes else 0}")
        
        # âœ… AMÃ‰LIORATION: Tests adaptatifs selon le contenu du repository
        logger.info("ğŸ§ª Lancement de la suite complÃ¨te de tests en couches...")
        
        # 1. D'abord vÃ©rifier s'il y a des tests existants dans le repository
        import os
        test_files_found = []
        for root, dirs, files in os.walk(working_directory):
            for file in files:
                if (file.startswith('test_') and file.endswith('.py')) or \
                   (file.endswith('_test.py')) or \
                   (file.endswith('.test.js')) or \
                   (file.endswith('.spec.js')):
                    test_files_found.append(os.path.join(root, file))
        
        if test_files_found:
            logger.info(f"ğŸ” Tests existants trouvÃ©s dans le repository: {len(test_files_found)} fichiers")
            # ExÃ©cuter les tests existants du repository
            result = await testing_engine._run_all_tests(
                working_directory=working_directory,
                include_coverage=True,
                code_changes=code_changes
            )
        else:
            logger.info("ğŸ“ Aucun test existant trouvÃ© - crÃ©ation de tests automatiques pour le code gÃ©nÃ©rÃ©")
            
            # Si pas de tests existants, crÃ©er des tests automatiques pour les fichiers modifiÃ©s
            if code_changes:
                result = await _create_and_run_automatic_tests(testing_engine, working_directory, code_changes)
            else:
                # Pas de code modifiÃ© et pas de tests - considÃ©rer comme succÃ¨s avec avertissement
                result = {
                    "success": True,
                    "warning": "Aucun test trouvÃ© et aucun code modifiÃ© - validation par dÃ©faut",
                    "total_tests": 0,
                    "passed_tests": 0,
                    "failed_tests": 0,
                    "test_type": "validation_par_defaut",
                    "message": "Pas de tests requis pour cette tÃ¢che"
                }
                logger.info("â„¹ï¸ Aucun test requis - validation automatique")

        # âœ… CORRECTION: S'assurer que result est un dictionnaire
        if not isinstance(result, dict):
            logger.warning(f"âš ï¸ RÃ©sultat de test invalide (type {type(result)}): {result}")
            result = {
                "success": False,
                "error": f"Type de rÃ©sultat invalide: {type(result)}",
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }

        # Traiter les rÃ©sultats des tests
        test_success = result.get("success", False)
        
        # Stocker les rÃ©sultats dans une liste (pour compatibilitÃ© avec _should_debug)
        if "test_results" not in state["results"]:
            state["results"]["test_results"] = []
        state["results"]["test_results"].append(result)
        state["results"]["test_success"] = test_success
        
        # Messages dÃ©taillÃ©s selon les rÃ©sultats
        if test_success:
            total_tests = result.get("total_tests", 0)
            passed_tests = result.get("passed_tests", 0)
            
            if total_tests > 0:
                success_msg = f"âœ… Tests rÃ©ussis: {passed_tests}/{total_tests}"
                logger.info(success_msg)
                state["results"]["ai_messages"].append(success_msg)
            else:
                warning_msg = result.get("warning", "âœ… Validation automatique - aucun test requis")
                logger.info(warning_msg)
                state["results"]["ai_messages"].append(warning_msg)
            
            state["results"]["current_status"] = "tests_passed".lower()
        else:
            failed_tests = result.get("failed_tests", 1)
            total_tests = result.get("total_tests", 1)
            error_msg = result.get("error", "Tests Ã©chouÃ©s - vÃ©rifier la logique mÃ©tier")
            
            failure_msg = f"âŒ Tests Ã©chouÃ©s: {failed_tests}/{total_tests} - {error_msg}"
            logger.warning(failure_msg)
            state["results"]["ai_messages"].append(failure_msg)
            state["results"]["current_status"] = "tests_failed".lower()
            
            # Ajouter les dÃ©tails d'erreur si disponibles
            if "error_details" in result:
                state["results"]["test_error_details"] = result["error_details"]

        # âœ… DÃ‰CISION: Continuer le workflow mÃªme si les tests Ã©chouent (pour la validation humaine)
        state["results"]["should_continue"] = True
        
        logger.info("ğŸ Tests terminÃ©s")
        return state

    except Exception as e:
        error_msg = f"Exception lors des tests: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # CrÃ©er un rÃ©sultat d'erreur
        test_result = {
            "success": False,
            "error": error_msg,
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 1,
            "exception": str(e)
        }
        
        # Stocker dans une liste pour compatibilitÃ©
        if "test_results" not in state["results"]:
            state["results"]["test_results"] = []
        state["results"]["test_results"].append(test_result)
        state["results"]["test_success"] = False
        state["results"]["ai_messages"].append(f"âŒ Exception tests: {error_msg}")
        state["results"]["current_status"] = "tests_error".lower()
        state["results"]["should_continue"] = True  # Continuer malgrÃ© l'erreur
        
        return state


async def _create_and_run_automatic_tests(testing_engine, working_directory: str, code_changes: dict) -> dict:
    """CrÃ©e et exÃ©cute des tests automatiques pour les fichiers modifiÃ©s."""
    import os
    
    logger.info(f"ğŸ”§ CrÃ©ation de tests automatiques pour {len(code_changes)} fichiers modifiÃ©s")
    
    # CrÃ©er un rÃ©pertoire de tests temporaire
    test_dir = os.path.join(working_directory, "auto_tests")
    os.makedirs(test_dir, exist_ok=True)
    
    test_files_created = []
    
    for file_path, file_content in code_changes.items():
        if file_path.endswith('.py'):
            # CrÃ©er un test basique pour le fichier Python
            test_content = _generate_basic_python_test(file_path, file_content)
            test_file_name = f"test_{os.path.basename(file_path)}"
            test_file_path = os.path.join(test_dir, test_file_name)
            
            with open(test_file_path, 'w') as f:
                f.write(test_content)
            
            test_files_created.append(test_file_path)
            logger.info(f"ğŸ“ Test automatique crÃ©Ã©: {test_file_name}")
    
    if test_files_created:
        # ExÃ©cuter les tests crÃ©Ã©s
        try:
            result = await testing_engine._run_test_directory(test_dir)
            result["auto_generated"] = True
            result["test_files_created"] = len(test_files_created)
            return result
        except Exception as e:
            return {
                "success": False,
                "error": f"Erreur lors de l'exÃ©cution des tests automatiques: {e}",
                "auto_generated": True,
                "test_files_created": len(test_files_created)
            }
    else:
        return {
            "success": True,
            "warning": "Aucun fichier Python modifiÃ© - tests automatiques non applicables",
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "auto_generated": True
        }


def _generate_basic_python_test(file_path: str, file_content: str) -> str:
    """GÃ©nÃ¨re un test Python basique pour un fichier donnÃ©."""
    import os
    module_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Test basique qui vÃ©rifie que le module peut Ãªtre importÃ© et exÃ©cute des vÃ©rifications de base
    test_content = f'''"""Test automatique gÃ©nÃ©rÃ© pour {file_path}"""
import unittest
import sys
import os

# Ajouter le rÃ©pertoire parent au path pour l'import
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

class Test{module_name.replace("_", "").title()}(unittest.TestCase):
    """Tests automatiques pour {module_name}"""
    
    def test_module_import(self):
        """Test que le module peut Ãªtre importÃ©"""
        try:
            import {module_name}
            self.assertTrue(True, "Module importÃ© avec succÃ¨s")
        except ImportError as e:
            self.fail(f"Impossible d'importer {module_name}: {{e}}")
    
    def test_basic_syntax(self):
        """Test basique de syntaxe"""
        # VÃ©rifier que le contenu contient au moins du code Python valide
        content = """{repr(file_content[:500])}"""
        self.assertIsInstance(content, str)
        self.assertGreater(len(content), 0)
    
    def setUp(self):
        """Configuration avant chaque test"""
        pass
    
    def tearDown(self):
        """Nettoyage aprÃ¨s chaque test"""
        pass

if __name__ == '__main__':
    unittest.main()
'''
    
    return test_content


async def _run_basic_tests(working_directory: str) -> Dict[str, Any]:
    """
    Fallback simple pour exÃ©cuter des tests basiques quand TestingEngine Ã©choue.
    """
    logger.info("ğŸ§ª ExÃ©cution de tests basiques...")
    
    # CrÃ©er une instance de ClaudeCodeTool pour exÃ©cuter les commandes
    claude_tool = ClaudeCodeTool()
    claude_tool.working_directory = working_directory
    
    # Commandes de test Ã  essayer dans l'ordre
    test_commands = [
        "python -m pytest -v --tb=short",
        "python -m pytest",
        "python -m unittest discover -v",
        "python -m unittest",
        "npm test",
        "yarn test"
    ]
    
    results = {
        "success": False,
        "test_results": {
            "passed": 0,
            "failed": 0,
            "errors": [],
            "coverage": 0
        },
        "command_used": None,
        "output": ""
    }
    
    for command in test_commands:
        logger.info(f"ğŸ§ª Test avec commande: {command}")
        
        try:
            result = await claude_tool._arun(
                action="execute_command",
                command=command,
                cwd=working_directory
            )
            
            if result.get("success", False):
                # Analyser la sortie pour extraire les rÃ©sultats
                output = result.get("output", "")
                results["output"] = output
                results["command_used"] = command
                
                # Parser simple des rÃ©sultats pytest/unittest
                if "pytest" in command.lower():
                    results.update(_parse_pytest_simple(output))
                elif "unittest" in command.lower():
                    results.update(_parse_unittest_simple(output))
                elif "test" in command.lower():  # npm/yarn test
                    results.update(_parse_npm_test_simple(output))
                
                # Si au moins une commande a fonctionnÃ©, considÃ©rer comme succÃ¨s
                results["success"] = True
                logger.info(f"âœ… Tests exÃ©cutÃ©s avec: {command}")
                break
                
        except Exception as e:
            logger.debug(f"âš ï¸ Commande {command} Ã©chouÃ©e: {e}")
            continue
    
    if not results["success"]:
        results["test_results"]["errors"].append("Aucune commande de test fonctionnelle trouvÃ©e")
        results["test_results"]["failed"] = 1
        logger.warning("âŒ Aucune commande de test n'a fonctionnÃ©")
    
    return results


def _parse_pytest_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie pytest."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Chercher les patterns pytest
    import re
    
    # Pattern pour "X passed, Y failed"
    match = re.search(r'(\d+) passed.*?(\d+) failed', output)
    if match:
        results["test_results"]["passed"] = int(match.group(1))
        results["test_results"]["failed"] = int(match.group(2))
        results["success"] = int(match.group(2)) == 0
    
    # Pattern pour "X passed" seulement
    elif re.search(r'(\d+) passed', output):
        match = re.search(r'(\d+) passed', output)
        results["test_results"]["passed"] = int(match.group(1))
        results["success"] = True
    
    # Pas de tests trouvÃ©s
    elif "no tests ran" in output.lower() or "collected 0 items" in output.lower():
        results["test_results"]["errors"].append("Aucun test trouvÃ© par pytest")
    
    return results


def _parse_unittest_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie unittest."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Chercher "Ran X tests"
    import re
    match = re.search(r'Ran (\d+) tests?', output)
    if match:
        total_tests = int(match.group(1))
        
        # Chercher les Ã©checs
        if "FAILED" in output:
            # Compter les lignes avec FAIL
            failed_count = output.count("FAIL")
            results["test_results"]["failed"] = failed_count
            results["test_results"]["passed"] = total_tests - failed_count
        else:
            results["test_results"]["passed"] = total_tests
            results["success"] = True
    else:
        results["test_results"]["errors"].append("Aucun test trouvÃ© par unittest")
    
    return results


def _parse_npm_test_simple(output: str) -> Dict[str, Any]:
    """Parse simple de la sortie npm/yarn test."""
    results = {"success": False, "test_results": {"passed": 0, "failed": 0, "errors": []}}
    
    # Pattern simple pour dÃ©tecter succÃ¨s/Ã©chec npm test
    if "Test Suites: " in output:
        # Jest format
        import re
        match = re.search(r'(\d+) passed.*?(\d+) failed', output)
        if match:
            results["test_results"]["passed"] = int(match.group(1))
            results["test_results"]["failed"] = int(match.group(2))
            results["success"] = int(match.group(2)) == 0
    elif "passing" in output.lower():
        # Mocha format
        results["test_results"]["passed"] = 1
        results["success"] = True
    else:
        results["test_results"]["errors"].append("Format de test npm/yarn non reconnu")
    
    return results

async def _detect_test_commands(claude_tool: ClaudeCodeTool) -> list[str]:
    """DÃ©tecte les commandes de test disponibles selon le type de projet."""
    
    test_commands = []
    
    try:
        # VÃ©rifier les fichiers de configuration pour dÃ©tecter le type de projet
        
        # 1. Projet Node.js / JavaScript
        package_json_result = await claude_tool._arun(action="read_file", file_path="package.json", required=False)
        if package_json_result["success"] and package_json_result.get("file_exists", True):
            import json
            try:
                package_data = json.loads(package_json_result["content"])
                scripts = package_data.get("scripts", {})
                
                # Prioriser les scripts de test dÃ©finis
                if "test" in scripts:
                    test_commands.append("npm test")
                if "test:unit" in scripts:
                    test_commands.append("npm run test:unit")
                if "jest" in scripts:
                    test_commands.append("npm run jest")
                    
                # Ajouter des alternatives
                if not test_commands:
                    test_commands.extend(["npm test", "yarn test", "npx jest"])
                    
            except json.JSONDecodeError:
                test_commands.append("npm test")
        elif not package_json_result["success"]:
            logger.debug("ğŸ“ package.json non trouvÃ© - probablement un projet Python")
        
        # 2. Projet Python
        requirements_result = await claude_tool._arun(action="read_file", file_path="requirements.txt", required=False)
        setup_py_result = await claude_tool._arun(action="read_file", file_path="setup.py", required=False)
        pyproject_result = await claude_tool._arun(action="read_file", file_path="pyproject.toml", required=False)
        
        # âœ… CORRECTION: VÃ©rifier si au moins un fichier existe et a Ã©tÃ© lu avec succÃ¨s
        python_config_files = [
            ("requirements.txt", requirements_result),
            ("setup.py", setup_py_result), 
            ("pyproject.toml", pyproject_result)
        ]
        
        # Log les fichiers Python non trouvÃ©s en debug seulement
        for file_name, result in python_config_files:
            if not result["success"]:
                logger.debug(f"ğŸ“ {file_name} non trouvÃ© - normal pour certains projets Python")
        
        python_files_exist = any(
            result["success"] and result.get("file_exists", True) 
            for _, result in python_config_files
        )
        
        if python_files_exist:
            # DÃ©tecter le framework de test utilisÃ©
            all_content = ""
            for config_file, result in python_config_files:
                if result["success"] and result.get("file_exists", True):
                    all_content += result["content"].lower()
            
            if "pytest" in all_content:
                test_commands.extend(["python -m pytest", "pytest"])
            if "unittest" in all_content:
                test_commands.append("python -m unittest discover")
            if "nose" in all_content:
                test_commands.append("nosetests")
            
            # Commandes par dÃ©faut Python
            if not any("python" in cmd for cmd in test_commands):
                test_commands.extend([
                    "python -m pytest",
                    "python -m unittest discover", 
                    "python -m pytest tests/",
                    "python -m unittest"
                ])
        
        # 3. Autres types de projets
        
        # Makefile
        makefile_result = await claude_tool._arun(action="read_file", file_path="Makefile")
        if makefile_result["success"] and makefile_result.get("file_exists", True) and "test" in makefile_result["content"].lower():
            test_commands.append("make test")
        
        # Cargo (Rust)
        cargo_result = await claude_tool._arun(action="read_file", file_path="Cargo.toml")
        if cargo_result["success"] and cargo_result.get("file_exists", True):
            test_commands.append("cargo test")
        
        # Go
        go_mod_result = await claude_tool._arun(action="read_file", file_path="go.mod")
        if go_mod_result["success"] and go_mod_result.get("file_exists", True):
            test_commands.append("go test ./...")
        
        # Composer (PHP)
        composer_result = await claude_tool._arun(action="read_file", file_path="composer.json")
        if composer_result["success"] and composer_result.get("file_exists", True):
            test_commands.extend(["composer test", "./vendor/bin/phpunit"])
        
    except Exception as e:
        logger.warning(f"Erreur lors de la dÃ©tection des commandes de test: {e}")
    
    # Supprimer les doublons tout en prÃ©servant l'ordre
    seen = set()
    unique_commands = []
    for cmd in test_commands:
        if cmd not in seen:
            seen.add(cmd)
            unique_commands.append(cmd)
    
    return unique_commands

async def _analyze_test_failure(test_result: Dict[str, Any], state: Dict[str, Any]) -> str:
    """Analyse les Ã©checs de tests pour fournir un diagnostic."""
    
    # GÃ©rer Ã  la fois les objets TestResult et les dictionnaires
    if hasattr(test_result, 'stderr'):
        # C'est un objet TestResult
        error_output = getattr(test_result, 'stderr', '') or getattr(test_result, 'stdout', '')
        exit_code = getattr(test_result, 'exit_code', 1)
    else:
        # C'est un dictionnaire
        error_output = test_result.get("error", "") or test_result.get("output", "")
        test_results = test_result.get("test_results", {})
        errors = test_results.get("errors", [])
        if errors:
            error_output += "\n" + "\n".join(errors)
        exit_code = test_result.get("exit_code", 1)
    
    # Patterns d'erreurs communes
    error_patterns = {
        "module not found": "Module manquant - vÃ©rifier les imports et dÃ©pendances",
        "no module named": "Module Python manquant - vÃ©rifier les imports",
        "cannot find module": "Module Node.js manquant - vÃ©rifier package.json",
        "import error": "Erreur d'import - vÃ©rifier les chemins et dÃ©pendances",
        "syntax error": "Erreur de syntaxe dans le code",
        "indentation error": "Erreur d'indentation Python",
        "unexpected token": "Erreur de syntaxe JavaScript/TypeScript",
        "command not found": "Commande de test non trouvÃ©e - installer les dÃ©pendances",
        "permission denied": "ProblÃ¨me de permissions - vÃ©rifier l'exÃ©cutable",
        "connection refused": "ProblÃ¨me de connexion - services externes requis?",
        "timeout": "Test trop lent - optimiser ou augmenter timeout",
        "assertion": "Assertion Ã©chouÃ©e - logique mÃ©tier incorrecte",
        "failed": "Test(s) Ã©chouÃ©(s)",
        "error": "Erreur gÃ©nÃ©rale"
    }
    
    error_analysis = "Ã‰chec de test non spÃ©cifique"
    
    # Chercher des patterns d'erreur dans la sortie
    error_output_lower = error_output.lower()
    for pattern, description in error_patterns.items():
        if pattern in error_output_lower:
            error_analysis = description
            break
    
    # Ajouter des dÃ©tails spÃ©cifiques si disponibles
    if exit_code == 127:
        error_analysis = "Commande de test non trouvÃ©e - installer les dÃ©pendances"
    elif exit_code == 2:
        error_analysis = "Erreur de configuration ou arguments invalides"
    elif exit_code == 1:
        error_analysis = "Tests Ã©chouÃ©s - vÃ©rifier la logique mÃ©tier"
    
    # Extraire les premiÃ¨res lignes d'erreur pour plus de contexte
    error_lines = error_output.split('\n')[:5]
    relevant_errors = [line.strip() for line in error_lines if line.strip() and not line.startswith('===')]
    
    if relevant_errors:
        error_context = ' | '.join(relevant_errors[:2])  # Maximum 2 lignes
        error_analysis += f" ({error_context})"
    
    return error_analysis

def should_continue_to_debug(state: Dict[str, Any]) -> bool:
    """DÃ©termine si le workflow doit continuer vers le debug."""
    
    # VÃ©rifier s'il y a eu des Ã©checs de tests rÃ©cents
    if not state["results"]["test_results"]:
        return False
    
    latest_test = state["results"]["test_results"][-1]
    
    # Si le dernier test a rÃ©ussi, pas besoin de debug
    if latest_test.success:
        return False
    
    # VÃ©rifier si on n'a pas dÃ©passÃ© la limite de tentatives
    debug_attempts = state["results"].get("debug_attempts", 0)
    max_debug_attempts = state["results"].get("max_debug_attempts", 3)
    if debug_attempts >= max_debug_attempts:
        return False
    
    return True

def should_continue_to_finalize(state: Dict[str, Any]) -> bool:
    """DÃ©termine si le workflow peut passer Ã  la finalisation."""
    
    # VÃ©rifier s'il y a eu des tests
    if not state["results"]["test_results"]:
        return False
    
    # Le dernier test doit avoir rÃ©ussi
    latest_test = state["results"]["test_results"][-1]
    return latest_test.success 