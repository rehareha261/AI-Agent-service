#!/usr/bin/env python3
"""Test de la configuration OpenAI comme provider principal."""

import asyncio
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()


async def test_configuration():
    """Teste que OpenAI est bien configurÃ© comme provider principal partout."""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         ğŸ§ª TEST CONFIGURATION OPENAI COMME PROVIDER PRINCIPAL     â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    results = []
    
    # Test 1: Settings
    print("ğŸ“‹ Test 1: Configuration Settings")
    print("â”€" * 70)
    try:
        from config.settings import get_settings
        settings = get_settings()
        
        print(f"  â€¢ default_ai_provider: {settings.default_ai_provider}")
        print(f"  â€¢ openai_api_key: {'âœ… DÃ©finie' if settings.openai_api_key else 'âŒ Manquante'}")
        print(f"  â€¢ anthropic_api_key: {'âœ… DÃ©finie' if settings.anthropic_api_key else 'âŒ Non requise'}")
        
        if settings.default_ai_provider.lower() == "openai":
            print("  âœ… Provider par dÃ©faut: OpenAI")
            results.append(("Settings", True))
        else:
            print(f"  âŒ Provider par dÃ©faut incorrect: {settings.default_ai_provider}")
            results.append(("Settings", False))
            
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("Settings", False))
    
    print()
    
    # Test 2: LLM Factory - ModÃ¨le par dÃ©faut
    print("ğŸ“‹ Test 2: ModÃ¨le OpenAI par dÃ©faut")
    print("â”€" * 70)
    try:
        from ai.llm.llm_factory import DEFAULT_MODELS
        
        openai_model = DEFAULT_MODELS.get("openai")
        print(f"  â€¢ ModÃ¨le OpenAI: {openai_model}")
        
        if openai_model == "gpt-4o":
            print("  âœ… ModÃ¨le moderne configurÃ© (gpt-4o)")
            results.append(("ModÃ¨le OpenAI", True))
        else:
            print(f"  âš ï¸  ModÃ¨le: {openai_model} (recommandÃ©: gpt-4o)")
            results.append(("ModÃ¨le OpenAI", True))  # Pas bloquant
            
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("ModÃ¨le OpenAI", False))
    
    print()
    
    # Test 3: get_llm par dÃ©faut
    print("ğŸ“‹ Test 3: Fonction get_llm() par dÃ©faut")
    print("â”€" * 70)
    try:
        from ai.llm.llm_factory import get_llm
        import inspect
        
        sig = inspect.signature(get_llm)
        provider_default = sig.parameters['provider'].default
        
        print(f"  â€¢ ParamÃ¨tre provider par dÃ©faut: {provider_default}")
        
        if provider_default == "openai":
            print("  âœ… get_llm() utilise OpenAI par dÃ©faut")
            results.append(("get_llm", True))
        else:
            print(f"  âŒ get_llm() utilise {provider_default} par dÃ©faut")
            results.append(("get_llm", False))
            
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("get_llm", False))
    
    print()
    
    # Test 4: get_llm_with_fallback par dÃ©faut
    print("ğŸ“‹ Test 4: Fonction get_llm_with_fallback() par dÃ©faut")
    print("â”€" * 70)
    try:
        from ai.llm.llm_factory import get_llm_with_fallback
        import inspect
        
        sig = inspect.signature(get_llm_with_fallback)
        primary_default = sig.parameters['primary_provider'].default
        
        print(f"  â€¢ ParamÃ¨tre primary_provider par dÃ©faut: {primary_default}")
        
        if primary_default == "openai":
            print("  âœ… get_llm_with_fallback() utilise OpenAI par dÃ©faut")
            results.append(("get_llm_with_fallback", True))
        else:
            print(f"  âŒ get_llm_with_fallback() utilise {primary_default} par dÃ©faut")
            results.append(("get_llm_with_fallback", False))
            
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("get_llm_with_fallback", False))
    
    print()
    
    # Test 5: CrÃ©ation d'un LLM par dÃ©faut
    print("ğŸ“‹ Test 5: CrÃ©ation effective d'un LLM par dÃ©faut")
    print("â”€" * 70)
    try:
        from ai.llm.llm_factory import get_default_llm_with_fallback
        
        print("  â€¢ CrÃ©ation du LLM...")
        llm = get_default_llm_with_fallback(temperature=0.1, max_tokens=100)
        
        # Le LLM avec fallback n'expose pas directement le provider
        # mais on peut vÃ©rifier via les logs
        print("  âœ… LLM crÃ©Ã© avec succÃ¨s")
        print("  â€¢ Configuration: OpenAI (principal) â†’ Anthropic (fallback)")
        results.append(("CrÃ©ation LLM", True))
        
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("CrÃ©ation LLM", False))
    
    print()
    
    # Test 6: Test de gÃ©nÃ©ration rÃ©elle
    print("ğŸ“‹ Test 6: GÃ©nÃ©ration de texte avec OpenAI")
    print("â”€" * 70)
    try:
        from ai.llm.llm_factory import get_llm
        from langchain_core.messages import HumanMessage
        
        print("  â€¢ CrÃ©ation du LLM OpenAI...")
        llm = get_llm(provider="openai", temperature=0.1, max_tokens=50)
        
        print("  â€¢ Envoi d'une requÃªte de test...")
        messages = [HumanMessage(content="RÃ©ponds juste 'OpenAI fonctionne' si tu me reÃ§ois.")]
        response = await llm.ainvoke(messages)
        
        print(f"  â€¢ RÃ©ponse: {response.content[:100]}...")
        print("  âœ… OpenAI rÃ©pond correctement")
        results.append(("GÃ©nÃ©ration", True))
        
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        results.append(("GÃ©nÃ©ration", False))
    
    print()
    
    # RÃ©sumÃ©
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                          RÃ‰SUMÃ‰ FINAL                              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    success_count = sum(1 for _, passed in results if passed)
    total_count = len(results)
    
    for name, passed in results:
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {name}")
    
    print()
    print(f"Score: {success_count}/{total_count} tests passÃ©s")
    print()
    
    if success_count == total_count:
        print("â•" * 70)
        print("ğŸ‰ SUCCÃˆS TOTAL! OpenAI est configurÃ© comme provider principal")
        print("â•" * 70)
        print()
        print("âœ… Tous les appels LLM utiliseront maintenant OpenAI par dÃ©faut")
        print("âœ… Fallback automatique vers Anthropic en cas d'erreur")
        print("âœ… ModÃ¨le: gpt-4o (performant et moderne)")
        print()
        return True
    else:
        print("â•" * 70)
        print("âš ï¸  CONFIGURATION INCOMPLÃˆTE")
        print("â•" * 70)
        print()
        print("Certains tests ont Ã©chouÃ©. VÃ©rifiez les erreurs ci-dessus.")
        print()
        return False


if __name__ == "__main__":
    try:
        result = asyncio.run(test_configuration())
        exit(0 if result else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrompu")
        exit(1)
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

