#!/usr/bin/env python3
"""Test rapide pour vÃ©rifier les corrections du workflow."""

import asyncio
from graph.workflow_graph import create_workflow_graph
from models.schemas import TaskRequest
from services.monitoring_service import monitoring_service
from utils.logger import get_logger

logger = get_logger(__name__)

async def test_workflow_compilation():
    """Test que le workflow se compile correctement."""
    logger.info("ğŸ§ª Test de compilation du workflow")
    
    try:
        # CrÃ©er le graphe
        workflow_graph = create_workflow_graph()
        logger.info("âœ… Graphe crÃ©Ã© avec succÃ¨s")
        
        # Compiler le graphe
        from langgraph.checkpoint.memory import MemorySaver
        checkpointer = MemorySaver()
        compiled_graph = workflow_graph.compile(checkpointer=checkpointer)
        logger.info("âœ… Graphe compilÃ© avec succÃ¨s")
        
        # VÃ©rifier que la mÃ©thode astream existe
        if hasattr(compiled_graph, 'astream'):
            logger.info("âœ… MÃ©thode astream disponible")
            return True
        else:
            logger.error("âŒ MÃ©thode astream manquante")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erreur de compilation: {e}")
        return False

async def test_monitoring_startup():
    """Test que le monitoring dÃ©marre correctement."""
    logger.info("ğŸ§ª Test de dÃ©marrage du monitoring")
    
    try:
        # ArrÃªter le monitoring s'il est actif
        if monitoring_service.is_monitoring_active:
            await monitoring_service.stop_monitoring()
        
        # DÃ©marrer le monitoring
        await monitoring_service.start_monitoring()
        
        # VÃ©rifier qu'il est actif
        if monitoring_service.is_monitoring_active:
            logger.info("âœ… Monitoring actif")
            
            # VÃ©rifier les tÃ¢ches en arriÃ¨re-plan
            active_tasks = sum(1 for task in monitoring_service.background_tasks if not task.done())
            logger.info(f"âœ… {active_tasks} tÃ¢ches de monitoring actives")
            
            return active_tasks > 0
        else:
            logger.error("âŒ Monitoring non actif")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erreur monitoring: {e}")
        return False

async def test_workflow_state_creation():
    """Test de crÃ©ation d'Ã©tat de workflow."""
    logger.info("ğŸ§ª Test de crÃ©ation d'Ã©tat workflow")
    
    try:
        # CrÃ©er une tÃ¢che de test
        task_request = TaskRequest(
            task_id="test_123",
            title="Test workflow fixes",
            description="Test de correction",
            priority="medium"
        )
        
        # Tester la crÃ©ation d'Ã©tat initial
        from graph.workflow_graph import _create_initial_state_with_recovery
        initial_state = _create_initial_state_with_recovery(task_request, "test_workflow", 1, "test_uuid")
        
        if isinstance(initial_state, dict) and "task" in initial_state:
            logger.info("âœ… Ã‰tat initial crÃ©Ã© correctement")
            return True
        else:
            logger.error("âŒ Ã‰tat initial invalide")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erreur crÃ©ation Ã©tat: {e}")
        return False

async def main():
    """Test principal."""
    logger.info("ğŸš€ Tests de correction du workflow")
    
    tests = [
        ("Compilation workflow", test_workflow_compilation),
        ("DÃ©marrage monitoring", test_monitoring_startup),
        ("CrÃ©ation Ã©tat workflow", test_workflow_state_creation),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = await test_func()
            results.append((test_name, result))
            status = "âœ… PASSÃ‰" if result else "âŒ Ã‰CHOUÃ‰"
            logger.info(f"{status} - {test_name}")
        except Exception as e:
            logger.error(f"ğŸ’¥ ERREUR - {test_name}: {e}")
            results.append((test_name, False))
    
    # RÃ©sumÃ©
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    logger.info(f"\nğŸ“Š RÃ©sultats: {passed}/{total} tests rÃ©ussis")
    
    if passed == total:
        logger.info("ğŸ‰ Toutes les corrections fonctionnent !")
    else:
        logger.warning("âš ï¸ Certaines corrections nÃ©cessitent encore du travail")
    
    # Nettoyer
    try:
        if monitoring_service.is_monitoring_active:
            await monitoring_service.stop_monitoring()
    except:
        pass
    
    return passed == total

if __name__ == "__main__":
    asyncio.run(main()) 