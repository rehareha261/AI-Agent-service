#!/usr/bin/env python3
"""
Tests d'int√©gration complets pour v√©rifier la coh√©rence des corrections avec le workflow AI-Agent.

Ce script teste l'int√©gration entre :
- Les am√©liorations de s√©rialisation JSON 
- La gestion robuste des r√©pertoires de travail
- Le nouveau service de PR
- Le service de logging am√©lior√©
- La coh√©rence entre tous les n≈ìuds
"""

import asyncio
import tempfile
import os
import json
from datetime import datetime
from unittest.mock import patch, AsyncMock

# Imports des corrections √† tester
from services.database_persistence_service import DatabasePersistenceService
from services.pull_request_service import pr_service
from services.logging_service import logging_service
from utils.helpers import get_working_directory, ensure_working_directory, validate_working_directory
from models.state import GraphState
from models.schemas import TaskRequest, TaskPriority, TaskType
from nodes.test_node import run_tests
from nodes.debug_node import debug_code
from nodes.qa_node import quality_assurance_automation
from nodes.implement_node import implement_task


async def test_workflow_integration():
    """Test d'int√©gration complet du workflow avec toutes les corrections."""
    
    print("üß™ TESTS D'INT√âGRATION WORKFLOW AI-AGENT")
    print("=" * 60)
    
    passed_tests = 0
    total_tests = 6
    
    # Test 1: Int√©gration s√©rialisation JSON avec n≈ìuds
    try:
        print("üß™ Test 1: Int√©gration s√©rialisation JSON avec workflow")
        await test_json_integration_with_nodes()
        print("  ‚úÖ Test int√©gration JSON: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur int√©gration JSON: {e}\n")
    
    # Test 2: Int√©gration gestion r√©pertoires entre n≈ìuds
    try:
        print("üß™ Test 2: Propagation r√©pertoire de travail entre n≈ìuds")
        await test_working_directory_propagation()
        print("  ‚úÖ Test propagation r√©pertoires: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur propagation r√©pertoires: {e}\n")
    
    # Test 3: Int√©gration service PR avec n≈ìuds
    try:
        print("üß™ Test 3: Int√©gration service PR avec merge_node")
        await test_pr_service_integration()
        print("  ‚úÖ Test int√©gration service PR: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur int√©gration service PR: {e}\n")
    
    # Test 4: Coh√©rence des √©tats entre n≈ìuds
    try:
        print("üß™ Test 4: Coh√©rence des √©tats entre n≈ìuds")
        await test_state_consistency_across_nodes()
        print("  ‚úÖ Test coh√©rence √©tats: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur coh√©rence √©tats: {e}\n")
    
    # Test 5: Gestion d'erreurs robuste
    try:
        print("üß™ Test 5: Gestion d'erreurs robuste entre n≈ìuds")
        await test_robust_error_handling()
        print("  ‚úÖ Test gestion erreurs robuste: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur gestion erreurs robuste: {e}\n")
    
    # Test 6: Performance et logging
    try:
        print("üß™ Test 6: Performance et logging int√©gr√©")
        await test_performance_and_logging()
        print("  ‚úÖ Test performance et logging: R√âUSSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  ‚ùå Erreur performance et logging: {e}\n")
    
    # R√©sum√© final
    print("=" * 60)
    print("üìä R√âSUM√â TESTS D'INT√âGRATION")
    print("=" * 60)
    print(f"Int√©gration JSON.............................. {'‚úÖ R√âUSSI' if passed_tests >= 1 else '‚ùå √âCHOU√â'}")
    print(f"Propagation r√©pertoires....................... {'‚úÖ R√âUSSI' if passed_tests >= 2 else '‚ùå √âCHOU√â'}")
    print(f"Int√©gration service PR........................ {'‚úÖ R√âUSSI' if passed_tests >= 3 else '‚ùå √âCHOU√â'}")
    print(f"Coh√©rence √©tats............................... {'‚úÖ R√âUSSI' if passed_tests >= 4 else '‚ùå √âCHOU√â'}")
    print(f"Gestion erreurs robuste....................... {'‚úÖ R√âUSSI' if passed_tests >= 5 else '‚ùå √âCHOU√â'}")
    print(f"Performance et logging........................ {'‚úÖ R√âUSSI' if passed_tests >= 6 else '‚ùå √âCHOU√â'}")
    print("=" * 60)
    print(f"üéØ R√âSULTAT GLOBAL: {passed_tests}/{total_tests} tests r√©ussis")
    
    if passed_tests == total_tests:
        print("‚úÖ Toutes les corrections sont coh√©rentes et int√©gr√©es !")
    elif passed_tests >= total_tests * 0.8:
        print("‚ö†Ô∏è La plupart des corrections fonctionnent, quelques ajustements peuvent √™tre n√©cessaires.")
    else:
        print("‚ùå Des probl√®mes d'int√©gration majeurs d√©tect√©s.")
    
    return passed_tests == total_tests


async def test_json_integration_with_nodes():
    """Test que la s√©rialisation JSON fonctionne avec les vrais n≈ìuds."""
    
    # Cr√©er un √©tat complexe comme dans un vrai workflow
    task = TaskRequest(
        task_id="integration_test",
        title="Test int√©gration JSON",
        description="Test pour v√©rifier l'int√©gration de la s√©rialisation JSON",
        priority=TaskPriority.HIGH,
        task_type=TaskType.FEATURE
    )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        state = GraphState(
            task=task,
            results={
                "working_directory": temp_dir,
                "complex_data": {
                    "nested_object": {"key": "value"},
                    "list_data": [1, 2, 3],
                    "datetime": datetime.now(),
                    "none_value": None
                },
                "ai_messages": [],
                "error_logs": []
            }
        )
        
        # Tester la persistence avec DatabasePersistenceService
        persistence = DatabasePersistenceService()
        
        # Nettoyer les donn√©es comme le ferait le service
        cleaned_data = persistence._clean_for_json_serialization(state["results"]["complex_data"])
        print(f"  ‚úÖ Donn√©es complexes nettoy√©es: {type(cleaned_data)}")
        
        # V√©rifier que les donn√©es peuvent √™tre s√©rialis√©es
        json_str = json.dumps(cleaned_data, ensure_ascii=False)
        print(f"  ‚úÖ S√©rialisation r√©ussie: {len(json_str)} caract√®res")
        
        # V√©rifier la d√©s√©rialisation
        deserialized = json.loads(json_str)
        assert isinstance(deserialized, dict), "La d√©s√©rialisation doit retourner un dict"
        print(f"  ‚úÖ D√©s√©rialisation r√©ussie: {len(deserialized)} cl√©s")


async def test_working_directory_propagation():
    """Test la propagation du r√©pertoire de travail entre les n≈ìuds."""
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # √âtat initial sans working_directory
        # Utiliser un dictionnaire standard au lieu de GraphState pour ce test
        initial_state = {
            "task": TaskRequest(
                task_id="wd_test",
                title="Test WD propagation",
                description="Test propagation working directory",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.BUGFIX
            ),
            "results": {
                "prepare_result": {"working_directory": temp_dir},
                "ai_messages": [],
                "error_logs": []
            }
        }
        
        # Test 1: get_working_directory trouve le r√©pertoire
        wd = get_working_directory(initial_state)
        assert wd == temp_dir, f"WD devrait √™tre {temp_dir}, trouv√© {wd}"
        print(f"  ‚úÖ get_working_directory fonctionne: {wd}")
        
        # Test 2: validate_working_directory valide correctement
        is_valid = validate_working_directory(wd, "test_propagation")
        assert is_valid, f"WD {wd} devrait √™tre valide"
        print(f"  ‚úÖ validate_working_directory fonctionne: {is_valid}")
        
        # Test 3: ensure_working_directory utilise l'existant
        ensured_wd = ensure_working_directory(initial_state, "test_")
        assert ensured_wd == temp_dir, f"ensure_working_directory devrait retourner {temp_dir}"
        print(f"  ‚úÖ ensure_working_directory utilise l'existant: {ensured_wd}")
        
        # Test 4: √âtat sans r√©pertoire - cr√©ation automatique
        empty_state = {
            "task": TaskRequest(
                task_id="empty_test",
                title="Empty test",
                description="Test creation automatique",
                priority=TaskPriority.LOW,
                task_type=TaskType.TESTING
            ),
            "results": {"ai_messages": [], "error_logs": []}
        }
        
        new_wd = ensure_working_directory(empty_state, "auto_")
        assert os.path.exists(new_wd), f"Nouveau WD {new_wd} devrait exister"
        print(f"  ‚úÖ Cr√©ation automatique WD fonctionne: {new_wd}")


async def test_pr_service_integration():
    """Test l'int√©gration du service PR avec les n≈ìuds."""
    
    # Mock des appels GitHub pour √©viter les erreurs d'API
    with patch('tools.github_tool.GitHubTool._arun') as mock_github:
        async def mock_github_call(*args, **kwargs):
            return {
                "success": True,
                "pr_number": 123,
                "pr_url": "https://github.com/test/repo/pull/123"
            }
        mock_github.side_effect = mock_github_call
        
        with tempfile.TemporaryDirectory() as temp_dir:
            state = GraphState(
                task=TaskRequest(
                    task_id="pr_test",
                    title="Test PR service",
                    description="Test int√©gration service PR",
                    priority=TaskPriority.HIGH,
                    task_type=TaskType.FEATURE,
                    repository_url="https://github.com/test/repo"
                ),
                results={
                    "working_directory": temp_dir,
                    "git_result": {"branch_name": "feature/test"},
                    "ai_messages": [],
                    "error_logs": []
                }
            )
            
            # Test cr√©ation PR via le service
            result = await pr_service.ensure_pull_request_created(state)
            
            # Le service devrait appeler GitHub m√™me si mock√©
            assert result.success, f"PR creation devrait r√©ussir: {result.error}"
            assert result.pr_info is not None, "pr_info devrait √™tre pr√©sent"
            assert result.pr_info.number == 123, "Le num√©ro de PR devrait √™tre 123"
            print(f"  ‚úÖ Service PR fonctionne: #{result.pr_info.number}")
            
            # Le service PR ne met pas automatiquement √† jour l'√©tat dans cette version
            # C'est le travail du n≈ìud qui l'appelle (merge_node)
            print(f"  ‚úÖ Service PR fonctionne correctement")


async def test_state_consistency_across_nodes():
    """Test la coh√©rence des √©tats quand ils passent entre les n≈ìuds."""
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # Cr√©er un fichier test pour que le r√©pertoire ne soit pas vide
        test_file = os.path.join(temp_dir, "test.py")
        with open(test_file, 'w') as f:
            f.write("# Test file\nprint('hello')\n")
        
        initial_state = GraphState(
            task=TaskRequest(
                task_id="consistency_test",
                title="Test coh√©rence √©tats",
                description="Test coh√©rence entre n≈ìuds",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.FEATURE
            ),
            results={
                "working_directory": temp_dir,
                "ai_messages": [],
                "error_logs": [],
                "debug_attempts": 0
            }
        )
        
        # Simuler le passage par plusieurs n≈ìuds et v√©rifier la coh√©rence
        print(f"  üìÅ √âtat initial WD: {initial_state['results']['working_directory']}")
        
        # Chaque n≈ìud devrait pr√©server et valider le working_directory
        for node_name in ["test_node", "debug_node", "qa_node"]:
            wd = get_working_directory(initial_state)
            assert wd == temp_dir, f"{node_name}: WD incoh√©rent, attendu {temp_dir}, trouv√© {wd}"
            
            is_valid = validate_working_directory(wd, node_name)
            assert is_valid, f"{node_name}: WD devrait √™tre valide"
            
            print(f"  ‚úÖ {node_name}: WD coh√©rent et valide")
        
        # V√©rifier que les messages d'erreur sont conserv√©s
        initial_state["results"]["error_logs"].append("Test error")
        assert len(initial_state["results"]["error_logs"]) == 1, "Error logs devraient √™tre pr√©serv√©s"
        print(f"  ‚úÖ Error logs pr√©serv√©s: {len(initial_state['results']['error_logs'])}")


async def test_robust_error_handling():
    """Test la gestion d'erreurs robuste entre les n≈ìuds."""
    
    # Test 1: √âtat avec r√©pertoire invalide
    invalid_state = GraphState(
        task=TaskRequest(
            task_id="error_test",
            title="Test gestion erreurs",
            description="Test robustesse",
            priority=TaskPriority.LOW,
            task_type=TaskType.BUGFIX
        ),
        results={
            "working_directory": "/path/that/does/not/exist",
            "ai_messages": [],
            "error_logs": []
        }
    )
    
    # ensure_working_directory devrait cr√©er un nouveau r√©pertoire
    recovered_wd = ensure_working_directory(invalid_state, "error_recovery_")
    assert os.path.exists(recovered_wd), "WD de r√©cup√©ration devrait exister"
    print(f"  ‚úÖ R√©cup√©ration r√©pertoire invalide: {recovered_wd}")
    
    # Test 2: √âtat avec donn√©es corrompues
    corrupted_state = GraphState(
        task=TaskRequest(
            task_id="corrupted_test",
            title="Test donn√©es corrompues",
            description="Test robustesse donn√©es",
            priority=TaskPriority.HIGH,
                            task_type=TaskType.TESTING
        ),
        results={
            "ai_messages": [],
            "error_logs": [],
            "corrupted_data": object()  # Objet non-s√©rialisable
        }
    )
    
    # La s√©rialisation devrait nettoyer les donn√©es corrompues
    persistence = DatabasePersistenceService()
    cleaned = persistence._clean_for_json_serialization(corrupted_state["results"]["corrupted_data"])
    assert isinstance(cleaned, str), "Donn√©es corrompues devraient √™tre converties en string"
    print(f"  ‚úÖ Donn√©es corrompues nettoy√©es: {type(cleaned)}")


async def test_performance_and_logging():
    """Test les performances et le logging int√©gr√©."""
    
    import time
    
    # Test configuration du logging
    logging_configured = logging_service.setup_logging()
    assert logging_configured, "Le logging devrait √™tre configur√©"
    
    logs_info = logging_service.get_logs_info()
    assert logs_info['logs_directory'] is not None, "R√©pertoire logs devrait √™tre configur√©"
    assert logs_info['handlers_count'] > 0, "Des handlers devraient √™tre configur√©s"
    print(f"  ‚úÖ Logging configur√©: {logs_info['handlers_count']} handlers")
    
    # Test performance des fonctions helpers
    with tempfile.TemporaryDirectory() as temp_dir:
        state = GraphState(
            task=TaskRequest(
                task_id="perf_test",
                title="Test performance",
                description="Test performances",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.PERFORMANCE
            ),
            results={
                "working_directory": temp_dir,
                "ai_messages": [],
                "error_logs": []
            }
        )
        
        # Mesurer le temps d'ex√©cution des op√©rations courantes
        start_time = time.time()
        
        for i in range(100):
            wd = get_working_directory(state)
            validate_working_directory(wd, f"perf_test_{i}")
        
        duration = time.time() - start_time
        assert duration < 1.0, f"100 op√©rations WD devraient prendre < 1s, pris {duration:.3f}s"
        print(f"  ‚úÖ Performance WD: 100 ops en {duration:.3f}s")
        
        # Test s√©rialisation de grandes donn√©es
        large_data = {"data": [{"item": i, "value": f"test_{i}"} for i in range(1000)]}
        
        start_time = time.time()
        persistence = DatabasePersistenceService()
        cleaned = persistence._clean_for_json_serialization(large_data)
        json_str = json.dumps(cleaned)
        duration = time.time() - start_time
        
        assert duration < 0.5, f"S√©rialisation 1000 items devrait prendre < 0.5s, pris {duration:.3f}s"
        assert len(json_str) > 0, "JSON s√©rialis√© devrait avoir du contenu"
        print(f"  ‚úÖ Performance JSON: 1000 items en {duration:.3f}s, {len(json_str)} chars")


if __name__ == "__main__":
    print("üöÄ D√©marrage des tests d'int√©gration...")
    success = asyncio.run(test_workflow_integration())
    exit(0 if success else 1) 