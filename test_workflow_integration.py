#!/usr/bin/env python3
"""
Tests d'intÃ©gration complets pour vÃ©rifier la cohÃ©rence des corrections avec le workflow AI-Agent.

Ce script teste l'intÃ©gration entre :
- Les amÃ©liorations de sÃ©rialisation JSON 
- La gestion robuste des rÃ©pertoires de travail
- Le nouveau service de PR
- Le service de logging amÃ©liorÃ©
- La cohÃ©rence entre tous les nÅ“uds
"""

import asyncio
import tempfile
import os
import json
from datetime import datetime
from unittest.mock import patch, AsyncMock

# Imports des corrections Ã  tester
from services.database_persistence_service import DatabasePersistenceService
from services.pull_request_service import pr_service
from services.logging_service import logging_service
from utils.helpers import get_working_directory, ensure_working_directory, validate_working_directory
from models.state import GraphState
from models.schemas import TaskRequest, TaskPriority, TaskType
from nodes.test_node import run_tests
from nodes.debug_node import debug_code
from nodes.qa_node import quality_assurance_automation
from nodes.implement_node import implement_task


async def test_workflow_integration():
    """Test d'intÃ©gration complet du workflow avec toutes les corrections."""
    
    print("ðŸ§ª TESTS D'INTÃ‰GRATION WORKFLOW AI-AGENT")
    print("=" * 60)
    
    passed_tests = 0
    total_tests = 6
    
    # Test 1: IntÃ©gration sÃ©rialisation JSON avec nÅ“uds
    try:
        print("ðŸ§ª Test 1: IntÃ©gration sÃ©rialisation JSON avec workflow")
        await test_json_integration_with_nodes()
        print("  âœ… Test intÃ©gration JSON: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur intÃ©gration JSON: {e}\n")
    
    # Test 2: IntÃ©gration gestion rÃ©pertoires entre nÅ“uds
    try:
        print("ðŸ§ª Test 2: Propagation rÃ©pertoire de travail entre nÅ“uds")
        await test_working_directory_propagation()
        print("  âœ… Test propagation rÃ©pertoires: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur propagation rÃ©pertoires: {e}\n")
    
    # Test 3: IntÃ©gration service PR avec nÅ“uds
    try:
        print("ðŸ§ª Test 3: IntÃ©gration service PR avec merge_node")
        await test_pr_service_integration()
        print("  âœ… Test intÃ©gration service PR: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur intÃ©gration service PR: {e}\n")
    
    # Test 4: CohÃ©rence des Ã©tats entre nÅ“uds
    try:
        print("ðŸ§ª Test 4: CohÃ©rence des Ã©tats entre nÅ“uds")
        await test_state_consistency_across_nodes()
        print("  âœ… Test cohÃ©rence Ã©tats: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur cohÃ©rence Ã©tats: {e}\n")
    
    # Test 5: Gestion d'erreurs robuste
    try:
        print("ðŸ§ª Test 5: Gestion d'erreurs robuste entre nÅ“uds")
        await test_robust_error_handling()
        print("  âœ… Test gestion erreurs robuste: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur gestion erreurs robuste: {e}\n")
    
    # Test 6: Performance et logging
    try:
        print("ðŸ§ª Test 6: Performance et logging intÃ©grÃ©")
        await test_performance_and_logging()
        print("  âœ… Test performance et logging: RÃ‰USSI\n")
        passed_tests += 1
    except Exception as e:
        print(f"  âŒ Erreur performance et logging: {e}\n")
    
    # RÃ©sumÃ© final
    print("=" * 60)
    print("ðŸ“Š RÃ‰SUMÃ‰ TESTS D'INTÃ‰GRATION")
    print("=" * 60)
    print(f"IntÃ©gration JSON.............................. {'âœ… RÃ‰USSI' if passed_tests >= 1 else 'âŒ Ã‰CHOUÃ‰'}")
    print(f"Propagation rÃ©pertoires....................... {'âœ… RÃ‰USSI' if passed_tests >= 2 else 'âŒ Ã‰CHOUÃ‰'}")
    print(f"IntÃ©gration service PR........................ {'âœ… RÃ‰USSI' if passed_tests >= 3 else 'âŒ Ã‰CHOUÃ‰'}")
    print(f"CohÃ©rence Ã©tats............................... {'âœ… RÃ‰USSI' if passed_tests >= 4 else 'âŒ Ã‰CHOUÃ‰'}")
    print(f"Gestion erreurs robuste....................... {'âœ… RÃ‰USSI' if passed_tests >= 5 else 'âŒ Ã‰CHOUÃ‰'}")
    print(f"Performance et logging........................ {'âœ… RÃ‰USSI' if passed_tests >= 6 else 'âŒ Ã‰CHOUÃ‰'}")
    print("=" * 60)
    print(f"ðŸŽ¯ RÃ‰SULTAT GLOBAL: {passed_tests}/{total_tests} tests rÃ©ussis")
    
    if passed_tests == total_tests:
        print("âœ… Toutes les corrections sont cohÃ©rentes et intÃ©grÃ©es !")
    elif passed_tests >= total_tests * 0.8:
        print("âš ï¸ La plupart des corrections fonctionnent, quelques ajustements peuvent Ãªtre nÃ©cessaires.")
    else:
        print("âŒ Des problÃ¨mes d'intÃ©gration majeurs dÃ©tectÃ©s.")
    
    return passed_tests == total_tests


async def test_json_integration_with_nodes():
    """Test que la sÃ©rialisation JSON fonctionne avec les vrais nÅ“uds."""
    
    # CrÃ©er un Ã©tat complexe comme dans un vrai workflow
    task = TaskRequest(
        task_id="integration_test",
        title="Test intÃ©gration JSON",
        description="Test pour vÃ©rifier l'intÃ©gration de la sÃ©rialisation JSON",
        priority=TaskPriority.HIGH,
        task_type=TaskType.FEATURE
    )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        state = GraphState(
            task=task,
            results={
                "working_directory": temp_dir,
                "complex_data": {
                    "nested_object": {"key": "value"},
                    "list_data": [1, 2, 3],
                    "datetime": datetime.now(),
                    "none_value": None
                },
                "ai_messages": [],
                "error_logs": []
            }
        )
        
        # Tester la persistence avec DatabasePersistenceService
        persistence = DatabasePersistenceService()
        
        # Nettoyer les donnÃ©es comme le ferait le service
        cleaned_data = persistence._clean_for_json_serialization(state["results"]["complex_data"])
        print(f"  âœ… DonnÃ©es complexes nettoyÃ©es: {type(cleaned_data)}")
        
        # VÃ©rifier que les donnÃ©es peuvent Ãªtre sÃ©rialisÃ©es
        json_str = json.dumps(cleaned_data, ensure_ascii=False)
        print(f"  âœ… SÃ©rialisation rÃ©ussie: {len(json_str)} caractÃ¨res")
        
        # VÃ©rifier la dÃ©sÃ©rialisation
        deserialized = json.loads(json_str)
        assert isinstance(deserialized, dict), "La dÃ©sÃ©rialisation doit retourner un dict"
        print(f"  âœ… DÃ©sÃ©rialisation rÃ©ussie: {len(deserialized)} clÃ©s")


async def test_working_directory_propagation():
    """Test la propagation du rÃ©pertoire de travail entre les nÅ“uds."""
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # Ã‰tat initial sans working_directory
        # Utiliser un dictionnaire standard au lieu de GraphState pour ce test
        initial_state = {
            "task": TaskRequest(
                task_id="wd_test",
                title="Test WD propagation",
                description="Test propagation working directory",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.BUGFIX
            ),
            "results": {
                "prepare_result": {"working_directory": temp_dir},
                "ai_messages": [],
                "error_logs": []
            }
        }
        
        # Test 1: get_working_directory trouve le rÃ©pertoire
        wd = get_working_directory(initial_state)
        assert wd == temp_dir, f"WD devrait Ãªtre {temp_dir}, trouvÃ© {wd}"
        print(f"  âœ… get_working_directory fonctionne: {wd}")
        
        # Test 2: validate_working_directory valide correctement
        is_valid = validate_working_directory(wd, "test_propagation")
        assert is_valid, f"WD {wd} devrait Ãªtre valide"
        print(f"  âœ… validate_working_directory fonctionne: {is_valid}")
        
        # Test 3: ensure_working_directory utilise l'existant
        ensured_wd = ensure_working_directory(initial_state, "test_")
        assert ensured_wd == temp_dir, f"ensure_working_directory devrait retourner {temp_dir}"
        print(f"  âœ… ensure_working_directory utilise l'existant: {ensured_wd}")
        
        # Test 4: Ã‰tat sans rÃ©pertoire - crÃ©ation automatique
        empty_state = {
            "task": TaskRequest(
                task_id="empty_test",
                title="Empty test",
                description="Test creation automatique",
                priority=TaskPriority.LOW,
                task_type=TaskType.TESTING
            ),
            "results": {"ai_messages": [], "error_logs": []}
        }
        
        new_wd = ensure_working_directory(empty_state, "auto_")
        assert os.path.exists(new_wd), f"Nouveau WD {new_wd} devrait exister"
        print(f"  âœ… CrÃ©ation automatique WD fonctionne: {new_wd}")


async def test_pr_service_integration():
    """Test l'intÃ©gration du service PR avec les nÅ“uds."""
    
    # Mock des appels GitHub pour Ã©viter les erreurs d'API
    with patch('tools.github_tool.GitHubTool._arun') as mock_github:
        async def mock_github_call(*args, **kwargs):
            return {
                "success": True,
                "pr_number": 123,
                "pr_url": "https://github.com/test/repo/pull/123"
            }
        mock_github.side_effect = mock_github_call
        
        with tempfile.TemporaryDirectory() as temp_dir:
            state = GraphState(
                task=TaskRequest(
                    task_id="pr_test",
                    title="Test PR service",
                    description="Test intÃ©gration service PR",
                    priority=TaskPriority.HIGH,
                    task_type=TaskType.FEATURE,
                    repository_url="https://github.com/test/repo"
                ),
                results={
                    "working_directory": temp_dir,
                    "git_result": {"branch_name": "feature/test"},
                    "ai_messages": [],
                    "error_logs": []
                }
            )
            
            # Test crÃ©ation PR via le service
            result = await pr_service.ensure_pull_request_created(state)
            
            # Le service devrait appeler GitHub mÃªme si mockÃ©
            assert result.success, f"PR creation devrait rÃ©ussir: {result.error}"
            assert result.pr_info is not None, "pr_info devrait Ãªtre prÃ©sent"
            assert result.pr_info.number == 123, "Le numÃ©ro de PR devrait Ãªtre 123"
            print(f"  âœ… Service PR fonctionne: #{result.pr_info.number}")
            
            # Le service PR ne met pas automatiquement Ã  jour l'Ã©tat dans cette version
            # C'est le travail du nÅ“ud qui l'appelle (merge_node)
            print(f"  âœ… Service PR fonctionne correctement")


async def test_state_consistency_across_nodes():
    """Test la cohÃ©rence des Ã©tats quand ils passent entre les nÅ“uds."""
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # CrÃ©er un fichier test pour que le rÃ©pertoire ne soit pas vide
        test_file = os.path.join(temp_dir, "test.py")
        with open(test_file, 'w') as f:
            f.write("# Test file\nprint('hello')\n")
        
        initial_state = GraphState(
            task=TaskRequest(
                task_id="consistency_test",
                title="Test cohÃ©rence Ã©tats",
                description="Test cohÃ©rence entre nÅ“uds",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.FEATURE
            ),
            results={
                "working_directory": temp_dir,
                "ai_messages": [],
                "error_logs": [],
                "debug_attempts": 0
            }
        )
        
        # Simuler le passage par plusieurs nÅ“uds et vÃ©rifier la cohÃ©rence
        print(f"  ðŸ“ Ã‰tat initial WD: {initial_state['results']['working_directory']}")
        
        # Chaque nÅ“ud devrait prÃ©server et valider le working_directory
        for node_name in ["test_node", "debug_node", "qa_node"]:
            wd = get_working_directory(initial_state)
            assert wd == temp_dir, f"{node_name}: WD incohÃ©rent, attendu {temp_dir}, trouvÃ© {wd}"
            
            is_valid = validate_working_directory(wd, node_name)
            assert is_valid, f"{node_name}: WD devrait Ãªtre valide"
            
            print(f"  âœ… {node_name}: WD cohÃ©rent et valide")
        
        # VÃ©rifier que les messages d'erreur sont conservÃ©s
        initial_state["results"]["error_logs"].append("Test error")
        assert len(initial_state["results"]["error_logs"]) == 1, "Error logs devraient Ãªtre prÃ©servÃ©s"
        print(f"  âœ… Error logs prÃ©servÃ©s: {len(initial_state['results']['error_logs'])}")


async def test_robust_error_handling():
    """Test la gestion d'erreurs robuste entre les nÅ“uds."""
    
    # Test 1: Ã‰tat avec rÃ©pertoire invalide
    invalid_state = GraphState(
        task=TaskRequest(
            task_id="error_test",
            title="Test gestion erreurs",
            description="Test robustesse",
            priority=TaskPriority.LOW,
            task_type=TaskType.BUGFIX
        ),
        results={
            "working_directory": "/path/that/does/not/exist",
            "ai_messages": [],
            "error_logs": []
        }
    )
    
    # ensure_working_directory devrait crÃ©er un nouveau rÃ©pertoire
    recovered_wd = ensure_working_directory(invalid_state, "error_recovery_")
    assert os.path.exists(recovered_wd), "WD de rÃ©cupÃ©ration devrait exister"
    print(f"  âœ… RÃ©cupÃ©ration rÃ©pertoire invalide: {recovered_wd}")
    
    # Test 2: Ã‰tat avec donnÃ©es corrompues
    corrupted_state = GraphState(
        task=TaskRequest(
            task_id="corrupted_test",
            title="Test donnÃ©es corrompues",
            description="Test robustesse donnÃ©es",
            priority=TaskPriority.HIGH,
                            task_type=TaskType.TESTING
        ),
        results={
            "ai_messages": [],
            "error_logs": [],
            "corrupted_data": object()  # Objet non-sÃ©rialisable
        }
    )
    
    # La sÃ©rialisation devrait nettoyer les donnÃ©es corrompues
    persistence = DatabasePersistenceService()
    cleaned = persistence._clean_for_json_serialization(corrupted_state["results"]["corrupted_data"])
    assert isinstance(cleaned, str), "DonnÃ©es corrompues devraient Ãªtre converties en string"
    print(f"  âœ… DonnÃ©es corrompues nettoyÃ©es: {type(cleaned)}")


async def test_performance_and_logging():
    """Test les performances et le logging intÃ©grÃ©."""
    
    import time
    
    # Test configuration du logging
    logging_configured = logging_service.setup_logging()
    assert logging_configured, "Le logging devrait Ãªtre configurÃ©"
    
    logs_info = logging_service.get_logs_info()
    assert logs_info['logs_directory'] is not None, "RÃ©pertoire logs devrait Ãªtre configurÃ©"
    assert logs_info['handlers_count'] > 0, "Des handlers devraient Ãªtre configurÃ©s"
    print(f"  âœ… Logging configurÃ©: {logs_info['handlers_count']} handlers")
    
    # Test performance des fonctions helpers
    with tempfile.TemporaryDirectory() as temp_dir:
        state = GraphState(
            task=TaskRequest(
                task_id="perf_test",
                title="Test performance",
                description="Test performances",
                priority=TaskPriority.MEDIUM,
                task_type=TaskType.PERFORMANCE
            ),
            results={
                "working_directory": temp_dir,
                "ai_messages": [],
                "error_logs": []
            }
        )
        
        # Mesurer le temps d'exÃ©cution des opÃ©rations courantes
        start_time = time.time()
        
        for i in range(100):
            wd = get_working_directory(state)
            validate_working_directory(wd, f"perf_test_{i}")
        
        duration = time.time() - start_time
        assert duration < 1.0, f"100 opÃ©rations WD devraient prendre < 1s, pris {duration:.3f}s"
        print(f"  âœ… Performance WD: 100 ops en {duration:.3f}s")
        
        # Test sÃ©rialisation de grandes donnÃ©es
        large_data = {"data": [{"item": i, "value": f"test_{i}"} for i in range(1000)]}
        
        start_time = time.time()
        persistence = DatabasePersistenceService()
        cleaned = persistence._clean_for_json_serialization(large_data)
        json_str = json.dumps(cleaned)
        duration = time.time() - start_time
        
        assert duration < 0.5, f"SÃ©rialisation 1000 items devrait prendre < 0.5s, pris {duration:.3f}s"
        assert len(json_str) > 0, "JSON sÃ©rialisÃ© devrait avoir du contenu"
        print(f"  âœ… Performance JSON: 1000 items en {duration:.3f}s, {len(json_str)} chars")


if __name__ == "__main__":
    print("ðŸš€ DÃ©marrage des tests d'intÃ©gration...")
    success = asyncio.run(test_workflow_integration())
    exit(0 if success else 1) 